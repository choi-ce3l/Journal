{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c6c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\n",
      "✅ 필터 적용 완료 후 Enter를 눌러주세요...\n",
      "📄 페이지 1에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 2에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 3에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 4에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 5에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 6에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 7에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 8에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 9에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 10에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 11에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 12에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 13에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 14에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 15에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 16에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 17에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 18에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 19에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 20에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 21에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 22에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 23에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 24에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 25에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 26에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 27에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 28에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 29에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 30에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 31에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 32에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 33에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 34에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 35에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 36에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 37에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 38에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 39에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 40에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 41에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 42에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 43에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 44에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 45에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 46에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 47에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 48에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 49에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 50에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 51에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 52에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 53에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 54에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 55에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 56에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 57에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 58에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 59에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 60에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 61에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 62에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 63에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 64에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 65에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 66에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 67에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 68에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 69에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 70에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 71에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 72에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 73에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 74에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 75에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 76에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 77에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 78에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 79에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 80에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 81에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 82에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 83에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 84에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 85에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 86에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 87에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 88에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 89에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 90에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 91에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 92에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 93에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 94에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 95에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 96에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 97에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 98에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 99에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 100에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 101에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 102에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 103에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 104에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 105에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 106에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 107에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 108에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 109에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 110에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 111에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 112에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 113에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 114에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 115에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 116에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 117에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 118에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 119에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 120에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 121에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 122에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 123에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 124에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 125에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 126에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 127에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 128에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 129에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 130에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 131에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 132에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 133에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 134에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 135에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 136에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 137에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 138에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 139에서 20개 링크 수집\n",
      "❌ 다음 페이지 버튼을 찾을 수 없습니다.\n",
      "✅ 완료: 총 2780개의 항목을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")  # 로그인해야 하므로 꺼둠\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 로그인 및 필터 후 검색 페이지 수동 접속\n",
    "print(\"🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\")\n",
    "driver.get(\"https://www.proquest.com/\")\n",
    "input(\"✅ 필터 적용 완료 후 Enter를 눌러주세요...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "MAX_PAGES = 139  # 원하는 페이지 수만큼 반복\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.select('div.resultHeader div h3 a')\n",
    "\n",
    "    print(f\"📄 페이지 {page}에서 {len(links)}개 링크 수집\")\n",
    "\n",
    "    for link in links:\n",
    "        title = link.get_text(strip=True)\n",
    "        href = link.get('href')\n",
    "        full_url = \"https://www.proquest.com\" + href if href.startswith(\"/\") else href\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': full_url\n",
    "        })\n",
    "\n",
    "    # 다음 페이지로 이동 시도\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//*[@id=\"updateForm\"]/nav/ul/li[9]/a')\n",
    "        if next_button.is_enabled():\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            print(\"➡️ 다음 페이지로 이동...\")\n",
    "            time.sleep(4)\n",
    "        else:\n",
    "            print(\"🚫 다음 페이지 버튼이 비활성화되어 있습니다.\")\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        print(\"❌ 다음 페이지 버튼을 찾을 수 없습니다.\")\n",
    "        break\n",
    "    except ElementClickInterceptedException:\n",
    "        print(\"⚠️ 버튼 클릭 실패. 스크롤 후 재시도\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# CSV 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"proquest_nyt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 완료: 총 {len(df)}개의 항목을 저장했습니다.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d312dfa",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-21T03:52:47.907421Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument('--headless')  # 필요 시 주석 해제\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv('proquest_nyt.csv')\n",
    "# df = df.head(20)  # 테스트로 일부만 실행\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['link']\n",
    "    title = row['title']\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # ✅ URL에서 docview ID 추출\n",
    "        doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "        doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "        mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "        # ✅ 본문(Content)\n",
    "        content = ''\n",
    "        xpaths_content = [\n",
    "            f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "            '//*[@id=\"fullTextZone\"]//p',\n",
    "            '//*[@id=\"main-content\"]//p',\n",
    "            '//*[@id=\"companionColumn-0\"]//p',\n",
    "        ]\n",
    "        for xp in xpaths_content:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                if paragraphs:\n",
    "                    content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "        date_text = ''\n",
    "        try:\n",
    "            strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "            if strong_tag and strong_tag.next_sibling:\n",
    "                raw = strong_tag.next_sibling.strip()\n",
    "                match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)  # \"04 Feb 2025\"\n",
    "                if match:\n",
    "                    date_text = match.group()\n",
    "                else:\n",
    "                    date_text = raw\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ✅ 미디어(Media)\n",
    "        media = ''\n",
    "        xpaths_media = [\n",
    "            f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "            '//*[@id=\"bibSource\"]/span',\n",
    "            '//div[@class=\"publicationTitle\"]',\n",
    "            '//div[@class=\"bibSource\"]',\n",
    "            '//span[@class=\"publicationTitle\"]',\n",
    "        ]\n",
    "        for xp in xpaths_media:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                media_el = driver.find_element(By.XPATH, xp)\n",
    "                media = media_el.text.strip().split(';')[0]\n",
    "                if media:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': date_text,\n",
    "            'media': media,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "        print(f\"[{idx+1}] ✅ 완료: {title[:40]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': '',\n",
    "            'media': '',\n",
    "            'content': ''\n",
    "        })\n",
    "\n",
    "# 저장\n",
    "output_df = pd.DataFrame(results, columns=['title', 'link', 'date', 'media', 'content'])\n",
    "output_df.to_csv('nyt_content.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"📄 저장 완료: nyt_content.csv\")\n",
    "\n",
    "driver.quit()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Academia-Industry-gap-analysis/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474164cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>media</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IsArtificialIntelligenceReally Worth the Hype?...</td>\n",
       "      <td>https://www.proquest.com/docview/3164759796/F4...</td>\n",
       "      <td>09 Feb 2025</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>After the arrival of a less costly A.I. model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As Millions Watched,ArtificialIntelligenceRout...</td>\n",
       "      <td>https://www.proquest.com/docview/3078865209/F4...</td>\n",
       "      <td>. 12 July 2024: A.4.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Lee Saedol was one of the world's top Go playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newsom Vetoes Sweeping Legislation That Sought...</td>\n",
       "      <td>https://www.proquest.com/docview/3111035315/F4...</td>\n",
       "      <td>30 Sep 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The bill would have been the first in the nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden to Lay Out Limits OnArtificialIntelligen...</td>\n",
       "      <td>https://www.proquest.com/docview/2883426688/F4...</td>\n",
       "      <td>31 Oct 2023</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>In an order to be issued on Monday, the White ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cohen UsedArtificialIntelligenceto Give Lawyer...</td>\n",
       "      <td>https://www.proquest.com/docview/2907646267/F4...</td>\n",
       "      <td>30 Dec 2023</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Donald Trump's former fixer had sought an earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AsArtificialIntelligenceGrows, a New Diplomati...</td>\n",
       "      <td>https://www.proquest.com/docview/3051253630/F4...</td>\n",
       "      <td>07 May 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The new U.S. approach to cyberthreats comes as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What to Expect in 2024 Regarding the Evolution...</td>\n",
       "      <td>https://www.proquest.com/docview/2907646242/F4...</td>\n",
       "      <td>30 Dec 2023</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Mustafa Suleyman remembers the epochal moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Climate Took Back Seat as Davos Talks Mostly F...</td>\n",
       "      <td>https://www.proquest.com/docview/2917144031/F4...</td>\n",
       "      <td>22 Jan 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Artificial intelligence was the unofficial the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rapid Growth ofArtificialIntelligenceIs Testin...</td>\n",
       "      <td>https://www.proquest.com/docview/2908033107/F4...</td>\n",
       "      <td>01 Jan 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The use of content from news and information p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Best Place for Jobs inArtificialIntelligence? ...</td>\n",
       "      <td>https://www.proquest.com/docview/2840134379/F4...</td>\n",
       "      <td>. 21 July 2023: B.4.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Brookings Institution researchers found that S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ArtificialIntelligenceIs Outperforming Radiolo...</td>\n",
       "      <td>https://www.proquest.com/docview/2331708237/F4...</td>\n",
       "      <td>02 Jan 2020</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Computers that are trained to recognize patter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CanArtificialIntelligenceBe Bias-Free?: [Money...</td>\n",
       "      <td>https://www.proquest.com/docview/2503207275/F4...</td>\n",
       "      <td>21 Mar 2021</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AI Is Starting to Wear Down Democracy</td>\n",
       "      <td>https://www.proquest.com/docview/3224957822/F4...</td>\n",
       "      <td>. 28 June 2025.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Since the explosion of generative artificial i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WhyArtificialIntelligenceOften Struggles With ...</td>\n",
       "      <td>https://www.proquest.com/docview/3083263459/F4...</td>\n",
       "      <td>. 22 July 2024.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>In the school year that ended recently, one cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ArtificialIntelligenceIs Changing How Silicon ...</td>\n",
       "      <td>https://www.proquest.com/docview/3175459776/F4...</td>\n",
       "      <td>09 Mar 2025</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>SAN FRANCISCO — Almost every day, Grant Lee, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is Tech Industry Already on Cusp ofArtificialI...</td>\n",
       "      <td>https://www.proquest.com/docview/3149184289/F4...</td>\n",
       "      <td>25 Dec 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>SAN FRANCISCO — Demis Hassabis, one of the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Heavy Investment in A.I. For China's Spy Agenc...</td>\n",
       "      <td>https://www.proquest.com/docview/3219498940/F4...</td>\n",
       "      <td>. 18 June 2025: A.12.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>A new report comes amid rising concern about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>China Has an Army of Robots on Its Side in the...</td>\n",
       "      <td>https://www.proquest.com/docview/3193862142/F4...</td>\n",
       "      <td>23 Apr 2025</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>NINGBO, China — China’s secret weapon in the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CanArtificialIntelligenceInvent?</td>\n",
       "      <td>https://www.proquest.com/docview/2838427557/F4...</td>\n",
       "      <td>. 17 July 2023.</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Generative artificial intelligence, the techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dark Corners of the Web Offer a Glimpse atArti...</td>\n",
       "      <td>https://www.proquest.com/docview/2911083281/F4...</td>\n",
       "      <td>08 Jan 2024</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When the Louisiana parole board met in October...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   IsArtificialIntelligenceReally Worth the Hype?...   \n",
       "1   As Millions Watched,ArtificialIntelligenceRout...   \n",
       "2   Newsom Vetoes Sweeping Legislation That Sought...   \n",
       "3   Biden to Lay Out Limits OnArtificialIntelligen...   \n",
       "4   Cohen UsedArtificialIntelligenceto Give Lawyer...   \n",
       "5   AsArtificialIntelligenceGrows, a New Diplomati...   \n",
       "6   What to Expect in 2024 Regarding the Evolution...   \n",
       "7   Climate Took Back Seat as Davos Talks Mostly F...   \n",
       "8   Rapid Growth ofArtificialIntelligenceIs Testin...   \n",
       "9   Best Place for Jobs inArtificialIntelligence? ...   \n",
       "10  ArtificialIntelligenceIs Outperforming Radiolo...   \n",
       "11  CanArtificialIntelligenceBe Bias-Free?: [Money...   \n",
       "12              AI Is Starting to Wear Down Democracy   \n",
       "13  WhyArtificialIntelligenceOften Struggles With ...   \n",
       "14  ArtificialIntelligenceIs Changing How Silicon ...   \n",
       "15  Is Tech Industry Already on Cusp ofArtificialI...   \n",
       "16  Heavy Investment in A.I. For China's Spy Agenc...   \n",
       "17  China Has an Army of Robots on Its Side in the...   \n",
       "18                   CanArtificialIntelligenceInvent?   \n",
       "19  Dark Corners of the Web Offer a Glimpse atArti...   \n",
       "\n",
       "                                                 link                   date  \\\n",
       "0   https://www.proquest.com/docview/3164759796/F4...            09 Feb 2025   \n",
       "1   https://www.proquest.com/docview/3078865209/F4...   . 12 July 2024: A.4.   \n",
       "2   https://www.proquest.com/docview/3111035315/F4...            30 Sep 2024   \n",
       "3   https://www.proquest.com/docview/2883426688/F4...            31 Oct 2023   \n",
       "4   https://www.proquest.com/docview/2907646267/F4...            30 Dec 2023   \n",
       "5   https://www.proquest.com/docview/3051253630/F4...            07 May 2024   \n",
       "6   https://www.proquest.com/docview/2907646242/F4...            30 Dec 2023   \n",
       "7   https://www.proquest.com/docview/2917144031/F4...            22 Jan 2024   \n",
       "8   https://www.proquest.com/docview/2908033107/F4...            01 Jan 2024   \n",
       "9   https://www.proquest.com/docview/2840134379/F4...   . 21 July 2023: B.4.   \n",
       "10  https://www.proquest.com/docview/2331708237/F4...            02 Jan 2020   \n",
       "11  https://www.proquest.com/docview/2503207275/F4...            21 Mar 2021   \n",
       "12  https://www.proquest.com/docview/3224957822/F4...        . 28 June 2025.   \n",
       "13  https://www.proquest.com/docview/3083263459/F4...        . 22 July 2024.   \n",
       "14  https://www.proquest.com/docview/3175459776/F4...            09 Mar 2025   \n",
       "15  https://www.proquest.com/docview/3149184289/F4...            25 Dec 2024   \n",
       "16  https://www.proquest.com/docview/3219498940/F4...  . 18 June 2025: A.12.   \n",
       "17  https://www.proquest.com/docview/3193862142/F4...            23 Apr 2025   \n",
       "18  https://www.proquest.com/docview/2838427557/F4...        . 17 July 2023.   \n",
       "19  https://www.proquest.com/docview/2911083281/F4...            08 Jan 2024   \n",
       "\n",
       "             media                                            content  \n",
       "0   New York Times  After the arrival of a less costly A.I. model ...  \n",
       "1   New York Times  Lee Saedol was one of the world's top Go playe...  \n",
       "2   New York Times  The bill would have been the first in the nati...  \n",
       "3   New York Times  In an order to be issued on Monday, the White ...  \n",
       "4   New York Times  Donald Trump's former fixer had sought an earl...  \n",
       "5   New York Times  The new U.S. approach to cyberthreats comes as...  \n",
       "6   New York Times  Mustafa Suleyman remembers the epochal moment ...  \n",
       "7   New York Times  Artificial intelligence was the unofficial the...  \n",
       "8   New York Times  The use of content from news and information p...  \n",
       "9   New York Times  Brookings Institution researchers found that S...  \n",
       "10  New York Times  Computers that are trained to recognize patter...  \n",
       "11  New York Times                                                NaN  \n",
       "12  New York Times  Since the explosion of generative artificial i...  \n",
       "13  New York Times  In the school year that ended recently, one cl...  \n",
       "14  New York Times  SAN FRANCISCO — Almost every day, Grant Lee, a...  \n",
       "15  New York Times  SAN FRANCISCO — Demis Hassabis, one of the mos...  \n",
       "16  New York Times  A new report comes amid rising concern about h...  \n",
       "17  New York Times  NINGBO, China — China’s secret weapon in the t...  \n",
       "18  New York Times  Generative artificial intelligence, the techno...  \n",
       "19  New York Times  When the Louisiana parole board met in October...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('nyt_content.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb1ffd",
   "metadata": {},
   "source": [
    "# 최선을 다해보는 수집 우하하 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd50f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 수집할 문서 수: 20\n",
      "[12] ✅ 업데이트 완료: CanArtificialIntelligenceBe Bias-Free?: ...\n",
      "[41] ✅ 업데이트 완료: Hochul Weighs Legislation Limiting AI an...\n",
      "[42] ✅ 업데이트 완료: More Than 100 Bills Await Decision by Ho...\n",
      "[43] ✅ 업데이트 완료: Hedge Fund Gave Birth To A.I. Star From ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_620286/1801790878.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0mdriver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m             \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m             \u001B[0msoup\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBeautifulSoup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdriver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpage_source\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'html.parser'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ▶ 기존 결과 불러오기\n",
    "existing_df = pd.read_csv('nyt_content.csv')\n",
    "\n",
    "# ▶ content가 빈 row만 필터링\n",
    "empty_content_df = existing_df[existing_df['content'].isna() | (existing_df['content'].str.strip() == '')]\n",
    "\n",
    "empty_content_df=empty_content_df.head(20)\n",
    "\n",
    "if empty_content_df.empty:\n",
    "    print(\"✅ 모든 content가 이미 수집되어 있습니다.\")\n",
    "else:\n",
    "    print(f\"🔄 수집할 문서 수: {len(empty_content_df)}\")\n",
    "\n",
    "    # ▶ 크롬 옵션 설정\n",
    "    options = Options()\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    # ▶ 드라이버 실행\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in empty_content_df.iterrows():\n",
    "        url = row['link']\n",
    "        title = row['title']\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # ✅ URL에서 docview ID 추출\n",
    "            doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "            doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "            mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "            # ✅ 본문(Content)\n",
    "            content = ''\n",
    "            xpaths_content = [\n",
    "                f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "                '//*[@id=\"fullTextZone\"]//p',\n",
    "                '//*[@id=\"main-content\"]//p',\n",
    "                '//*[@id=\"companionColumn-0\"]//p',\n",
    "            ]\n",
    "            for xp in xpaths_content:\n",
    "                if not xp:\n",
    "                    continue\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                    if paragraphs:\n",
    "                        content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "            date_text = row['date']  # 기본값: 기존 값 유지\n",
    "            try:\n",
    "                strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "                if strong_tag and strong_tag.next_sibling:\n",
    "                    raw = strong_tag.next_sibling.strip()\n",
    "                    match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)\n",
    "                    if match:\n",
    "                        date_text = match.group()\n",
    "                    else:\n",
    "                        date_text = raw\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # ✅ 미디어(Media)\n",
    "            media = row['media']  # 기본값: 기존 값 유지\n",
    "            xpaths_media = [\n",
    "                f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "                '//*[@id=\"bibSource\"]/span',\n",
    "                '//div[@class=\"publicationTitle\"]',\n",
    "                '//div[@class=\"bibSource\"]',\n",
    "                '//span[@class=\"publicationTitle\"]',\n",
    "            ]\n",
    "            for xp in xpaths_media:\n",
    "                if not xp:\n",
    "                    continue\n",
    "                try:\n",
    "                    media_el = driver.find_element(By.XPATH, xp)\n",
    "                    media = media_el.text.strip().split(';')[0]\n",
    "                    if media:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # ✅ 결과 저장\n",
    "            existing_df.loc[idx, 'content'] = content\n",
    "            existing_df.loc[idx, 'date'] = date_text\n",
    "            existing_df.loc[idx, 'media'] = media\n",
    "\n",
    "            print(f\"[{idx+1}] ✅ 업데이트 완료: {title[:40]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "\n",
    "    # ▶ CSV 덮어쓰기 저장\n",
    "    existing_df.to_csv('nyt_content.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"📄 저장 완료: nyt_content.csv\")\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6c1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2780 entries, 0 to 2779\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    2780 non-null   object\n",
      " 1   link     2780 non-null   object\n",
      " 2   date     858 non-null    object\n",
      " 3   media    858 non-null    object\n",
      " 4   content  770 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 108.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test=pd.read_csv('nyt_content.csv')\n",
    "test.info() #기존 770개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bac03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
