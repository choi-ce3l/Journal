{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c6c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\n",
      "✅ 필터 적용 완료 후 Enter를 눌러주세요...\n",
      "📄 페이지 1에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 2에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 3에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 4에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 5에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 6에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 7에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 8에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 9에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 10에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 11에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 12에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 13에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 14에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 15에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 16에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 17에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 18에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 19에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 20에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 21에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 22에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 23에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 24에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 25에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 26에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 27에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 28에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 29에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 30에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 31에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 32에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 33에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 34에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 35에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 36에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 37에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 38에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 39에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 40에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 41에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 42에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 43에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 44에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 45에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 46에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 47에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 48에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 49에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 50에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 51에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 52에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 53에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 54에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 55에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 56에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 57에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 58에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 59에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 60에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 61에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 62에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 63에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 64에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 65에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 66에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 67에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 68에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 69에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 70에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 71에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 72에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 73에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 74에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 75에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 76에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 77에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 78에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 79에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 80에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 81에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 82에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 83에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 84에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 85에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 86에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 87에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 88에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 89에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 90에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 91에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 92에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 93에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 94에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 95에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 96에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 97에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 98에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 99에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 100에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 101에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 102에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 103에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 104에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 105에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 106에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 107에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 108에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 109에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 110에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 111에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 112에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 113에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 114에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 115에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 116에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 117에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 118에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 119에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 120에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 121에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 122에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 123에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 124에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 125에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 126에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 127에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 128에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 129에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 130에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 131에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 132에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 133에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 134에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 135에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 136에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 137에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 138에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 139에서 20개 링크 수집\n",
      "❌ 다음 페이지 버튼을 찾을 수 없습니다.\n",
      "✅ 완료: 총 2780개의 항목을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")  # 로그인해야 하므로 꺼둠\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 로그인 및 필터 후 검색 페이지 수동 접속\n",
    "print(\"🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\")\n",
    "driver.get(\"https://www.proquest.com/\")\n",
    "input(\"✅ 필터 적용 완료 후 Enter를 눌러주세요...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "MAX_PAGES = 139  # 원하는 페이지 수만큼 반복\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.select('div.resultHeader div h3 a')\n",
    "\n",
    "    print(f\"📄 페이지 {page}에서 {len(links)}개 링크 수집\")\n",
    "\n",
    "    for link in links:\n",
    "        title = link.get_text(strip=True)\n",
    "        href = link.get('href')\n",
    "        full_url = \"https://www.proquest.com\" + href if href.startswith(\"/\") else href\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': full_url\n",
    "        })\n",
    "\n",
    "    # 다음 페이지로 이동 시도\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//*[@id=\"updateForm\"]/nav/ul/li[9]/a')\n",
    "        if next_button.is_enabled():\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            print(\"➡️ 다음 페이지로 이동...\")\n",
    "            time.sleep(4)\n",
    "        else:\n",
    "            print(\"🚫 다음 페이지 버튼이 비활성화되어 있습니다.\")\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        print(\"❌ 다음 페이지 버튼을 찾을 수 없습니다.\")\n",
    "        break\n",
    "    except ElementClickInterceptedException:\n",
    "        print(\"⚠️ 버튼 클릭 실패. 스크롤 후 재시도\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# CSV 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"proquest_nyt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 완료: 총 {len(df)}개의 항목을 저장했습니다.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d312dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T04:04:16.864240Z",
     "start_time": "2025-09-21T03:52:47.907421Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument('--headless')  # 필요 시 주석 해제\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv('proquest_nyt.csv')\n",
    "# df = df.head(20)  # 테스트로 일부만 실행\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['link']\n",
    "    title = row['title']\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # ✅ URL에서 docview ID 추출\n",
    "        doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "        doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "        mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "        # ✅ 본문(Content)\n",
    "        content = ''\n",
    "        xpaths_content = [\n",
    "            f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "            '//*[@id=\"fullTextZone\"]//p',\n",
    "            '//*[@id=\"main-content\"]//p',\n",
    "            '//*[@id=\"companionColumn-0\"]//p',\n",
    "        ]\n",
    "        for xp in xpaths_content:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                if paragraphs:\n",
    "                    content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "        date_text = ''\n",
    "        try:\n",
    "            strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "            if strong_tag and strong_tag.next_sibling:\n",
    "                raw = strong_tag.next_sibling.strip()\n",
    "                match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)  # \"04 Feb 2025\"\n",
    "                if match:\n",
    "                    date_text = match.group()\n",
    "                else:\n",
    "                    date_text = raw\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ✅ 미디어(Media)\n",
    "        media = ''\n",
    "        xpaths_media = [\n",
    "            f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "            '//*[@id=\"bibSource\"]/span',\n",
    "            '//div[@class=\"publicationTitle\"]',\n",
    "            '//div[@class=\"bibSource\"]',\n",
    "            '//span[@class=\"publicationTitle\"]',\n",
    "        ]\n",
    "        for xp in xpaths_media:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                media_el = driver.find_element(By.XPATH, xp)\n",
    "                media = media_el.text.strip().split(';')[0]\n",
    "                if media:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': date_text,\n",
    "            'media': media,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "        print(f\"[{idx+1}] ✅ 완료: {title[:40]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': '',\n",
    "            'media': '',\n",
    "            'content': ''\n",
    "        })\n",
    "\n",
    "# 저장\n",
    "output_df = pd.DataFrame(results, columns=['title', 'link', 'date', 'media', 'content'])\n",
    "output_df.to_csv('nyt_content.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"📄 저장 완료: nyt_content.csv\")\n",
    "\n",
    "driver.quit()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Academia-Industry-gap-analysis/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ✅ 완료: Bringing Art Back to Life WithArtificial...\n",
      "[2] ✅ 완료: Trump Plans to Give AI Developers a Free...\n",
      "[3] ✅ 완료: Trump Administration Plans to Give AI De...\n",
      "[4] ✅ 완료: A.I. Changes Video Games And Alters An I...\n",
      "[5] ✅ 완료: Their Water Taps Ran Dry When Meta Built...\n",
      "[6] ✅ 완료: AI-Driven Education: Founded in Texas an...\n",
      "[7] ✅ 완료: AI-Driven Education: Founded in Texas an...\n",
      "[8] ✅ 완료: Accelerating the Pace of Math Discoverie...\n",
      "[9] ✅ 완료: Like U.S. and China, U.A.E. Shares A.I. ...\n",
      "[10] ✅ 완료: He Sold His Likeness. Now His Avatar Is ...\n",
      "[11] ✅ 완료: China Summons Nvidia Over 'Backdoor' Ris...\n",
      "[12] ✅ 완료: China Turns to AI in Information Warfare...\n",
      "[13] ✅ 완료: China Turning To A.I. to Push Its Propag...\n",
      "[14] ✅ 완료: Nvidia Chips Take Spotlight in China, Wh...\n",
      "[15] ✅ 완료: AI is Coming to the NFL, and It Could Tr...\n",
      "[16] ✅ 완료: U.S. Government to Take Cut of Nvidia an...\n",
      "[17] ✅ 완료: A.I. Firm Collapses In Middle Of A.I. Bo...\n",
      "[18] ✅ 완료: Nvidia's C.E.O. Treads Carefully Between...\n",
      "[19] ✅ 완료: Nvidia Becomes First Public Company Wort...\n",
      "[20] ✅ 완료: First Lady Puts Humanity on Alert: 'The ...\n",
      "[21] ✅ 완료: U.S. Gets Cut Of Chip Sales To Chinese: ...\n",
      "[22] ✅ 완료: Nvidia Reaches $4 Trillion In Value, a L...\n",
      "[23] ✅ 완료: A.I. Job Interviewers Want to Know the R...\n",
      "[24] ✅ 완료: China Expands Influence in UNESCO as Tru...\n",
      "[25] ✅ 완료: At the Heart of a Tech Town: [Business/F...\n",
      "[26] ✅ 완료: How Nvidia’s Jensen Huang Persuaded Trum...\n",
      "[27] ✅ 완료: Democrats Warn Gabbard Of China Election...\n",
      "[28] ✅ 완료: 'Panic Withdrawal' of Goods At Duty-Free...\n",
      "[29] ✅ 완료: The Global AI Divide...\n",
      "[30] ✅ 완료: Nvidia Gives Intel Lifeline In Deal to I...\n",
      "[31] ✅ 완료: 2 Leading European Tech Firms Strike an ...\n",
      "[32] ✅ 완료: European Union Unveils Guidelines for Po...\n",
      "[33] ✅ 완료: Top N.S.A. Official Tried to Save Scient...\n",
      "[34] ✅ 완료: Anatomy of 2 Giant Deals: The UAE Got Ch...\n",
      "[35] ✅ 완료: Trump Intends To Unleash A.I. To Spur Bo...\n",
      "[36] ✅ 완료: Welcome to Your Job Interview. Your Inte...\n",
      "[37] ✅ 완료: Silicon Valley Is in Its ‘Hard Tech’ Era...\n",
      "[38] ✅ 완료: Researchers Courted Like N.B.A. Stars: [...\n",
      "[39] ✅ 완료: China Puts Big Money Behind A.I.: [Corre...\n",
      "[40] ✅ 완료: AI Researchers Are Negotiating $250 Mill...\n",
      "[41] ✅ 완료: Inside Tech's Temple Of Ideas: [Money an...\n",
      "[42] ✅ 완료: The Coder ‘Village’ at the Heart of Chin...\n",
      "[43] ✅ 완료: Losses for Health Care Sector Help Push ...\n",
      "[44] ✅ 완료: Trump Is Taking Aim at Chipmakers In Sou...\n",
      "[45] ✅ 완료: State Dept. Is Investigating Messages Im...\n",
      "[46] ✅ 완료: Loomer Scuttles Senator's Visit to Spy A...\n",
      "[47] ✅ 완료: Musk's xAI Sues Apple and OpenAI, Claimi...\n",
      "[48] ✅ 완료: Zuckerberg Shakes Up A.I. Efforts At Met...\n",
      "[49] ✅ 완료: Madison Avenue Is Starting to Love A.I. ...\n",
      "[50] ✅ 완료: How a Video Studio Embraced AI and Storm...\n",
      "[51] ✅ 완료: ICE Prepares to Expand Scope With $20 Bi...\n",
      "[52] ✅ 완료: Ex-N.S.A. Head Warns of Chinese Threats ...\n",
      "[53] ✅ 완료: Apple Introduces a Thinner, Smaller Vers...\n",
      "[54] ✅ 완료: Google's Monopoly Win: [Business/Financi...\n",
      "[55] ✅ 완료: Anthropic Agrees to Pay $1.5 Billion to ...\n",
      "[56] ✅ 완료: A New Crop of 20-Something C.E.O.s Is Po...\n",
      "[57] ✅ 완료: What a Ruling to Fix Google’s Search Mon...\n",
      "[58] ✅ 완료: Tesla Pay May Create A Trillionaire: [Bu...\n",
      "[59] ✅ 완료: U.S. Pushes a Tech Race, but Trump's Foc...\n",
      "[60] ✅ 완료: Intel Shows Even Giants In Tech Fade: [B...\n",
      "[61] ✅ 완료: Believe in A.I.? Buy Beaten-Down Value S...\n",
      "[62] ✅ 완료: The Future of Weather Prediction Is Here...\n",
      "[63] ✅ 완료: Research Papers Hold The Hints of a Chat...\n",
      "[64] ✅ 완료: A Hacker House Of Women Harnessing A.I.:...\n",
      "[65] ✅ 완료: In Mideast, The Warfare Goes Online: [Bu...\n",
      "[66] ✅ 완료: Why AI Should Make Parents Rethink Posti...\n",
      "[67] ✅ 완료: How Nvidia's C.E.O. Paved Way for Chip S...\n",
      "[68] ✅ 완료: Ellison Wants to Do Good, and Profit: [B...\n",
      "[69] ✅ 완료: Musk to Get Stock Worth $29 Billion: [Bu...\n",
      "[70] ✅ 완료: C.E.O.s Push A.I., but Do They Get It?: ...\n",
      "[71] ✅ 완료: Training A.I. to Think Like Us, Quirks a...\n",
      "[72] ✅ 완료: The Global A.I. Divide: [Business/Financ...\n",
      "[73] ✅ 완료: Unsettled Future For the Business Of Pre...\n",
      "[74] ✅ 완료: The Future of Weather Prediction Is Here...\n",
      "[75] ✅ 완료: Targeting Obama, Trump’s Retribution Cam...\n",
      "[76] ✅ 완료: Insider's Guide To the Home Of A.I.'s Bo...\n",
      "[77] ✅ 완료: In Shift, OpenAI Shares Tech: [Business/...\n",
      "[78] ✅ 완료: In London, Digital Policy Fuels Debate: ...\n",
      "[79] ✅ 완료: Google A.I. System Wins Gold in Global M...\n",
      "[80] ✅ 완료: Building Data Centers Pays Off for Micro...\n",
      "[81] ✅ 완료: Google Hires Leaders of A.I. Start-Up: [...\n",
      "[82] ✅ 완료: Zuckerberg Pushes 'Superintelligent' A.I...\n",
      "[83] ✅ 완료: 'Horrific' Grok Chatbot Posts Echoed X U...\n",
      "[84] ✅ 완료: Social Media Outcry After Grok Chatbot S...\n",
      "[85] ✅ 완료: Wall Street Extends Its Record High Into...\n",
      "[86] ✅ 완료: Microsoft Plans Over $4 Billion For Educ...\n",
      "[87] ✅ 완료: $165,000: [Money and Business/Financial ...\n",
      "[88] ✅ 완료: The Digest: [Business/Financial Desk]...\n",
      "[89] ✅ 완료: Silicon Valley Puts Millions Into Politi...\n",
      "[90] ✅ 완료: New Valuation For Anthropic Nearly Tripl...\n",
      "[91] ✅ 완료: OpenAI Signs $300 Billion Data Center Pa...\n",
      "[92] ✅ 완료: Big Tech Sector Gains Nudge Wall Street ...\n",
      "[93] ✅ 완료: They're Fabulous, Famous and Fake: [Styl...\n",
      "[94] ✅ 완료: OpenAI Deal Could Value It at $500 Billi...\n",
      "[95] ✅ 완료: German Carmakers Staging a Comeback: [Bu...\n",
      "[96] ✅ 완료: Video Game Actors End Contract Dispute O...\n",
      "[97] ✅ 완료: Trump Plan Lets Patients Widely Share Me...\n",
      "[98] ✅ 완료: Seeing the Future You, With an Assist Fr...\n",
      "[99] ✅ 완료: Games Show Robots' Potential, But Your J...\n",
      "[100] ✅ 완료: Meta Debuts Smart Glasses Featuring an A...\n",
      "[101] ✅ 완료: Trump Seeks to Cut Funds For Science by ...\n",
      "[102] ✅ 완료: Nvidia to Buy $5 Billion Stake in Intel,...\n",
      "[103] ✅ 완료: Galleries: [Movies, Performing Arts/Week...\n",
      "[104] ✅ 완료: The Newest Face of Long-Term Unemploymen...\n",
      "[105] ✅ 완료: Nvidia Readies New Chip for China As Was...\n",
      "[106] ✅ 완료: Stock Market Prospers Amid Indications o...\n",
      "[107] ✅ 완료: Apple's New AirPods Are Not Leaving Anyt...\n",
      "[108] ✅ 완료: The New AirPods Can Translate Languages ...\n",
      "[109] ✅ 완료: Restrictions Rolled Back For Nvidia: [Bu...\n",
      "[110] ✅ 완료: College Graduates Facing Blow Of Long-Te...\n",
      "[111] ✅ 완료: A.I.-Generated Images of Child Sexual Ab...\n",
      "[112] ✅ 완료: Whose Jobs Are Doomed In A.I.'s Rise?: [...\n",
      "[113] ✅ 완료: Forget the Food. Check Out the Crowds.: ...\n",
      "[114] ✅ 완료: At Paramount, Big Challenges Await: [Cor...\n",
      "[115] ✅ 완료: Overseas Auto Suppliers Brace for Tariff...\n",
      "[116] ✅ 완료: Right-Wing Squawks Led To American Eagle...\n",
      "[117] ✅ 완료: The Increasing Danger of a Market Melt-U...\n",
      "[118] ✅ 완료: The Revival Of a Start-Up Once Dead: [Bu...\n",
      "[119] ✅ 완료: Fake Papers Found to Be Churned Out At F...\n",
      "[120] ✅ 완료: Israel and Iran Usher In New Era of Psyc...\n",
      "[121] ✅ 완료: A Giant Investment in A.I. Has Yet to Pa...\n",
      "[122] ✅ 완료: How Policy Bill Could Put U.S. Behind Ch...\n",
      "[123] ✅ 완료: The Old Days Are Now Over For Valley Job...\n",
      "[124] ✅ 완료: Under China’s Threat, Taiwan Needs Its O...\n",
      "[125] ✅ 완료: Trump Takes A Lead Role In Business Of A...\n",
      "[126] ✅ 완료: Is A.I. the Future of Web Browsing?: [Bu...\n",
      "[127] ✅ 완료: Trump Has Made Himself Commander in Chie...\n",
      "[128] ✅ 완료: A.I. Firm Will Pay $1.5 Billion To Autho...\n",
      "[129] ✅ 완료: He's Trying to Preserve His Company's St...\n",
      "[130] ✅ 완료: 8 Women, 4 Bedrooms and 1 Cause: Breakin...\n",
      "[131] ✅ 완료: What's in the Future? Nick Foster Has Ti...\n",
      "[132] ✅ 완료: As Trump Courts a More Assertive Beijing...\n",
      "[133] ✅ 완료: A.I. Images Are Out of Their Control: [M...\n",
      "[134] ✅ 완료: The Irrationalist...\n",
      "[135] ✅ 완료: New Tool Allows Websites To Block A.I. D...\n",
      "[136] ✅ 완료: Meta Superintelligence Lab Considers Maj...\n",
      "[137] ✅ 완료: Chatbots Can Go Into a Delusional Spiral...\n",
      "[138] ✅ 완료: Cognition AI Buys Windsurf in Race to Ma...\n",
      "[139] ✅ 완료: Computer Science Courses Require Pivot i...\n",
      "[140] ✅ 완료: How Apple’s iOS 26 and Google’s Android ...\n",
      "[141] ✅ 완료: Nvidia Sales Jump 56%, a Sign the AI Boo...\n",
      "[142] ✅ 완료: The AI Spending Frenzy Is Propping Up th...\n",
      "[143] ✅ 완료: OpenAI and Microsoft Fund A.I. Training ...\n",
      "[144] ✅ 완료: A Teen Was Suicidal. ChatGPT Was the Fri...\n",
      "[145] ✅ 완료: At Amazon’s Biggest Data Center, Everyth...\n",
      "[146] ✅ 완료: How Will David Ellison Tackle the Big Pr...\n",
      "[147] ✅ 완료: AI Is Making Sure You Pay for That Ding ...\n",
      "[148] ✅ 완료: Money Being Poured Into A.I. Is Propping...\n",
      "[149] ✅ 완료: An Opera Takes AI, Pronatalism and Hustl...\n",
      "[150] ✅ 완료: The Chatbot Culture Wars Are Here...\n",
      "[151] ✅ 완료: They Fought Against A.I., Then People St...\n",
      "[152] ✅ 완료: Big Tech's Net-Zero Goals Are Looking Sh...\n",
      "[153] ✅ 완료: Big Tech’s AI Data Centers Are Driving U...\n",
      "[154] ✅ 완료: A.I.-Detection Tools Are on the Lookout ...\n",
      "[155] ✅ 완료: Delta's Plan to Use A.I. To Set Some Pri...\n",
      "[156] ✅ 완료: Silicon Valley Toughens Up For 'Hard Tec...\n",
      "[157] ✅ 완료: Which Workers Will AI Hurt Most: The You...\n",
      "[158] ✅ 완료: The Doctors Are Real, but the Sales Pitc...\n",
      "[159] ✅ 완료: Your Rental Car Get Dinged? You Will, To...\n",
      "[160] ✅ 완료: They’re Stuffed Animals. They’re Also AI...\n",
      "[161] ✅ 완료: A.I.-Fueled Smear Attack Highlights Arge...\n",
      "[162] ✅ 완료: If A.I. Outwits Phones, What's Next?: [B...\n",
      "[163] ✅ 완료: The Doctors Are Real. The Quack Cures Ar...\n",
      "[164] ✅ 완료: Amazon Supersizes Its A.I. Works: [Busin...\n",
      "📄 저장 완료: nyt_content.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "474164cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T04:04:31.910925Z",
     "start_time": "2025-09-21T04:04:31.880557Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('nyt_content.csv')\n",
    "df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164 entries, 0 to 163\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    164 non-null    object\n",
      " 1   link     164 non-null    object\n",
      " 2   date     58 non-null     object\n",
      " 3   media    58 non-null     object\n",
      " 4   content  58 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "13fb1ffd",
   "metadata": {},
   "source": "# 빈 콘텐츠 추가 수집"
  },
  {
   "cell_type": "code",
   "id": "0cd50f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T01:55:40.358525Z",
     "start_time": "2025-09-22T01:55:40.309214Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "# ▶ 기존 결과 불러오기\n",
    "existing_df = pd.read_csv('/home/dslab/choi/Journal/Data/Industry/nyt_content.csv')\n",
    "\n",
    "# ▶ content가 빈 row만 필터링\n",
    "empty_content_df = existing_df[existing_df['content'].isna() | (existing_df['content'].str.strip() == '')]\n",
    "\n",
    "empty_content_df = empty_content_df.head(20)\n",
    "\n",
    "if empty_content_df.empty:\n",
    "    print(\"✅ 모든 content가 이미 수집되어 있습니다.\")\n",
    "else:\n",
    "    print(f\"🔄 수집할 문서 수: {len(empty_content_df)}\")\n",
    "\n",
    "    # ▶ 크롬 옵션 설정 (서버용)\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # 🔥 반드시 필요!\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--disable-plugins')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--remote-debugging-port=9222')\n",
    "    # 고유한 사용자 데이터 디렉토리 설정\n",
    "    options.add_argument(f'--user-data-dir={tempfile.gettempdir()}/chrome-{uuid.uuid4()}')\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        # ▶ 드라이버 실행\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for idx, row in empty_content_df.iterrows():\n",
    "            url = row['link']\n",
    "            title = row['title']\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(3)\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                # ✅ URL에서 docview ID 추출\n",
    "                doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "                doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "                mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "                # ✅ 본문(Content)\n",
    "                content = ''\n",
    "                xpaths_content = [\n",
    "                    f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "                    '//*[@id=\"fullTextZone\"]//p',\n",
    "                    '//*[@id=\"main-content\"]//p',\n",
    "                    '//*[@id=\"companionColumn-0\"]//p',\n",
    "                ]\n",
    "                for xp in xpaths_content:\n",
    "                    if not xp:\n",
    "                        continue\n",
    "                    try:\n",
    "                        paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                        if paragraphs:\n",
    "                            content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "                date_text = row['date']  # 기본값: 기존 값 유지\n",
    "                try:\n",
    "                    strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "                    if strong_tag and strong_tag.next_sibling:\n",
    "                        raw = strong_tag.next_sibling.strip()\n",
    "                        match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)\n",
    "                        if match:\n",
    "                            date_text = match.group()\n",
    "                        else:\n",
    "                            date_text = raw\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # ✅ 미디어(Media)\n",
    "                media = row['media']  # 기본값: 기존 값 유지\n",
    "                xpaths_media = [\n",
    "                    f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "                    '//*[@id=\"bibSource\"]/span',\n",
    "                    '//div[@class=\"publicationTitle\"]',\n",
    "                    '//div[@class=\"bibSource\"]',\n",
    "                    '//span[@class=\"publicationTitle\"]',\n",
    "                ]\n",
    "                for xp in xpaths_media:\n",
    "                    if not xp:\n",
    "                        continue\n",
    "                    try:\n",
    "                        media_el = driver.find_element(By.XPATH, xp)\n",
    "                        media = media_el.text.strip().split(';')[0]\n",
    "                        if media:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                # ✅ 결과 저장\n",
    "                existing_df.loc[idx, 'content'] = content\n",
    "                existing_df.loc[idx, 'date'] = date_text\n",
    "                existing_df.loc[idx, 'media'] = media\n",
    "\n",
    "                print(f\"[{idx+1}] ✅ 업데이트 완료: {title[:40]}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "\n",
    "        # ▶ CSV 덮어쓰기 저장\n",
    "        existing_df.to_csv('/home/dslab/choi/Journal/Data/Industry/nyt_content.csv', index=False, encoding='utf-8-sig')\n",
    "        print(\"📄 저장 완료: nyt_content.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"드라이버 초기화 실패: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # 반드시 드라이버 종료\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"🔄 드라이버 종료 완료\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 content가 이미 수집되어 있습니다.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7a6c1f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T01:55:57.186624Z",
     "start_time": "2025-09-22T01:55:57.151873Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "test=pd.read_csv('/home/dslab/choi/Journal/Data/Industry/nyt_content.csv')\n",
    "test.info() # 158"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164 entries, 0 to 163\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    164 non-null    object\n",
      " 1   link     164 non-null    object\n",
      " 2   date     164 non-null    object\n",
      " 3   media    164 non-null    object\n",
      " 4   content  164 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bac03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Journal",
   "language": "python",
   "name": "journal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
