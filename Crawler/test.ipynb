{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:55:41.149898Z",
     "start_time": "2025-09-13T04:55:24.139231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install habanero backoff requests beautifulsoup4 pandas\n",
    "\n",
    "import re, json, time, pandas as pd, requests\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from habanero import Crossref\n",
    "\n",
    "HDRS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ko;q=0.8\",\n",
    "}\n",
    "\n",
    "def clean(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def clean_abs(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"</?jats:[^>]*>\", \" \", s)     # JATS 제거\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)            # 기타 태그 제거\n",
    "    return clean(s)\n",
    "\n",
    "def fetch_urls_from_crossref(journal=\"Decision Support Systems\", volume=164, year=2023) -> List[Dict]:\n",
    "    \"\"\"Crossref에서 URL, DOI, 제목/초록(폴백용)만 가져오기\"\"\"\n",
    "    cr = Crossref(mailto=\"you@example.com\")\n",
    "    flt = {\"container-title\": journal}\n",
    "    if year:\n",
    "        flt[\"from-pub-date\"] = f\"{year}-01-01\"\n",
    "        flt[\"until-pub-date\"] = f\"{year}-12-31\"\n",
    "\n",
    "    res = cr.works(filter=flt, cursor=\"*\", cursor_max=2000, limit=200)\n",
    "\n",
    "    def items(it):\n",
    "        if isinstance(it, dict):\n",
    "            yield from it.get(\"message\", {}).get(\"items\", [])\n",
    "        else:\n",
    "            for chunk in it:\n",
    "                yield from chunk.get(\"message\", {}).get(\"items\", [])\n",
    "\n",
    "    out = []\n",
    "    for it in items(res):\n",
    "        if str(it.get(\"volume\",\"\")) != str(volume):\n",
    "            continue\n",
    "        url = it.get(\"URL\") or (f\"https://doi.org/{it.get('DOI')}\" if it.get(\"DOI\") else \"\")\n",
    "        out.append({\n",
    "            \"url\": url,\n",
    "            \"doi\": it.get(\"DOI\",\"\"),\n",
    "            \"title_xref\": (it.get(\"title\") or [\"\"])[0],\n",
    "            \"abstract_xref\": clean_abs(it.get(\"abstract\",\"\")),\n",
    "            \"keywords_xref\": \", \".join(it.get(\"subject\", []) or [])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def get_soup(url: str) -> BeautifulSoup | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HDRS, timeout=30, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return None\n",
    "        return BeautifulSoup(r.text, \"html.parser\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_title_abs_kw_from_page(url: str, xref_fallback: Dict) -> Dict:\n",
    "    soup = get_soup(url)\n",
    "    title = abstract = \"\"\n",
    "    keywords: List[str] = []\n",
    "\n",
    "    if soup:\n",
    "        # title\n",
    "        cand_title = [\n",
    "            \"h1\", \"h1.article-title\", \"span.Title\", \"span.headline\",\n",
    "            \"meta[property='og:title']\"\n",
    "        ]\n",
    "        for sel in cand_title:\n",
    "            if sel.startswith(\"meta\"):\n",
    "                el = soup.select_one(sel)\n",
    "                if el and el.get(\"content\"):\n",
    "                    title = clean(el[\"content\"]); break\n",
    "            else:\n",
    "                el = soup.select_one(sel)\n",
    "                if el and clean(el.get_text()):\n",
    "                    title = clean(el.get_text()); break\n",
    "\n",
    "        # abstract\n",
    "        cand_abs = [\n",
    "            \"section#abstract p\", \".article__abstract p\", \"div.abstract p\",\n",
    "            \"div.Abstracts div.abstract\", \"div#abspara p\"\n",
    "        ]\n",
    "        for sel in cand_abs:\n",
    "            el = soup.select_one(sel)\n",
    "            if el and clean(el.get_text()):\n",
    "                abstract = clean(el.get_text()); break\n",
    "        if not abstract:\n",
    "            meta = soup.select_one(\"meta[name='description']\")\n",
    "            if meta and meta.get(\"content\"):\n",
    "                abstract = clean(meta[\"content\"])\n",
    "\n",
    "        # keywords\n",
    "        # 일반 블록\n",
    "        blk = soup.select_one(\"ul.keywords, .keywords, #keywords, section.keywords\")\n",
    "        if blk:\n",
    "            keywords = [clean(x.get_text()) for x in blk.select(\"li, span, a\") if clean(x.get_text())]\n",
    "        # 메타\n",
    "        if not keywords:\n",
    "            meta_kw = soup.select_one(\"meta[name='keywords']\")\n",
    "            if meta_kw and meta_kw.get(\"content\"):\n",
    "                keywords = [clean(x) for x in re.split(r\",|;\", meta_kw[\"content\"]) if clean(x)]\n",
    "        # 스키마(JSON-LD)\n",
    "        if not keywords:\n",
    "            for s in soup.select(\"script[type='application/ld+json']\"):\n",
    "                try:\n",
    "                    data = json.loads(s.string or \"{}\")\n",
    "                    if isinstance(data, dict) and \"keywords\" in data:\n",
    "                        v = data[\"keywords\"]\n",
    "                        if isinstance(v, str):\n",
    "                            keywords = [clean(x) for x in re.split(r\",|;\", v) if clean(x)]\n",
    "                        elif isinstance(v, list):\n",
    "                            keywords = [clean(str(x)) for x in v if clean(str(x))]\n",
    "                        if keywords: break\n",
    "                except:  # 잘못된 JSON 무시\n",
    "                    pass\n",
    "\n",
    "    # 폴백: Crossref 메타\n",
    "    if not title:\n",
    "        title = xref_fallback.get(\"title_xref\",\"\")\n",
    "    if not abstract:\n",
    "        abstract = xref_fallback.get(\"abstract_xref\",\"\")\n",
    "    if not keywords:\n",
    "        kw = xref_fallback.get(\"keywords_xref\",\"\")\n",
    "        keywords = [k.strip() for k in kw.split(\",\") if k.strip()] if kw else []\n",
    "\n",
    "    # 정리\n",
    "    # 중복 제거\n",
    "    seen, dedup = set(), []\n",
    "    for k in keywords:\n",
    "        if k not in seen:\n",
    "            seen.add(k); dedup.append(k)\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": \", \".join(dedup)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    journal = \"Decision Support Systems\"\n",
    "    volume, year = 164, 2023\n",
    "\n",
    "    print(\"1) Crossref에서 URL 수집\")\n",
    "    seeds = fetch_urls_from_crossref(journal, volume, year)\n",
    "    if not seeds:\n",
    "        print(\"Crossref 결과 없음\"); return\n",
    "\n",
    "    # URL만 저장\n",
    "    pd.DataFrame([{\"url\": s[\"url\"], \"doi\": s[\"doi\"]} for s in seeds]).to_csv(\n",
    "        f\"dss_vol{volume}_urls.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"URL 저장 완료 -> dss_vol{volume}_urls.csv (총 {len(seeds)}건)\")\n",
    "\n",
    "    print(\"2) 각 URL에서 title/abstract/keywords 수집\")\n",
    "    rows = []\n",
    "    for i, sx in enumerate(seeds, 1):\n",
    "        row = parse_title_abs_kw_from_page(sx[\"url\"], sx)\n",
    "        rows.append(row)\n",
    "        print(f\"[{i}/{len(seeds)}] {row['title'][:80]}\")\n",
    "        time.sleep(0.3)  # 예의상\n",
    "\n",
    "    pd.DataFrame(rows, columns=[\"title\",\"abstract\",\"keywords\",\"url\"]).to_csv(\n",
    "        f\"dss_vol{volume}_details.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"세부 저장 완료 -> dss_vol{volume}_details.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "e32a8a7cdf7e2c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Crossref에서 URL 수집\n",
      "URL 저장 완료 -> dss_vol164_urls.csv (총 8건)\n",
      "2) 각 URL에서 title/abstract/keywords 수집\n",
      "[1/8] Pay-for-performance schemes and hospital HIT adoption\n",
      "[2/8] CATCHM: A novel network-based credit card fraud detection method using node repr\n",
      "[3/8] Impact of content ideology on social media opinion polarization: The moderating \n",
      "[4/8] Assuring quality and waiting time in real-time spatial crowdsourcing\n",
      "[5/8] IFC/Editorial Board\n",
      "[6/8] The role of web browsing in credit risk prediction\n",
      "[7/8] Exploring the effects of relationship quality and c-commerce behavior on firms' \n",
      "[8/8] A novel label-based multimodal topic model for social media analysis\n",
      "세부 저장 완료 -> dss_vol164_details.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T05:07:20.685531Z",
     "start_time": "2025-09-13T05:07:08.776176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def extract_paper_info(url):\n",
    "    \"\"\"\n",
    "    ScienceDirect 논문에서 제목, 초록, 키워드 추출\n",
    "    \"\"\"\n",
    "    # Chrome 옵션 설정\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # 브라우저 창 숨기기\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "    paper_data = {\n",
    "        'title': '',\n",
    "        'abstract': '',\n",
    "        'keywords': [],\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # WebDriver 실행\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # 페이지 로딩 대기\n",
    "\n",
    "        # HTML 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 제목 추출 - #screen-reader-main-title > span\n",
    "        title_element = soup.select_one('#screen-reader-main-title > span')\n",
    "        if title_element:\n",
    "            paper_data['title'] = title_element.get_text().strip()\n",
    "            print(f\"✓ 제목 추출 성공: {paper_data['title'][:50]}...\")\n",
    "        else:\n",
    "            print(\"✗ 제목 추출 실패\")\n",
    "\n",
    "        # 초록 추출 - #sp0055\n",
    "        abstract_element = soup.select_one('#sp0055')\n",
    "        if abstract_element:\n",
    "            paper_data['abstract'] = abstract_element.get_text().strip()\n",
    "            print(f\"✓ 초록 추출 성공: {len(paper_data['abstract'])}자\")\n",
    "        else:\n",
    "            print(\"✗ 초록 추출 실패\")\n",
    "\n",
    "        # 키워드 추출 - #ks0005 div\n",
    "        keyword_elements = soup.select('#ks0005 div')\n",
    "        if keyword_elements:\n",
    "            keywords = []\n",
    "            for element in keyword_elements:\n",
    "                keyword_text = element.get_text().strip()\n",
    "                # 빈 텍스트나 \"Keywords\" 라벨 제외\n",
    "                if keyword_text and keyword_text.lower() not in ['keywords', 'keyword']:\n",
    "                    keywords.append(keyword_text)\n",
    "\n",
    "            paper_data['keywords'] = keywords\n",
    "            print(f\"✓ 키워드 추출 성공: {len(keywords)}개\")\n",
    "            for i, keyword in enumerate(keywords):\n",
    "                print(f\"  {i+1}. {keyword}\")\n",
    "        else:\n",
    "            print(\"✗ 키워드 추출 실패\")\n",
    "\n",
    "        driver.quit()\n",
    "        return paper_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "        return paper_data\n",
    "\n",
    "def save_data(data, filename='paper_data'):\n",
    "    \"\"\"데이터를 JSON과 텍스트 파일로 저장\"\"\"\n",
    "\n",
    "    # JSON 저장\n",
    "    with open(f'{filename}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 읽기 쉬운 텍스트 파일로 저장\n",
    "    with open(f'{filename}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"논문 정보\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"URL: {data['url']}\\n\\n\")\n",
    "\n",
    "        f.write(\"제목 (Title):\\n\")\n",
    "        f.write(f\"{data['title']}\\n\\n\")\n",
    "\n",
    "        f.write(\"키워드 (Keywords):\\n\")\n",
    "        if data['keywords']:\n",
    "            for i, keyword in enumerate(data['keywords']):\n",
    "                f.write(f\"{i+1}. {keyword}\\n\")\n",
    "        else:\n",
    "            f.write(\"키워드 없음\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"초록 (Abstract):\\n\")\n",
    "        f.write(f\"{data['abstract']}\\n\")\n",
    "        f.write(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    print(f\"\\n데이터가 {filename}.json과 {filename}.txt에 저장되었습니다.\")\n",
    "\n",
    "def main():\n",
    "    url = \"https://www.sciencedirect.com/science/article/pii/S0167923622001166\"\n",
    "\n",
    "    print(\"ScienceDirect 논문 정보 추출 시작...\")\n",
    "    print(f\"URL: {url}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 데이터 추출\n",
    "    paper_data = extract_paper_info(url)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"추출 결과:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if paper_data['title']:\n",
    "        print(f\"제목: {paper_data['title']}\")\n",
    "\n",
    "    if paper_data['keywords']:\n",
    "        print(f\"\\n키워드 ({len(paper_data['keywords'])}개):\")\n",
    "        for i, keyword in enumerate(paper_data['keywords']):\n",
    "            print(f\"  {i+1}. {keyword}\")\n",
    "\n",
    "    if paper_data['abstract']:\n",
    "        print(f\"\\n초록 (길이: {len(paper_data['abstract'])}자):\")\n",
    "        # 초록이 너무 길면 일부만 출력\n",
    "        if len(paper_data['abstract']) > 200:\n",
    "            print(f\"{paper_data['abstract'][:200]}...\")\n",
    "        else:\n",
    "            print(paper_data['abstract'])\n",
    "\n",
    "    # 파일로 저장\n",
    "    if any([paper_data['title'], paper_data['abstract'], paper_data['keywords']]):\n",
    "        save_data(paper_data, 'sciencedirect_paper')\n",
    "    else:\n",
    "        print(\"\\n추출된 데이터가 없어 파일을 저장하지 않습니다.\")\n",
    "        print(\"\\n문제 해결 방법:\")\n",
    "        print(\"1. Chrome WebDriver가 올바르게 설치되었는지 확인\")\n",
    "        print(\"2. 인터넷 연결 확인\")\n",
    "        print(\"3. VPN 사용 또는 대학 네트워크에서 실행\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "5ae91f1f5514d1c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScienceDirect 논문 정보 추출 시작...\n",
      "URL: https://www.sciencedirect.com/science/article/pii/S0167923622001166\n",
      "--------------------------------------------------\n",
      "✓ 제목 추출 성공: Impact of content ideology on social media opinion...\n",
      "✓ 초록 추출 성공: 922자\n",
      "✓ 키워드 추출 성공: 6개\n",
      "  1. Social media\n",
      "  2. Opinion polarization\n",
      "  3. Sentiment analysis\n",
      "  4. Ideology\n",
      "  5. Functional affordance\n",
      "  6. Symbolic expression\n",
      "\n",
      "==================================================\n",
      "추출 결과:\n",
      "==================================================\n",
      "제목: Impact of content ideology on social media opinion polarization: The moderating role of functional affordances and symbolic expressions\n",
      "\n",
      "키워드 (6개):\n",
      "  1. Social media\n",
      "  2. Opinion polarization\n",
      "  3. Sentiment analysis\n",
      "  4. Ideology\n",
      "  5. Functional affordance\n",
      "  6. Symbolic expression\n",
      "\n",
      "초록 (길이: 922자):\n",
      "We offer theory and evidence regarding the impact of content ideology (i.e., emotionally charged beliefs expressed in sentiments) on opinion polarization (i.e., conflicting attitudes about an event) o...\n",
      "\n",
      "데이터가 sciencedirect_paper.json과 sciencedirect_paper.txt에 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T05:24:26.995634Z",
     "start_time": "2025-09-13T05:24:20.759193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install habanero backoff requests beautifulsoup4 pandas selenium\n",
    "\n",
    "import re, json, time, pandas as pd, requests\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from habanero import Crossref\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "HDRS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ko;q=0.8\",\n",
    "}\n",
    "\n",
    "def clean(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def clean_abs(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"</?jats:[^>]*>\", \" \", s)     # JATS 제거\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)            # 기타 태그 제거\n",
    "    return clean(s)\n",
    "\n",
    "class IntegratedPaperScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.setup_selenium()\n",
    "\n",
    "    def setup_selenium(self):\n",
    "        \"\"\"Selenium WebDriver 설정\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # 브라우저 창 숨기기\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chrome driver 설정 실패: {e}\")\n",
    "            self.driver = None\n",
    "\n",
    "    def fetch_urls_from_crossref(self, journal=\"Decision Support Systems\", volume=164, year=2023) -> List[Dict]:\n",
    "        \"\"\"Crossref에서 URL, DOI, 제목/초록(폴백용) 가져오기 (타임아웃 및 재시도 처리)\"\"\"\n",
    "        print(f\"Crossref에서 {journal} Vol.{volume} ({year}) 논문 검색 중...\")\n",
    "\n",
    "        # 여러 전략으로 시도\n",
    "        strategies = [\n",
    "            # 전략 1: 기본 검색\n",
    "            {\"filter\": {\"container-title\": journal, \"from-pub-date\": f\"{year}-01-01\", \"until-pub-date\": f\"{year}-12-31\"}},\n",
    "            # 전략 2: DOI 패턴으로 검색 (ScienceDirect는 보통 10.1016으로 시작)\n",
    "            {\"query\": f'\"{journal}\" volume:{volume} {year}'},\n",
    "            # 전략 3: 간단한 쿼리\n",
    "            {\"query\": f\"Decision Support Systems {year}\"}\n",
    "        ]\n",
    "\n",
    "        out = []\n",
    "\n",
    "        for i, strategy in enumerate(strategies, 1):\n",
    "            try:\n",
    "                print(f\"  시도 {i}/{len(strategies)}: \", end=\"\")\n",
    "                cr = Crossref(mailto=\"researcher@example.com\", timeout=30)\n",
    "\n",
    "                if \"filter\" in strategy:\n",
    "                    print(\"필터 방식\")\n",
    "                    res = cr.works(filter=strategy[\"filter\"], limit=100, timeout=30)\n",
    "                else:\n",
    "                    print(\"쿼리 방식\")\n",
    "                    res = cr.works(query=strategy[\"query\"], limit=100, timeout=30)\n",
    "\n",
    "                # 결과 처리\n",
    "                items = res.get(\"message\", {}).get(\"items\", []) if isinstance(res, dict) else []\n",
    "\n",
    "                for it in items:\n",
    "                    # 볼륨과 저널명 확인 (더 유연하게)\n",
    "                    item_journal = \"\"\n",
    "                    if it.get(\"container-title\"):\n",
    "                        item_journal = it[\"container-title\"][0] if isinstance(it[\"container-title\"], list) else str(it[\"container-title\"])\n",
    "\n",
    "                    item_volume = str(it.get(\"volume\", \"\"))\n",
    "\n",
    "                    # 저널명과 볼륨 매칭 (부분 매칭 허용)\n",
    "                    journal_match = \"decision support\" in item_journal.lower() and \"systems\" in item_journal.lower()\n",
    "                    volume_match = item_volume == str(volume) or not item_volume  # 볼륨 정보가 없으면 일단 포함\n",
    "\n",
    "                    if not journal_match:\n",
    "                        continue\n",
    "\n",
    "                    url = it.get(\"URL\") or (f\"https://doi.org/{it.get('DOI')}\" if it.get(\"DOI\") else \"\")\n",
    "                    if not url:\n",
    "                        continue\n",
    "\n",
    "                    # 중복 제거 (DOI 기준)\n",
    "                    doi = it.get(\"DOI\", \"\")\n",
    "                    if doi and any(existing.get(\"doi\") == doi for existing in out):\n",
    "                        continue\n",
    "\n",
    "                    out.append({\n",
    "                        \"url\": url,\n",
    "                        \"doi\": doi,\n",
    "                        \"title_xref\": (it.get(\"title\") or [\"\"])[0],\n",
    "                        \"abstract_xref\": clean_abs(it.get(\"abstract\",\"\")),\n",
    "                        \"keywords_xref\": \", \".join(it.get(\"subject\", []) or []),\n",
    "                        \"authors\": self.extract_authors(it.get(\"author\", [])),\n",
    "                        \"year\": self.extract_year(it.get(\"published-print\", it.get(\"published-online\"))),\n",
    "                        \"volume\": item_volume,\n",
    "                        \"journal\": item_journal\n",
    "                    })\n",
    "\n",
    "                if out:\n",
    "                    print(f\"    -> {len(out)}개 논문 발견\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"    -> 결과 없음\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    -> 오류: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 볼륨으로 한번 더 필터링\n",
    "        if volume and out:\n",
    "            filtered_out = [item for item in out if str(item.get(\"volume\", \"\")) == str(volume)]\n",
    "            if filtered_out:\n",
    "                out = filtered_out\n",
    "                print(f\"볼륨 {volume} 필터링 후: {len(out)}개\")\n",
    "\n",
    "        if not out:\n",
    "            print(\"⚠️  Crossref에서 논문을 찾을 수 없습니다. 대안 방법을 시도합니다...\")\n",
    "            # 대안: 알려진 DOI 패턴으로 URL 생성\n",
    "            out = self.generate_fallback_urls(journal, volume, year)\n",
    "\n",
    "        print(f\"최종 수집: {len(out)}개 논문\")\n",
    "        return out\n",
    "\n",
    "    def generate_fallback_urls(self, journal, volume, year) -> List[Dict]:\n",
    "        \"\"\"CrossRef 실패 시 대안 URL 생성\"\"\"\n",
    "        print(\"대안 방법: 알려진 논문 패턴으로 URL 생성 중...\")\n",
    "\n",
    "        # ScienceDirect Decision Support Systems의 일반적인 DOI 패턴\n",
    "        # 실제로는 이 방법보다는 저널 홈페이지를 크롤링하는 것이 좋습니다\n",
    "        fallback_urls = [\n",
    "            \"https://www.sciencedirect.com/journal/decision-support-systems/vol/164/suppl/C\"  # 볼륨 페이지\n",
    "        ]\n",
    "\n",
    "        out = []\n",
    "        for i, url in enumerate(fallback_urls):\n",
    "            out.append({\n",
    "                \"url\": url,\n",
    "                \"doi\": f\"fallback_{i}\",\n",
    "                \"title_xref\": f\"Fallback Paper {i+1}\",\n",
    "                \"abstract_xref\": \"\",\n",
    "                \"keywords_xref\": \"\",\n",
    "                \"authors\": \"\",\n",
    "                \"year\": str(year),\n",
    "                \"volume\": str(volume),\n",
    "                \"journal\": journal\n",
    "            })\n",
    "\n",
    "        return out\n",
    "\n",
    "    def extract_authors(self, authors_list: List[Dict]) -> str:\n",
    "        \"\"\"저자 정보 추출 및 포맷팅\"\"\"\n",
    "        if not authors_list:\n",
    "            return ''\n",
    "\n",
    "        author_names = []\n",
    "        for author in authors_list:\n",
    "            given = author.get('given', '')\n",
    "            family = author.get('family', '')\n",
    "            if given and family:\n",
    "                author_names.append(f\"{given} {family}\")\n",
    "            elif family:\n",
    "                author_names.append(family)\n",
    "\n",
    "        return ', '.join(author_names)\n",
    "\n",
    "    def extract_year(self, date_info) -> str:\n",
    "        \"\"\"발행년도 추출\"\"\"\n",
    "        if not date_info:\n",
    "            return ''\n",
    "\n",
    "        if isinstance(date_info, dict):\n",
    "            date_parts = date_info.get('date-parts', [[]])[0]\n",
    "            if date_parts:\n",
    "                return str(date_parts[0])\n",
    "\n",
    "        return ''\n",
    "\n",
    "    def extract_paper_details_selenium(self, url: str, xref_fallback: Dict) -> Dict:\n",
    "        \"\"\"Selenium을 사용하여 ScienceDirect에서 상세 정보 추출\"\"\"\n",
    "        paper_data = {\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'abstract': '',\n",
    "            'keywords': [],\n",
    "            'authors': xref_fallback.get('authors', ''),\n",
    "            'year': xref_fallback.get('year', ''),\n",
    "            'doi': xref_fallback.get('doi', '')\n",
    "        }\n",
    "\n",
    "        if not self.driver:\n",
    "            # Selenium이 없으면 requests로 시도\n",
    "            return self.extract_paper_details_requests(url, xref_fallback)\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(3)  # 페이지 로딩 대기\n",
    "\n",
    "            # HTML 파싱\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "            # 제목 추출 - ScienceDirect 특화 셀렉터 우선\n",
    "            title_element = soup.select_one('#screen-reader-main-title > span')\n",
    "            if title_element:\n",
    "                paper_data['title'] = title_element.get_text().strip()\n",
    "            else:\n",
    "                # 백업 셀렉터들\n",
    "                title_selectors = [\n",
    "                    'h1', 'h1.article-title', 'span.Title', 'span.headline',\n",
    "                    'meta[property=\"og:title\"]', 'meta[name=\"citation_title\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in title_selectors:\n",
    "                    if selector.startswith('meta'):\n",
    "                        el = soup.select_one(selector)\n",
    "                        if el and el.get(\"content\"):\n",
    "                            paper_data['title'] = clean(el[\"content\"])\n",
    "                            break\n",
    "                    else:\n",
    "                        el = soup.select_one(selector)\n",
    "                        if el and clean(el.get_text()):\n",
    "                            paper_data['title'] = clean(el.get_text())\n",
    "                            break\n",
    "\n",
    "            # 초록 추출 - ScienceDirect 특화 셀렉터 우선\n",
    "            abstract_element = soup.select_one('#sp0055')\n",
    "            if abstract_element:\n",
    "                paper_data['abstract'] = abstract_element.get_text().strip()\n",
    "            else:\n",
    "                # 백업 셀렉터들\n",
    "                abstract_selectors = [\n",
    "                    \"section#abstract p\", \".article__abstract p\", \"div.abstract p\",\n",
    "                    \"div.Abstracts div.abstract\", \"div#abspara p\", \"#abstract\",\n",
    "                    \".abstract\", '[data-testid=\"abstract\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in abstract_selectors:\n",
    "                    el = soup.select_one(selector)\n",
    "                    if el and clean(el.get_text()):\n",
    "                        paper_data['abstract'] = clean(el.get_text())\n",
    "                        break\n",
    "\n",
    "            # 키워드 추출 - ScienceDirect 특화 셀렉터 우선\n",
    "            keyword_elements = soup.select('#ks0005 div')\n",
    "            if keyword_elements:\n",
    "                keywords = []\n",
    "                for element in keyword_elements:\n",
    "                    keyword_text = element.get_text().strip()\n",
    "                    if keyword_text and keyword_text.lower() not in ['keywords', 'keyword']:\n",
    "                        keywords.append(keyword_text)\n",
    "                paper_data['keywords'] = keywords\n",
    "            else:\n",
    "                # 백업 키워드 추출\n",
    "                keywords = self.extract_keywords_fallback(soup)\n",
    "                paper_data['keywords'] = keywords\n",
    "\n",
    "            return paper_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Selenium 추출 중 오류 (URL: {url}): {e}\")\n",
    "            # 오류 시 requests로 대체\n",
    "            return self.extract_paper_details_requests(url, xref_fallback)\n",
    "\n",
    "    def extract_paper_details_requests(self, url: str, xref_fallback: Dict) -> Dict:\n",
    "        \"\"\"requests를 사용하여 논문 상세 정보 추출 (백업)\"\"\"\n",
    "        paper_data = {\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'abstract': '',\n",
    "            'keywords': [],\n",
    "            'authors': xref_fallback.get('authors', ''),\n",
    "            'year': xref_fallback.get('year', ''),\n",
    "            'doi': xref_fallback.get('doi', '')\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=HDRS, timeout=30, allow_redirects=True)\n",
    "            if response.status_code >= 400:\n",
    "                # Crossref 폴백 데이터 사용\n",
    "                paper_data.update(self.use_crossref_fallback(xref_fallback))\n",
    "                return paper_data\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # 제목 추출\n",
    "            title_selectors = [\n",
    "                \"h1\", \"h1.article-title\", \"span.Title\", \"span.headline\",\n",
    "                \"meta[property='og:title']\", \"meta[name='citation_title']\"\n",
    "            ]\n",
    "\n",
    "            for selector in title_selectors:\n",
    "                if selector.startswith(\"meta\"):\n",
    "                    el = soup.select_one(selector)\n",
    "                    if el and el.get(\"content\"):\n",
    "                        paper_data['title'] = clean(el[\"content\"])\n",
    "                        break\n",
    "                else:\n",
    "                    el = soup.select_one(selector)\n",
    "                    if el and clean(el.get_text()):\n",
    "                        paper_data['title'] = clean(el.get_text())\n",
    "                        break\n",
    "\n",
    "            # 초록 추출\n",
    "            abstract_selectors = [\n",
    "                \"section#abstract p\", \".article__abstract p\", \"div.abstract p\",\n",
    "                \"div.Abstracts div.abstract\", \"div#abspara p\"\n",
    "            ]\n",
    "\n",
    "            for selector in abstract_selectors:\n",
    "                el = soup.select_one(selector)\n",
    "                if el and clean(el.get_text()):\n",
    "                    paper_data['abstract'] = clean(el.get_text())\n",
    "                    break\n",
    "\n",
    "            if not paper_data['abstract']:\n",
    "                meta = soup.select_one(\"meta[name='description']\")\n",
    "                if meta and meta.get(\"content\"):\n",
    "                    paper_data['abstract'] = clean(meta[\"content\"])\n",
    "\n",
    "            # 키워드 추출\n",
    "            paper_data['keywords'] = self.extract_keywords_fallback(soup)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Requests 추출 중 오류 (URL: {url}): {e}\")\n",
    "\n",
    "        # 폴백: Crossref 메타 사용\n",
    "        if not paper_data['title']:\n",
    "            paper_data['title'] = xref_fallback.get(\"title_xref\", \"\")\n",
    "        if not paper_data['abstract']:\n",
    "            paper_data['abstract'] = xref_fallback.get(\"abstract_xref\", \"\")\n",
    "        if not paper_data['keywords']:\n",
    "            kw = xref_fallback.get(\"keywords_xref\", \"\")\n",
    "            paper_data['keywords'] = [k.strip() for k in kw.split(\",\") if k.strip()] if kw else []\n",
    "\n",
    "        return paper_data\n",
    "\n",
    "    def extract_keywords_fallback(self, soup) -> List[str]:\n",
    "        \"\"\"키워드 추출 백업 메소드\"\"\"\n",
    "        keywords = []\n",
    "\n",
    "        # 일반 블록에서 추출\n",
    "        blk = soup.select_one(\"ul.keywords, .keywords, #keywords, section.keywords\")\n",
    "        if blk:\n",
    "            keywords = [clean(x.get_text()) for x in blk.select(\"li, span, a\") if clean(x.get_text())]\n",
    "\n",
    "        # 메타 태그에서 추출\n",
    "        if not keywords:\n",
    "            meta_kw = soup.select_one(\"meta[name='keywords']\")\n",
    "            if meta_kw and meta_kw.get(\"content\"):\n",
    "                keywords = [clean(x) for x in re.split(r\",|;\", meta_kw[\"content\"]) if clean(x)]\n",
    "\n",
    "        # JSON-LD 스키마에서 추출\n",
    "        if not keywords:\n",
    "            for s in soup.select(\"script[type='application/ld+json']\"):\n",
    "                try:\n",
    "                    data = json.loads(s.string or \"{}\")\n",
    "                    if isinstance(data, dict) and \"keywords\" in data:\n",
    "                        v = data[\"keywords\"]\n",
    "                        if isinstance(v, str):\n",
    "                            keywords = [clean(x) for x in re.split(r\",|;\", v) if clean(x)]\n",
    "                        elif isinstance(v, list):\n",
    "                            keywords = [clean(str(x)) for x in v if clean(str(x))]\n",
    "                        if keywords:\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # 중복 제거\n",
    "        seen, dedup = set(), []\n",
    "        for k in keywords:\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                dedup.append(k)\n",
    "\n",
    "        return dedup\n",
    "\n",
    "    def use_crossref_fallback(self, xref_fallback: Dict) -> Dict:\n",
    "        \"\"\"Crossref 폴백 데이터 사용\"\"\"\n",
    "        return {\n",
    "            'title': xref_fallback.get(\"title_xref\", \"\"),\n",
    "            'abstract': xref_fallback.get(\"abstract_xref\", \"\"),\n",
    "            'keywords': [k.strip() for k in xref_fallback.get(\"keywords_xref\", \"\").split(\",\") if k.strip()]\n",
    "        }\n",
    "\n",
    "    def save_results(self, results: List[Dict], filename_prefix: str):\n",
    "        \"\"\"결과를 CSV와 JSON으로 저장\"\"\"\n",
    "        if not results:\n",
    "            print(\"저장할 데이터가 없습니다.\")\n",
    "            return\n",
    "\n",
    "        # DataFrame 생성\n",
    "        df_data = []\n",
    "        for result in results:\n",
    "            df_data.append({\n",
    "                'title': result.get('title', ''),\n",
    "                'authors': result.get('authors', ''),\n",
    "                'year': result.get('year', ''),\n",
    "                'abstract': result.get('abstract', ''),\n",
    "                'keywords': ', '.join(result.get('keywords', [])) if isinstance(result.get('keywords'), list) else result.get('keywords', ''),\n",
    "                'doi': result.get('doi', ''),\n",
    "                'url': result.get('url', '')\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "\n",
    "        # CSV 저장\n",
    "        csv_filename = f\"{filename_prefix}.csv\"\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # JSON 저장\n",
    "        json_filename = f\"{filename_prefix}.json\"\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"결과 저장 완료:\")\n",
    "        print(f\"  - CSV: {csv_filename}\")\n",
    "        print(f\"  - JSON: {json_filename}\")\n",
    "        print(f\"  - 총 {len(results)}개 논문\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"리소스 정리\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "def main():\n",
    "    # 설정\n",
    "    journal = \"Decision Support Systems\"\n",
    "    volume = 164\n",
    "    year = 2023\n",
    "\n",
    "    scraper = IntegratedPaperScraper()\n",
    "\n",
    "    try:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"논문 정보 수집 시작: {journal} Vol.{volume} ({year})\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # 1단계: Crossref에서 URL 수집\n",
    "        seeds = scraper.fetch_urls_from_crossref(journal, volume, year)\n",
    "\n",
    "        if not seeds:\n",
    "            print(\"Crossref에서 논문을 찾을 수 없습니다.\")\n",
    "            return\n",
    "\n",
    "        # URL 목록만 먼저 저장\n",
    "        url_data = [{\"url\": s[\"url\"], \"doi\": s[\"doi\"], \"title_crossref\": s[\"title_xref\"]} for s in seeds]\n",
    "        pd.DataFrame(url_data).to_csv(f\"dss_vol{volume}_urls.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"URL 목록 저장: dss_vol{volume}_urls.csv ({len(seeds)}개)\")\n",
    "\n",
    "        # 2단계: 각 논문 페이지에서 상세 정보 추출\n",
    "        print(f\"\\n각 논문 페이지에서 상세 정보 추출 중...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        results = []\n",
    "        failed_count = 0\n",
    "\n",
    "        for i, seed in enumerate(seeds, 1):\n",
    "            print(f\"[{i}/{len(seeds)}] 처리 중...\")\n",
    "\n",
    "            try:\n",
    "                result = scraper.extract_paper_details_selenium(seed[\"url\"], seed)\n",
    "                results.append(result)\n",
    "\n",
    "                # 진행상황 출력\n",
    "                title = result.get('title', 'No title')[:60]\n",
    "                keywords_count = len(result.get('keywords', []))\n",
    "                abstract_length = len(result.get('abstract', ''))\n",
    "\n",
    "                print(f\"  ✓ {title}\")\n",
    "                print(f\"    키워드: {keywords_count}개, 초록: {abstract_length}자\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ 오류: {e}\")\n",
    "                failed_count += 1\n",
    "                # 오류가 있어도 기본 정보는 저장\n",
    "                results.append({\n",
    "                    'url': seed[\"url\"],\n",
    "                    'title': seed.get(\"title_xref\", \"\"),\n",
    "                    'abstract': seed.get(\"abstract_xref\", \"\"),\n",
    "                    'keywords': [],\n",
    "                    'authors': seed.get('authors', ''),\n",
    "                    'year': seed.get('year', ''),\n",
    "                    'doi': seed.get('doi', '')\n",
    "                })\n",
    "\n",
    "            # 예의상 잠시 대기\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # 3단계: 결과 저장\n",
    "        print(f\"\\n\" + \"=\" * 50)\n",
    "        print(\"수집 완료!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"성공: {len(seeds) - failed_count}개\")\n",
    "        print(f\"실패: {failed_count}개\")\n",
    "\n",
    "        scraper.save_results(results, f\"dss_vol{volume}_complete\")\n",
    "\n",
    "        # 간단한 통계\n",
    "        titles_with_content = sum(1 for r in results if r.get('title'))\n",
    "        abstracts_with_content = sum(1 for r in results if r.get('abstract'))\n",
    "        keywords_with_content = sum(1 for r in results if r.get('keywords'))\n",
    "\n",
    "        print(f\"\\n수집 통계:\")\n",
    "        print(f\"  - 제목 수집: {titles_with_content}/{len(results)}\")\n",
    "        print(f\"  - 초록 수집: {abstracts_with_content}/{len(results)}\")\n",
    "        print(f\"  - 키워드 수집: {keywords_with_content}/{len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"전체 프로세스 오류: {e}\")\n",
    "\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "7daf78abb899d1d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "논문 정보 수집 시작: Decision Support Systems Vol.164 (2023)\n",
      "============================================================\n",
      "Crossref에서 Decision Support Systems Vol.164 (2023) 논문 검색 중...\n",
      "  시도 1/3: 필터 방식\n",
      "    -> 오류: Request.__init__() got multiple values for argument 'timeout'\n",
      "  시도 2/3: 쿼리 방식\n",
      "    -> 오류: Request.__init__() got multiple values for argument 'timeout'\n",
      "  시도 3/3: 쿼리 방식\n",
      "    -> 오류: Request.__init__() got multiple values for argument 'timeout'\n",
      "⚠️  Crossref에서 논문을 찾을 수 없습니다. 대안 방법을 시도합니다...\n",
      "대안 방법: 알려진 논문 패턴으로 URL 생성 중...\n",
      "최종 수집: 1개 논문\n",
      "URL 목록 저장: dss_vol164_urls.csv (1개)\n",
      "\n",
      "각 논문 페이지에서 상세 정보 추출 중...\n",
      "--------------------------------------------------\n",
      "[1/1] 처리 중...\n",
      "  ✓ Decision Support Systems\n",
      "    키워드: 0개, 초록: 0자\n",
      "\n",
      "==================================================\n",
      "수집 완료!\n",
      "==================================================\n",
      "성공: 1개\n",
      "실패: 0개\n",
      "결과 저장 완료:\n",
      "  - CSV: dss_vol164_complete.csv\n",
      "  - JSON: dss_vol164_complete.json\n",
      "  - 총 1개 논문\n",
      "\n",
      "수집 통계:\n",
      "  - 제목 수집: 1/1\n",
      "  - 초록 수집: 0/1\n",
      "  - 키워드 수집: 0/1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5e2ee2b6e93eca6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
