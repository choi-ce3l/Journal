{
 "cells": [
  {
   "cell_type": "code",
   "id": "67c6c150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T03:49:28.975213Z",
     "start_time": "2025-09-21T03:45:22.171781Z"
    }
   },
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")  # 로그인해야 하므로 꺼둠\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 로그인 및 필터 후 검색 페이지 수동 접속\n",
    "print(\"🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\")\n",
    "driver.get(\"https://www.proquest.com/\")\n",
    "input(\"✅ 필터 적용 완료 후 Enter를 눌러주세요...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "MAX_PAGES = 80  # 원하는 페이지 수만큼 반복\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.select('div.resultHeader div h3 a')\n",
    "\n",
    "    print(f\"📄 페이지 {page}에서 {len(links)}개 링크 수집\")\n",
    "\n",
    "    for link in links:\n",
    "        title = link.get_text(strip=True)\n",
    "        href = link.get('href')\n",
    "        full_url = \"https://www.proquest.com\" + href if href.startswith(\"/\") else href\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': full_url\n",
    "        })\n",
    "\n",
    "    # 다음 페이지로 이동 시도\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//*[@id=\"updateForm\"]/nav/ul/li[9]/a')\n",
    "        if next_button.is_enabled():\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            print(\"➡️ 다음 페이지로 이동...\")\n",
    "            time.sleep(4)\n",
    "        else:\n",
    "            print(\"🚫 다음 페이지 버튼이 비활성화되어 있습니다.\")\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        print(\"❌ 다음 페이지 버튼을 찾을 수 없습니다.\")\n",
    "        break\n",
    "    except ElementClickInterceptedException:\n",
    "        print(\"⚠️ 버튼 클릭 실패. 스크롤 후 재시도\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# CSV 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"proquest_wallstreet.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 완료: 총 {len(df)}개의 항목을 저장했습니다.\")\n",
    "driver.quit()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Academia-Industry-gap-analysis/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 브라우저가 열립니다. ProQuest에 로그인하고, 필터와 검색어 설정 후 원하는 검색결과 페이지로 이동하세요.\n",
      "📄 페이지 1에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 2에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 3에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 4에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 5에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 6에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 7에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 8에서 20개 링크 수집\n",
      "➡️ 다음 페이지로 이동...\n",
      "📄 페이지 9에서 4개 링크 수집\n",
      "❌ 다음 페이지 버튼을 찾을 수 없습니다.\n",
      "✅ 완료: 총 164개의 항목을 저장했습니다.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2d312dfa",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-21T03:49:49.399204Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "options = Options()\n",
    "# options.add_argument('--headless')  # 필요 시 주석 해제\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "\n",
    "# 드라이버 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv('proquest_wallstreet.csv')\n",
    "# df = df.head(20)  # 테스트로 일부만 실행\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['link']\n",
    "    title = row['title']\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # ✅ URL에서 docview ID 추출\n",
    "        doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "        doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "        mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "        # ✅ 본문(Content)\n",
    "        content = ''\n",
    "        xpaths_content = [\n",
    "            f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "            '//*[@id=\"fullTextZone\"]//p',\n",
    "            '//*[@id=\"main-content\"]//p',\n",
    "            '//*[@id=\"companionColumn-0\"]//p',\n",
    "        ]\n",
    "        for xp in xpaths_content:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                if paragraphs:\n",
    "                    content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "        date_text = ''\n",
    "        try:\n",
    "            strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "            if strong_tag and strong_tag.next_sibling:\n",
    "                raw = strong_tag.next_sibling.strip()\n",
    "                match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)  # \"04 Feb 2025\"\n",
    "                if match:\n",
    "                    date_text = match.group()\n",
    "                else:\n",
    "                    date_text = raw\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ✅ 미디어(Media)\n",
    "        media = ''\n",
    "        xpaths_media = [\n",
    "            f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "            '//*[@id=\"bibSource\"]/span',\n",
    "            '//div[@class=\"publicationTitle\"]',\n",
    "            '//div[@class=\"bibSource\"]',\n",
    "            '//span[@class=\"publicationTitle\"]',\n",
    "        ]\n",
    "        for xp in xpaths_media:\n",
    "            if not xp:\n",
    "                continue\n",
    "            try:\n",
    "                media_el = driver.find_element(By.XPATH, xp)\n",
    "                media = media_el.text.strip().split(';')[0]\n",
    "                if media:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': date_text,\n",
    "            'media': media,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "        print(f\"[{idx+1}] ✅ 완료: {title[:40]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'link': url,\n",
    "            'date': '',\n",
    "            'media': '',\n",
    "            'content': ''\n",
    "        })\n",
    "\n",
    "# 저장\n",
    "output_df = pd.DataFrame(results, columns=['title', 'link', 'date', 'media', 'content'])\n",
    "output_df.to_csv('wallstreet_content.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"📄 저장 완료: wallstreet_content.csv\")\n",
    "\n",
    "driver.quit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d62b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>media</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- H...</td>\n",
       "      <td>https://www.proquest.com/docview/3132486081/2B...</td>\n",
       "      <td>25 Nov 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>The current generation of college students is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- T...</td>\n",
       "      <td>https://www.proquest.com/docview/3132486071/2B...</td>\n",
       "      <td>25 Nov 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>ChatGPT is barely two years old. And yet it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- F...</td>\n",
       "      <td>https://www.proquest.com/docview/3162691166/2B...</td>\n",
       "      <td>03 Feb 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>The race for AI dominance launched a stampede ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crunchbase UsesArtificialIntelligenceTo Predic...</td>\n",
       "      <td>https://www.proquest.com/docview/3168537629/2B...</td>\n",
       "      <td>20 Feb 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Crunchbase, the firm best known for its startu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the Clock: Bosses' Mental Fitness Set for A...</td>\n",
       "      <td>https://www.proquest.com/docview/3096788378/2B...</td>\n",
       "      <td>26 Aug 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Bosses already live in fear that a verbal miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EXCHANGE --- The Rundown on Building an AI Sup...</td>\n",
       "      <td>https://www.proquest.com/docview/3191507034/2B...</td>\n",
       "      <td>19 Apr 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>President Trump enthused on social media this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- H...</td>\n",
       "      <td>https://www.proquest.com/docview/3132486040/2B...</td>\n",
       "      <td>25 Nov 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Artificial intelligence is poised to transform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- H...</td>\n",
       "      <td>https://www.proquest.com/docview/3162691094/2B...</td>\n",
       "      <td>03 Feb 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>You can't stop an AI chatbot from sometimes ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Future of Everything: TheArtificialIntelli...</td>\n",
       "      <td>https://www.proquest.com/docview/3171573768/2B...</td>\n",
       "      <td>27 Feb 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>In a large, brightly-lit warehouse in Flowery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Future of Everything: TheArtificialIntelli...</td>\n",
       "      <td>https://www.proquest.com/docview/3098087388/2B...</td>\n",
       "      <td>29 Aug 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>A plate of food has been set before you -- per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Future of Everything: TheArtificialIntelli...</td>\n",
       "      <td>https://www.proquest.com/docview/3171573764/2B...</td>\n",
       "      <td>27 Feb 2025</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Deep under the sea, pipelines and cables carry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rising Data Center Costs Linked ToArtificialIn...</td>\n",
       "      <td>https://www.proquest.com/docview/2836987360/2B...</td>\n",
       "      <td>. 14 July 2023: B.4.</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Runaway demand for artificial intelligence is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Boom inArtificialIntelligenceHelps Lift Tech-S...</td>\n",
       "      <td>https://www.proquest.com/docview/2833514990/2B...</td>\n",
       "      <td>. 06 July 2023: A.1.</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Excitement over artificial intelligence is pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Biotech Joins AI-Fueled Rally --- Investors be...</td>\n",
       "      <td>https://www.proquest.com/docview/2842338996/2B...</td>\n",
       "      <td>. 27 July 2023: B.11.</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Healthcare shares have struggled this year, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AI Spurs New Cybersecurity Threats --- Hackers...</td>\n",
       "      <td>https://www.proquest.com/docview/2873154470/2B...</td>\n",
       "      <td>06 Oct 2023</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Hackers are using AI and encryption in new way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ArtificialIntelligence: Finance Chiefs Tackle ...</td>\n",
       "      <td>https://www.proquest.com/docview/2973859850/2B...</td>\n",
       "      <td>25 Mar 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Finance chiefs are making sure their companies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>U.S. News: OpenAI's New CEO at Center of AI Dr...</td>\n",
       "      <td>https://www.proquest.com/docview/2891939812/2B...</td>\n",
       "      <td>21 Nov 2023</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Emmett Shear's sudden appointment as OpenAI CE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- S...</td>\n",
       "      <td>https://www.proquest.com/docview/2887205702/2B...</td>\n",
       "      <td>09 Nov 2023</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Kevin Lisle, who has been teaching high-school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- C...</td>\n",
       "      <td>https://www.proquest.com/docview/2887205653/2B...</td>\n",
       "      <td>09 Nov 2023</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Recyclers across the U.S. are struggling, hurt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- G...</td>\n",
       "      <td>https://www.proquest.com/docview/2917151619/2B...</td>\n",
       "      <td>22 Jan 2024</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>President Biden issued an executive order late...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   ArtificialIntelligence(A Special Report) --- H...   \n",
       "1   ArtificialIntelligence(A Special Report) --- T...   \n",
       "2   ArtificialIntelligence(A Special Report) --- F...   \n",
       "3   Crunchbase UsesArtificialIntelligenceTo Predic...   \n",
       "4   On the Clock: Bosses' Mental Fitness Set for A...   \n",
       "5   EXCHANGE --- The Rundown on Building an AI Sup...   \n",
       "6   ArtificialIntelligence(A Special Report) --- H...   \n",
       "7   ArtificialIntelligence(A Special Report) --- H...   \n",
       "8   The Future of Everything: TheArtificialIntelli...   \n",
       "9   The Future of Everything: TheArtificialIntelli...   \n",
       "10  The Future of Everything: TheArtificialIntelli...   \n",
       "11  Rising Data Center Costs Linked ToArtificialIn...   \n",
       "12  Boom inArtificialIntelligenceHelps Lift Tech-S...   \n",
       "13  Biotech Joins AI-Fueled Rally --- Investors be...   \n",
       "14  AI Spurs New Cybersecurity Threats --- Hackers...   \n",
       "15  ArtificialIntelligence: Finance Chiefs Tackle ...   \n",
       "16  U.S. News: OpenAI's New CEO at Center of AI Dr...   \n",
       "17  ArtificialIntelligence(A Special Report) --- S...   \n",
       "18  ArtificialIntelligence(A Special Report) --- C...   \n",
       "19  ArtificialIntelligence(A Special Report) --- G...   \n",
       "\n",
       "                                                 link                   date  \\\n",
       "0   https://www.proquest.com/docview/3132486081/2B...            25 Nov 2024   \n",
       "1   https://www.proquest.com/docview/3132486071/2B...            25 Nov 2024   \n",
       "2   https://www.proquest.com/docview/3162691166/2B...            03 Feb 2025   \n",
       "3   https://www.proquest.com/docview/3168537629/2B...            20 Feb 2025   \n",
       "4   https://www.proquest.com/docview/3096788378/2B...            26 Aug 2024   \n",
       "5   https://www.proquest.com/docview/3191507034/2B...            19 Apr 2025   \n",
       "6   https://www.proquest.com/docview/3132486040/2B...            25 Nov 2024   \n",
       "7   https://www.proquest.com/docview/3162691094/2B...            03 Feb 2025   \n",
       "8   https://www.proquest.com/docview/3171573768/2B...            27 Feb 2025   \n",
       "9   https://www.proquest.com/docview/3098087388/2B...            29 Aug 2024   \n",
       "10  https://www.proquest.com/docview/3171573764/2B...            27 Feb 2025   \n",
       "11  https://www.proquest.com/docview/2836987360/2B...   . 14 July 2023: B.4.   \n",
       "12  https://www.proquest.com/docview/2833514990/2B...   . 06 July 2023: A.1.   \n",
       "13  https://www.proquest.com/docview/2842338996/2B...  . 27 July 2023: B.11.   \n",
       "14  https://www.proquest.com/docview/2873154470/2B...            06 Oct 2023   \n",
       "15  https://www.proquest.com/docview/2973859850/2B...            25 Mar 2024   \n",
       "16  https://www.proquest.com/docview/2891939812/2B...            21 Nov 2023   \n",
       "17  https://www.proquest.com/docview/2887205702/2B...            09 Nov 2023   \n",
       "18  https://www.proquest.com/docview/2887205653/2B...            09 Nov 2023   \n",
       "19  https://www.proquest.com/docview/2917151619/2B...            22 Jan 2024   \n",
       "\n",
       "                  media                                            content  \n",
       "0   Wall Street Journal  The current generation of college students is ...  \n",
       "1   Wall Street Journal  ChatGPT is barely two years old. And yet it's ...  \n",
       "2   Wall Street Journal  The race for AI dominance launched a stampede ...  \n",
       "3   Wall Street Journal  Crunchbase, the firm best known for its startu...  \n",
       "4   Wall Street Journal  Bosses already live in fear that a verbal miss...  \n",
       "5   Wall Street Journal  President Trump enthused on social media this ...  \n",
       "6   Wall Street Journal  Artificial intelligence is poised to transform...  \n",
       "7   Wall Street Journal  You can't stop an AI chatbot from sometimes ha...  \n",
       "8   Wall Street Journal  In a large, brightly-lit warehouse in Flowery ...  \n",
       "9   Wall Street Journal  A plate of food has been set before you -- per...  \n",
       "10  Wall Street Journal  Deep under the sea, pipelines and cables carry...  \n",
       "11  Wall Street Journal  Runaway demand for artificial intelligence is ...  \n",
       "12  Wall Street Journal  Excitement over artificial intelligence is pro...  \n",
       "13  Wall Street Journal  Healthcare shares have struggled this year, bu...  \n",
       "14  Wall Street Journal  Hackers are using AI and encryption in new way...  \n",
       "15  Wall Street Journal  Finance chiefs are making sure their companies...  \n",
       "16  Wall Street Journal  Emmett Shear's sudden appointment as OpenAI CE...  \n",
       "17  Wall Street Journal  Kevin Lisle, who has been teaching high-school...  \n",
       "18  Wall Street Journal  Recyclers across the U.S. are struggling, hurt...  \n",
       "19  Wall Street Journal  President Biden issued an executive order late...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('wallstreet_content.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f8d34",
   "metadata": {},
   "source": [
    "# 빈 콘텐츠 추가수집 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ff36d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 수집할 문서 수: 20\n",
      "[357] ✅ 업데이트 완료: EXCHANGE --- Markets & Finance: Aflac Sa...\n",
      "[358] ✅ 업데이트 완료: OpenAI Aims for $90 Billion Value With S...\n",
      "[359] ✅ 업데이트 완료: Microsoft CEO Says Google's Tactics Hurt...\n",
      "[360] ✅ 업데이트 완료: Business & Finance...\n",
      "[361] ✅ 업데이트 완료: U.S. News: FBI Investigates Chief-of-Sta...\n",
      "[362] ✅ 업데이트 완료: Apple to Source More Phones From India -...\n",
      "[363] ✅ 업데이트 완료: Microsoft To Invest In U.A.E. AI Firm...\n",
      "[364] ✅ 업데이트 완료: World News: Lebanon Takes Steps to Loose...\n",
      "[396] ✅ 업데이트 완료: Business & Finance...\n",
      "[397] ✅ 업데이트 완료: Business & Finance...\n",
      "[398] ✅ 업데이트 완료: Business & Finance...\n",
      "[399] ✅ 업데이트 완료: Business & Finance...\n",
      "[400] ✅ 업데이트 완료: Business & Finance...\n",
      "[401] ✅ 업데이트 완료: Markets: Stock Spotlight...\n",
      "[402] ✅ 업데이트 완료: Former Binance.US CEO Has New Firm...\n",
      "[403] ✅ 업데이트 완료: Markets: Stock Spotlight...\n",
      "[404] ✅ 업데이트 완료: Markets & Finance: Stock Spotlight...\n",
      "[405] ✅ 업데이트 완료: Business & Finance...\n",
      "[406] ✅ 업데이트 완료: Business & Finance...\n",
      "[407] ✅ 업데이트 완료: Markets: Stock Spotlight...\n",
      "📄 저장 완료: wallstreet_content.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ▶ 기존 결과 불러오기\n",
    "existing_df = pd.read_csv('wallstreet_content.csv')\n",
    "\n",
    "# ▶ content가 빈 row만 필터링\n",
    "empty_content_df = existing_df[existing_df['content'].isna() | (existing_df['content'].str.strip() == '')]\n",
    "\n",
    "empty_content_df=empty_content_df.head(20)\n",
    "\n",
    "if empty_content_df.empty:\n",
    "    print(\"✅ 모든 content가 이미 수집되어 있습니다.\")\n",
    "else:\n",
    "    print(f\"🔄 수집할 문서 수: {len(empty_content_df)}\")\n",
    "\n",
    "    # ▶ 크롬 옵션 설정\n",
    "    options = Options()\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    # ▶ 드라이버 실행\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in empty_content_df.iterrows():\n",
    "        url = row['link']\n",
    "        title = row['title']\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # ✅ URL에서 docview ID 추출\n",
    "            doc_id_match = re.search(r'/docview/(\\d+)', url)\n",
    "            doc_id = doc_id_match.group(1) if doc_id_match else ''\n",
    "            mstar_id = f\"MSTAR_{doc_id}\" if doc_id else ''\n",
    "\n",
    "            # ✅ 본문(Content)\n",
    "            content = ''\n",
    "            xpaths_content = [\n",
    "                f'//*[@id=\"fulltext_field_{mstar_id}\"]/div/root/text/p' if mstar_id else '',\n",
    "                '//*[@id=\"fullTextZone\"]//p',\n",
    "                '//*[@id=\"main-content\"]//p',\n",
    "                '//*[@id=\"companionColumn-0\"]//p',\n",
    "            ]\n",
    "            for xp in xpaths_content:\n",
    "                if not xp:\n",
    "                    continue\n",
    "                try:\n",
    "                    paragraphs = driver.find_elements(By.XPATH, xp)\n",
    "                    if paragraphs:\n",
    "                        content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # ✅ 날짜(Date): BeautifulSoup로 strong + 형제 텍스트에서 추출\n",
    "            date_text = row['date']  # 기본값: 기존 값 유지\n",
    "            try:\n",
    "                strong_tag = soup.find('strong', string=lambda s: s and ';' in s)\n",
    "                if strong_tag and strong_tag.next_sibling:\n",
    "                    raw = strong_tag.next_sibling.strip()\n",
    "                    match = re.search(r'\\d{2} \\w{3} \\d{4}', raw)\n",
    "                    if match:\n",
    "                        date_text = match.group()\n",
    "                    else:\n",
    "                        date_text = raw\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # ✅ 미디어(Media)\n",
    "            media = row['media']  # 기본값: 기존 값 유지\n",
    "            xpaths_media = [\n",
    "                f'//*[@id=\"pubPopoverTrigger-{mstar_id}\"]/span' if mstar_id else '',\n",
    "                '//*[@id=\"bibSource\"]/span',\n",
    "                '//div[@class=\"publicationTitle\"]',\n",
    "                '//div[@class=\"bibSource\"]',\n",
    "                '//span[@class=\"publicationTitle\"]',\n",
    "            ]\n",
    "            for xp in xpaths_media:\n",
    "                if not xp:\n",
    "                    continue\n",
    "                try:\n",
    "                    media_el = driver.find_element(By.XPATH, xp)\n",
    "                    media = media_el.text.strip().split(';')[0]\n",
    "                    if media:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # ✅ 결과 저장\n",
    "            existing_df.loc[idx, 'content'] = content\n",
    "            existing_df.loc[idx, 'date'] = date_text\n",
    "            existing_df.loc[idx, 'media'] = media\n",
    "\n",
    "            print(f\"[{idx+1}] ✅ 업데이트 완료: {title[:40]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1}] ❌ 실패: {url} → {e}\")\n",
    "\n",
    "    # ▶ CSV 덮어쓰기 저장\n",
    "    existing_df.to_csv('wallstreet_content.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"📄 저장 완료: wallstreet_content.csv\")\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "455ae27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1593 entries, 0 to 1592\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    1593 non-null   object\n",
      " 1   link     1593 non-null   object\n",
      " 2   date     962 non-null    object\n",
      " 3   media    962 non-null    object\n",
      " 4   content  962 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 62.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test=pd.read_csv('wallstreet_content.csv')\n",
    "test.info() #기존 962개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faaf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
