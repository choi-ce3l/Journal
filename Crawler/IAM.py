import os
import time
import random
import pandas as pd
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, WebDriverException

# ===== ì‚¬ìš©ì í™˜ê²½ =====
# 60 - 62
# 1- 8
vol_from=62
vol_to=63
issue_from=8
issue_to=9
SAVE_DIR = r"/Crawler/IAM"
os.makedirs(SAVE_DIR, exist_ok=True)
OUTPUT_CSV = os.path.join(SAVE_DIR, f"iam_vol{vol_from}to{vol_to}_issue{issue_from}.csv")  # íŒŒì¼ëª… ì •ì •
HEADLESS = False  # í•„ìš” ì‹œ True

# ===== ìœ í‹¸ =====
def random_wait(a=1.0, b=3.0):
    time.sleep(random.uniform(a, b))

def get_driver():
    opts = uc.ChromeOptions()
    # macOS ê³µí†µ ì•ˆì „ ì˜µì…˜
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--window-size=1280,800")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--disable-infobars")
    opts.add_argument("--disable-extensions")
    opts.add_argument("--disable-popup-blocking")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    # User-Agent
    opts.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    # ë³„ë„ í”„ë¡œí•„ ì‚¬ìš©ì„ ê¶Œí•˜ì§€ ì•ŠìŒ(íƒì§€ ìœ„í—˜â†‘). ê¼­ í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ í›„ ê²½ë¡œ ìˆ˜ì •.
    # opts.add_argument("--user-data-dir=/Users/choihj/Library/Application Support/Google/Chrome/Default")

    drv = uc.Chrome(options=opts)
    try:
        drv.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"}
        )
    except WebDriverException:
        pass
    return drv

def extract_text(el):
    return el.get_text(strip=True) if el else ""

def parse_article_page(html):
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª©
    title = extract_text(soup.select_one("span.title-text"))

    # ì €ì
    authors = [extract_text(a) for a in soup.select("div.author-group span.react-xocs-alternative-link")]
    authors = ", ".join([a for a in authors if a])

    # ì´ˆë¡
    abs_el = soup.select_one("div.abstract.author") or soup.select_one("div[id^='sp']") \
             or soup.select_one("div.Abstracts div.abstract")
    abstract = extract_text(abs_el)
    if abstract.lower().startswith("abstract"):
        abstract = abstract[len("abstract"):].strip()

    # ë‚ ì§œ: ë©”íƒ€ ìš°ì„  -> í˜ì´ì§€ ë‚´ í…ìŠ¤íŠ¸ ë³´ì¡°
    pub_date = ""
    meta_date = soup.select_one("meta[name='citation_publication_date']")
    if meta_date and meta_date.get("content"):
        pub_date = meta_date["content"].strip()
    if not pub_date:
        date_candidate = soup.select_one("div.text-xs, dl.article-header-details")
        pub_date = extract_text(date_candidate)

    # í‚¤ì›Œë“œ
    # ScienceDirectëŠ” 'Author keywords' ì„¹ì…˜ì´ ìˆê±°ë‚˜ ì—†ì„ ìˆ˜ ìˆìŒ
    keywords = [extract_text(k) for k in soup.select("div.keywords-section div.keyword > span")]
    if not keywords:
        keywords = [extract_text(k) for k in soup.select("div.Keywords div.keyword")]
    keywords = ", ".join([k for k in keywords if k])

    return title, authors, abstract, pub_date, keywords

# ===== í¬ë¡¤ë§ =====
driver = get_driver()
wait = WebDriverWait(driver, 25)
all_rows = []

try:
    for vol in range(vol_from, vol_to):
        for issue in range(issue_from, issue_to):
            toc_url = f"https://www.sciencedirect.com/journal/information-and-management/vol/{vol}/issue/{issue}"
            print(f"\n[DSS] Volume {vol} ì ‘ì† ì¤‘...")
            driver.get(toc_url)
            random_wait(2, 4)

            # ì¿ í‚¤ ìˆ˜ë½ ì‹œë„
            try:
                cookie_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
                )
                cookie_btn.click()
                print("ì¿ í‚¤ ìˆ˜ë½ ì™„ë£Œ")
                random_wait()
            except Exception:
                print("ì¿ í‚¤ ìˆ˜ë½ ìŠ¤í‚µ")

            # TOC ë¡œë”©
            try:
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".text-l")))
            except TimeoutException:
                print(f"âŒ ë…¼ë¬¸ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨: Vol {vol}")
                continue

            # ë™ì  í˜ì´ì§€ì—ì„œ ì°¸ì¡° ë¬´íš¨í™” ë°©ì§€: href ìˆ˜ì§‘ í›„ ìˆœíšŒ
            title_spans = driver.find_elements(By.CSS_SELECTOR, ".text-l")
            if not title_spans:
                print(f"âš ï¸ ë…¼ë¬¸ ë§í¬ ì—†ìŒ: Vol {vol}")
                continue

            hrefs = []
            for el in title_spans:
                try:
                    parent = el.find_element(By.XPATH, '//*[@id="S0167923625001083"]')
                    hrefs.append(parent.get_attribute("href"))
                except Exception:
                    # ì¼ë¶€ êµ¬ì¡°ëŠ” span ìì²´ì— í´ë¦­ ì´ë²¤íŠ¸ë§Œ ìˆìŒ â†’ span í´ë¦­ ì²˜ë¦¬ìš© ë§ˆì»¤
                    hrefs.append(None)

            for idx, href in enumerate(hrefs):
                try:
                    if href:
                        driver.get(href)
                    else:
                        # hrefê°€ ì—†ìœ¼ë©´ ë‹¤ì‹œ TOCë¡œ ê°€ì„œ í•´ë‹¹ index í´ë¦­
                        driver.get(toc_url)
                        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.js-article-title.text-l")))
                        spans = driver.find_elements(By.CSS_SELECTOR, "span.js-article-title.text-l")
                        if idx >= len(spans):
                            print(f"ì¸ë±ìŠ¤ ì´ˆê³¼ ìŠ¤í‚µ: Vol {vol}, idx {idx}, issue {issue}")
                            continue
                        link = spans[idx]
                        driver.execute_script("arguments[0].scrollIntoView({behavior:'instant',block:'center'});", link)
                        random_wait(0.6, 1.2)
                        driver.execute_script("arguments[0].click();", link)

                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "span.title-text")))
                    # í•˜ë‹¨ê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì„¹ì…˜ ë¡œë”©
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    random_wait(1.2, 2.2)

                    title, authors, abstract, pub_date, keywords = parse_article_page(driver.page_source)

                    all_rows.append({
                        "volume": vol,
                        "issue": "suppl/C",
                        "title": title,
                        "authors": authors,
                        "abstract": abstract,
                        "date": pub_date,
                        "keywords": keywords,
                        "url": driver.current_url
                    })

                    # ì¤‘ê°„ ì €ì¥ ë° ë“œë¼ì´ë²„ ì¬ì‹œì‘
                    if len(all_rows) % 30 == 0:
                        pd.DataFrame(all_rows).to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
                        print(f"ì¤‘ê°„ ì €ì¥ë¨ ({len(all_rows)}ê°œ)")
                        driver.quit()
                        driver = get_driver()
                        wait = WebDriverWait(driver, 25)
                        driver.get(toc_url)
                        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.js-article-title.text-l")))
                        random_wait(2, 4)

                except (StaleElementReferenceException, TimeoutException, WebDriverException) as e:
                    print(f"ğŸš§ ì‹¤íŒ¨ (Vol {vol}, idx {idx}): {e}")
                    # TOC ë³µêµ¬
                    try:
                        driver.get(toc_url)
                        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.js-article-title.text-l")))
                    except Exception:
                        # ë“œë¼ì´ë²„ ë¦¬ì…‹
                        driver.quit()
                        driver = get_driver()
                        wait = WebDriverWait(driver, 25)
                        driver.get(toc_url)
                        try:
                            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "span.js-article-title.text-l")))
                        except Exception:
                            print("TOC ë³µêµ¬ ì‹¤íŒ¨. ë‹¤ìŒ ë³¼ë¥¨ìœ¼ë¡œ ì§„í–‰.")
                            break
                    random_wait()
                    continue

finally:
    try:
        driver.quit()
    except Exception:
        pass

df = pd.DataFrame(all_rows)
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
print(f"\nâœ… ì €ì¥ ì™„ë£Œ! ì´ {len(df)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ë¨")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {OUTPUT_CSV}")