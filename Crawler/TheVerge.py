import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil.relativedelta import relativedelta
import re
import os
from tqdm import tqdm
import time  # ìš”ì²­ ì‚¬ì´ ê°„ê²©ì„ ë‘ê³  ì‹¶ì„ ê²½ìš° ì‚¬ìš©

def collect_theverge_links(start_date: str, end_date: str) -> list:
    """
    ì£¼ì–´ì§„ ê¸°ê°„ ë™ì•ˆ The Verge AI ì•„ì¹´ì´ë¸Œì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    valid_links = []

    current_date = start
    while current_date <= end:
        year = current_date.year
        month = current_date.month

        first_url = f'https://www.theverge.com/archives/ai-artificial-intelligence/{year}/{month}/1'
        print(f"í˜ì´ì§€ ìˆ˜ í™•ì¸ ì¤‘: {first_url}")

        try:
            first_response = requests.get(first_url)
            first_response.raise_for_status()
            first_soup = BeautifulSoup(first_response.text, 'html.parser')
            pages = first_soup.select_one("span.i0ukxu3.i0ukxu1")
            match = re.search(r'of\s*(\d+)', pages.text) if pages else None
            total_pages = int(match.group(1)) if match else 1
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            total_pages = 1

        for page in range(1, total_pages + 1):
            url = f'https://www.theverge.com/archives/ai-artificial-intelligence/{year}/{month}/{page}'
            print(f"í¬ë¡¤ë§ ì¤‘: {url}")
            try:
                response = requests.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                a_tag = soup.select('div.hp1qhq3 div div a')

                for a in a_tag:
                    link = a.get('href')
                    # print(link)
                    if link and not link.endswith("#comments"):
                        valid_links.append(link)
                        print(link)
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {e}")

        current_date += relativedelta(months=1)

    print(f"ì´ {len(valid_links)}ê°œì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return valid_links

def scrape_theverge_article(href: str, file_path: str = "data.csv"):
    """
    The Verge ê¸°ì‚¬ í•˜ë‚˜ë¥¼ í¬ë¡¤ë§í•˜ì—¬ CSV íŒŒì¼ì— ì €ì¥
    """
    url = f'https://www.theverge.com{href}'

    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        head = soup.select_one('#content h1')
        contents = soup.select('#zephr-anchor > div > p')
        keywords = soup.select('#zephr-anchor > div > ul > li')
        date = soup.select_one('time')

        title_text = head.get_text(strip=True) if head else "N/A"
        content_text = "\n".join(p.get_text(strip=True) for p in contents) if contents else "N/A"
        keyword_text = ", ".join(k.get_text(strip=True) for k in keywords) if keywords else "N/A"
        date_text = date.get_text(strip=True) if date else "N/A"

        new_data = pd.DataFrame([{
            "date": date_text,
            "title": title_text,
            "content": content_text,
            "keywords": keyword_text,
            "url": url
        }])

        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            df = pd.concat([df, new_data], ignore_index=True)
        else:
            df = new_data

        df.to_csv(file_path, index=False, encoding="utf-8-sig")
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {url}")
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} / ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="The Verge AI ê¸°ì‚¬ í¬ë¡¤ëŸ¬")
    parser.add_argument('--start', type=str, required=True, help="ì‹œì‘ ë‚ ì§œ (ì˜ˆ: 2023-01-01)")
    parser.add_argument('--end', type=str, required=True, help="ë ë‚ ì§œ (ì˜ˆ: 2023-01-31)")
    parser.add_argument('--output', type=str, default="data.csv", help="ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()

    # ë§í¬ ìˆ˜ì§‘
    links = collect_theverge_links(args.start, args.end)

    # ê¸°ì‚¬ í¬ë¡¤ë§ ë° ì €ì¥ with tqdm
    for href in tqdm(links, desc="ğŸ“° ê¸°ì‚¬ í¬ë¡¤ë§ ì§„í–‰ ì¤‘"):
        relative_href = href.replace("https://www.theverge.com", "") if href.startswith("https") else href
        scrape_theverge_article(relative_href, file_path=args.output)
        time.sleep(1)

'''
ì‹¤í–‰ ë°©ë²•

'''