# -*- coding: utf-8 -*-
"""
The Verge AI ì•„ì¹´ì´ë¸Œ í¬ë¡¤ëŸ¬ (ë§í¬ ìˆ˜ì§‘ + ê¸°ì‚¬ íŒŒì‹± + CSV ì €ì¥)

ì‚¬ìš© ì˜ˆ:
python theverge_ai_scraper.py --start 2025-09-01 --end 2025-09-01 --output ./TheVerge.csv
python theverge_ai_scraper.py --start 2025-01-01 --end 2025-09-01 --section ai-artificial-intelligence --output ~/Downloads/theverge_ai.csv
"""

import os, re, sys, json, time, argparse
from datetime import datetime
from dateutil.relativedelta import relativedelta
from urllib.parse import urljoin, urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# =========================
# ì„¤ì •
# =========================
BASE = "https://www.theverge.com"
DEFAULT_SECTION = "ai-artificial-intelligence"  # ì•„ì¹´ì´ë¸Œ ì„¹ì…˜
PAUSE = 1.2  # ì„œë²„ ì˜ˆì˜ìƒ ëŒ€ê¸°

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/116.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
}

ARTICLE_HREF_PAT = re.compile(
    r"^(/|https?://www\.theverge\.com/)(\d{4}/\d{1,2}/\d{1,2}/|news/|tech/|ai-artificial-intelligence/|column/|podcast/|video/)"
)

# =========================
# ìœ í‹¸
# =========================
def get_session():
    s = requests.Session()
    retries = Retry(
        total=5,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"],
        raise_on_status=False,
    )
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.headers.update(HEADERS)
    return s

def ensure_parent_dir(path: str):
    parent = os.path.dirname(os.path.abspath(path))
    if parent and not os.path.exists(parent):
        os.makedirs(parent, exist_ok=True)

def to_abs(href: str) -> str:
    return urljoin(BASE + "/", href)

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def coerce_date_iso(s: str) -> str:
    if not s or s == "N/A":
        return "N/A"
    # ISO in datetime attribute or JSON-LD datePublished ìš°ì„ 
    try:
        dt = datetime.fromisoformat(s.replace("Z","+00:00"))
        return dt.astimezone().isoformat()
    except Exception:
        pass
    # ê³µì§€ ë¬¸ìì—´ì—ì„œ ë‚ ì§œ ì¶”ì •
    try:
        dt = datetime.strptime(s[:25], "%B %d, %Y at %I:%M %p")
        return dt.isoformat()
    except Exception:
        pass
    return s  # ì›ë¬¸ ë³´ì¡´

# =========================
# ë§í¬ ìˆ˜ì§‘
# =========================
def guess_total_pages(html: str) -> int:
    """
    'Page 1 of N' íŒ¨í„´ ìš°ì„  íƒì§€. ì‹¤íŒ¨ ì‹œ 1 í˜ì´ì§€ë¡œ ê°€ì •.
    """
    soup = BeautifulSoup(html, "html.parser")
    # ìì£¼ ë³´ì´ëŠ” 'Page X of Y' í…ìŠ¤íŠ¸ íƒìƒ‰
    text = soup.get_text(" ", strip=True)
    m = re.search(r"[Pp]age\s+\d+\s+of\s+(\d+)", text)
    if m:
        return max(1, int(m.group(1)))
    # í˜ì´ì§€ë„¤ì´ì…˜ a[aria-label="Page N"] ìµœëŒ“ê°’ íƒìƒ‰
    nums = []
    for a in soup.find_all("a"):
        label = (a.get("aria-label") or "").strip()
        m2 = re.match(r"Page\s+(\d+)$", label)
        if m2:
            nums.append(int(m2.group(1)))
    if nums:
        return max(nums)
    return 1

def extract_archive_links(html: str) -> list:
    """
    ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ. íŒ¨í„´ ê¸°ë°˜ í•„í„°ë¡œ ì•ˆì •í™”.
    """
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "#comments" in href:
            continue
        if ARTICLE_HREF_PAT.search(href):
            links.add(to_abs(href))
    return list(links)

def collect_theverge_links(start_date: str, end_date: str, section: str = DEFAULT_SECTION) -> list:
    """
    start_date~end_date ì‚¬ì´ ê° ì›”ì˜ ì•„ì¹´ì´ë¸Œì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘.
    YYYY-MM-DD í˜•ì‹.
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    sess = get_session()

    collected = set()
    current = start

    while current <= end:
        y, m = current.year, current.month
        first_url = f"{BASE}/archives/{section}/{y}/{m}/1"
        print(f"\nğŸ” í˜ì´ì§€ ìˆ˜ í™•ì¸: {first_url}")
        try:
            r = sess.get(first_url, timeout=30)
            r.raise_for_status()
            total_pages = guess_total_pages(r.text)
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            total_pages = 1

        # í˜ì´ì§€ ìˆœíšŒ
        seen_any = False
        for page in range(1, total_pages + 1):
            url = f"{BASE}/archives/{section}/{y}/{m}/{page}"
            print(f"ğŸ“„ í¬ë¡¤ë§: {url}")
            try:
                resp = sess.get(url, timeout=30)
                if resp.status_code >= 400:
                    print(f"  â†³ ê±´ë„ˆëœ€ HTTP {resp.status_code}")
                    break
                page_links = extract_archive_links(resp.text)
                if not page_links:
                    print("  â†³ ê¸°ì‚¬ ë§í¬ ì—†ìŒ. ë‹¤ìŒ ë‹¬ë¡œ.")
                    break
                for lk in page_links:
                    collected.add(lk)
                seen_any = True
                time.sleep(PAUSE)
            except requests.RequestException as e:
                print(f"  â†³ ìš”ì²­ ì‹¤íŒ¨: {e}")
                break

        if not seen_any:
            print("  â†³ ì´ ë‹¬ì€ ìˆ˜ì§‘ ë§í¬ ì—†ìŒ.")
        current += relativedelta(months=1)

    print(f"\nâœ… ìˆ˜ì§‘ ë§í¬: {len(collected)}ê°œ")
    return sorted(collected)

# =========================
# ê¸°ì‚¬ íŒŒì‹±
# =========================
def parse_json_ld(soup: BeautifulSoup) -> dict:
    """
    JSON-LDì—ì„œ headline, description, datePublished, keywordsë¥¼ ìš°ì„  ì¶”ì¶œ.
    """
    data = {"title": None, "abstract": None, "date": None, "keywords": None}
    scripts = soup.find_all("script", type="application/ld+json")
    for sc in scripts:
        try:
            payload = json.loads(sc.string or "")
        except Exception:
            continue
        cand = []
        if isinstance(payload, dict):
            cand = [payload]
        elif isinstance(payload, list):
            cand = payload
        else:
            continue

        for item in cand:
            if not isinstance(item, dict):
                continue
            typ = item.get("@type") or item.get("type")
            if isinstance(typ, list):
                types = [t.lower() for t in typ if isinstance(t, str)]
            elif isinstance(typ, str):
                types = [typ.lower()]
            else:
                types = []
            if any(t in ("newsarticle", "article", "reportageNewsArticle".lower()) for t in types):
                data["title"] = item.get("headline") or data["title"]
                data["abstract"] = item.get("description") or data["abstract"]
                data["date"] = item.get("datePublished") or data["date"]
                kw = item.get("keywords")
                if isinstance(kw, list):
                    data["keywords"] = ", ".join([str(k) for k in kw if k])
                elif isinstance(kw, str):
                    data["keywords"] = kw
                return data
    return data

def parse_meta_fallback(soup: BeautifulSoup, current: dict) -> dict:
    """
    ë©”íƒ€ íƒœê·¸ì™€ ë³¸ë¬¸ì—ì„œ í´ë°± ì¶”ì¶œ.
    """
    out = dict(current)

    # ì œëª©
    if not out.get("title"):
        h1 = soup.find("h1")
        out["title"] = clean_text(h1.get_text()) if h1 else None

    # ìš”ì•½
    if not out.get("abstract"):
        md = soup.find("meta", attrs={"name": "description"})
        if not md:
            md = soup.find("meta", property="og:description") or soup.find("meta", attrs={"name": "twitter:description"})
        if md and md.get("content"):
            out["abstract"] = clean_text(md["content"])
        else:
            # ë³¸ë¬¸ ì¼ë¶€
            paras = soup.select("article p")
            if not paras:
                # ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆ í´ë°±
                paras = soup.find_all("p")
            text = " ".join([clean_text(p.get_text()) for p in paras[:5]])
            out["abstract"] = text if text else None

    # í‚¤ì›Œë“œ
    if not out.get("keywords"):
        mk = soup.find("meta", attrs={"name": "news_keywords"}) or soup.find("meta", attrs={"name": "keywords"})
        if mk and mk.get("content"):
            out["keywords"] = clean_text(mk["content"])
        else:
            # ëª©ë¡ í´ë°±
            kws = [li.get_text(strip=True) for li in soup.select("#zephr-anchor ul li")]
            out["keywords"] = ", ".join(kws) if kws else None

    # ë‚ ì§œ
    if not out.get("date"):
        t = soup.find("time")
        dt = t.get("datetime") if t and t.has_attr("datetime") else (t.get_text(strip=True) if t else None)
        out["date"] = dt or None

    return out

def scrape_article(url: str, sess: requests.Session) -> dict:
    """
    ë‹¨ì¼ ê¸°ì‚¬ íŒŒì‹± â†’ dict ë°˜í™˜
    """
    try:
        r = sess.get(url, timeout=40)
        r.raise_for_status()
    except requests.RequestException as e:
        return {"url": url, "error": f"request_failed: {e}"}

    soup = BeautifulSoup(r.text, "html.parser")

    data = parse_json_ld(soup)
    data = parse_meta_fallback(soup, data)

    return {
        "date": coerce_date_iso(data.get("date") or "N/A"),
        "title": clean_text(data.get("title") or "N/A"),
        "abstract": clean_text(data.get("abstract") or "N/A"),
        "keywords": clean_text(data.get("keywords") or "N/A"),
        "url": url,
    }

# =========================
# ë©”ì¸
# =========================
def main():
    ap = argparse.ArgumentParser(description="The Verge AI ì•„ì¹´ì´ë¸Œ í¬ë¡¤ëŸ¬")
    ap.add_argument("--start", required=True, help="ì‹œì‘ ë‚ ì§œ YYYY-MM-DD")
    ap.add_argument("--end", required=True, help="ì¢…ë£Œ ë‚ ì§œ YYYY-MM-DD")
    ap.add_argument("--section", default=DEFAULT_SECTION, help="ì•„ì¹´ì´ë¸Œ ì„¹ì…˜ ê²½ë¡œ (ê¸°ë³¸: ai-artificial-intelligence)")
    ap.add_argument("--output", default="/Users/choihj/PycharmProjects/Journal/Data/TheVerge.csv", help="ì¶œë ¥ CSV ê²½ë¡œ")
    ap.add_argument("--resume", action="store_true", help="ì´ë¯¸ ì €ì¥ëœ URLì€ ê±´ë„ˆë›°ê¸°")
    ap.add_argument("--limit", type=int, default=0, help="ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (0=ë¬´ì œí•œ)")
    args = ap.parse_args()

    ensure_parent_dir(args.output)
    sess = get_session()

    # ê¸°ì¡´ CSV ë¡œë“œ ë° ìŠ¤í‚µ ëª©ë¡
    existing_urls = set()
    if args.resume and os.path.exists(args.output):
        try:
            prev = pd.read_csv(args.output)
            if "url" in prev.columns:
                existing_urls = set(prev["url"].dropna().tolist())
                print(f"â†º ì¬ê°œ ëª¨ë“œ: ê¸°ì¡´ {len(existing_urls)}ê°œ URL ìŠ¤í‚µ")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ë§í¬ ìˆ˜ì§‘
    links = collect_theverge_links(args.start, args.end, section=args.section)
    if args.resume and existing_urls:
        links = [u for u in links if u not in existing_urls]
        print(f"â†º ìŠ¤í‚µ í›„ ì”ì—¬ ë§í¬: {len(links)}")

    # ê¸°ì‚¬ íŒŒì‹±
    rows = []
    count = 0
    for url in tqdm(links, desc="ğŸ“° ê¸°ì‚¬ íŒŒì‹±"):
        row = scrape_article(url, sess)
        rows.append(row)
        count += 1
        if args.limit and count >= args.limit:
            break
        time.sleep(PAUSE)

    # CSV ì €ì¥
    df_new = pd.DataFrame(rows, columns=["date", "title", "abstract", "keywords", "url"])
    if os.path.exists(args.output) and not df_new.empty:
        try:
            df_old = pd.read_csv(args.output)
            df_all = pd.concat([df_old, df_new], ignore_index=True)
        except Exception:
            df_all = df_new
    else:
        df_all = df_new

    # ì¤‘ë³µ ì œê±°
    if not df_all.empty:
        df_all.drop_duplicates(subset=["url"], inplace=True)

    try:
        df_all.to_csv(args.output, index=False, encoding="utf-8-sig")
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {os.path.abspath(args.output)} (ì´ {len(df_all)}ê±´)")
    except PermissionError:
        print("âŒ ì €ì¥ ì‹¤íŒ¨: Permission denied. ì“°ê¸° ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”. ì˜ˆ: --output ~/Downloads/theverge.csv")
        sys.exit(1)

if __name__ == "__main__":
    main()