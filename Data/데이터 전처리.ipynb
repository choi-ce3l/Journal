{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Academia",
   "id": "91b50615df3ef7f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. MISQ.csv 만들기\n",
    "- 2023년 ~ 2025년, 3월, 6월, 9월, 12월로 나눠서 date 저장\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 MISQ로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "cc0a2c7b2f001625"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-22T02:04:39.253559Z",
     "start_time": "2025-10-22T02:04:38.989556Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정: 여기만 수정하세요\n",
    "# ========================================\n",
    "VOL_NUMBERS = [47, 48, 49]  # 처리할 vol 번호 리스트\n",
    "START_YEAR = 2023  # 첫 번째 vol의 시작 연도\n",
    "JOURNAL_PREFIX = \"misq\"  # 파일명 접두사 (예: misq, jmis, isj 등)\n",
    "# ========================================\n",
    "\n",
    "# 파일들이 있는 디렉토리 경로\n",
    "data_dir = \"/home/dslab/choi/Journal/Data/Academia/MISQ\"\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "file_pattern = f\"{JOURNAL_PREFIX}_vol*.csv\"\n",
    "csv_files = glob(os.path.join(data_dir, file_pattern))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{file_pattern}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {data_dir}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 {JOURNAL_PREFIX.upper()} 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(vol, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    vol: volume 번호\n",
    "    iss: issue 번호 (1~4)\n",
    "    \"\"\"\n",
    "    # vol 번호에서 연도 계산\n",
    "    if vol in VOL_NUMBERS:\n",
    "        year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑\n",
    "    month_mapping = {\n",
    "        1: \"03\",\n",
    "        2: \"06\",\n",
    "        3: \"09\",\n",
    "        4: \"12\"\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    # 파일명에서 vol과 iss 추출\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.search(r'vol(\\d+)_iss(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        vol_num = int(match.group(1))\n",
    "        iss_num = int(match.group(2))\n",
    "\n",
    "        # CSV 파일 읽기\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # date 칼럼 추가\n",
    "            df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "            # affiliations 칼럼 추가 (저널명을 대문자로)\n",
    "            df['affiliations'] = JOURNAL_PREFIX.upper()\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            print(f\"처리: {filename:30s} -> date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행\")\n",
    "\n",
    "    # 1. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[1단계] Null 값 제거 (title, abstract 기준)\")\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 2. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 3. 최종 컬럼 선택\n",
    "    print(f\"\\n[3단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 4. 날짜별 통계\n",
    "    print(f\"\\n[4단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 5. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_PREFIX.upper()}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_PREFIX.upper()}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    print(final_df.head(3).to_string())\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 11개의 MISQ 파일 발견\n",
      "\n",
      "처리: misq_vol47_iss1.csv            -> date: 2023-03, rows: 18\n",
      "처리: misq_vol47_iss2.csv            -> date: 2023-06, rows: 17\n",
      "처리: misq_vol47_iss3.csv            -> date: 2023-09, rows: 17\n",
      "처리: misq_vol47_iss4.csv            -> date: 2023-12, rows: 19\n",
      "처리: misq_vol48_iss1.csv            -> date: 2024-03, rows: 17\n",
      "처리: misq_vol48_iss2.csv            -> date: 2024-06, rows: 19\n",
      "처리: misq_vol48_iss3.csv            -> date: 2024-09, rows: 19\n",
      "처리: misq_vol48_iss4.csv            -> date: 2024-12, rows: 26\n",
      "처리: misq_vol49_iss1.csv            -> date: 2025-03, rows: 20\n",
      "처리: misq_vol49_iss2.csv            -> date: 2025-06, rows: 19\n",
      "처리: misq_vol49_iss3.csv            -> date: 2025-09, rows: 19\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 210개 행\n",
      "\n",
      "[1단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 210개 행\n",
      "  - 제거 후: 195개 행\n",
      "  - 제거됨: 15개 행\n",
      "\n",
      "[2단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 195개 행\n",
      "  - 제거 후: 194개 행\n",
      "  - 제거됨: 1개 행\n",
      "\n",
      "[3단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[4단계] 날짜별 통계\n",
      "  - 2023-03: 17개\n",
      "  - 2023-06: 15개\n",
      "  - 2023-09: 15개\n",
      "  - 2023-12: 15개\n",
      "  - 2024-03: 15개\n",
      "  - 2024-06: 15개\n",
      "  - 2024-09: 19개\n",
      "  - 2024-12: 25개\n",
      "  - 2025-03: 20개\n",
      "  - 2025-06: 19개\n",
      "  - 2025-09: 19개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/MISQ.csv\n",
      "✓ 최종 행 수: 194개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: MISQ\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                               title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         abstract                                                                                                                                                                                      keywords affiliations\n",
      "1  2023-03  Know Your Firm: Managing Social Media Engagement to Improve Firm Sales Performance                                                                                                                                                                                                                                                                                     We examine the impact of firm social media engagement on sales performance, answering “whether,” “what,” and “how” questions. The study uses a quasi-experimental design in a social e-commerce setting, for which propensity score matching and difference-in-differences methods quantify a mean 20.67% sales increase after firm social media adoption. We also find that firms that sell low-involvement products benefit more from social media adoption, compared to those that sell high-involvement products. Further, in terms of how to manage social media engagement, we find that informative content, in general, is effective for sales of high-involvement products, whereas promotional content, a new type of content discovered in this study, is more beneficial for sales of low-involvement products. Meanwhile, more social media followers generate better firm sales performance. We used instrumental variables and the control function method to address endogeneity issues and conducted robustness checks to support our conclusions. This study sheds light on the value of firm social media, particularly regarding industry differences and firm know-how.  Firm social media, sales performance, product involvement, content analysis, quasi-experiment, propensity score matching, difference-in-differences, instrumental variable, control function         MISQ\n",
      "2  2023-03                                                       It Depends On When You Search                                                                                                                                                                Existing studies have found that online search is a revealed measure for investor attention and a useful predictor of stock returns. We study the heterogeneity in retail investor attention by comparing search conducted on weekdays vs. weekends and investigate the price pressure channel and information processing channel for stock return predictability. According to the information processing channel, weekends afford retail investors more time for the intensive cognitive analysis necessary to make better predictions. Alternatively, weekend search might better capture the price pressure from retail investors’ trading activities. We provide empirical results that support the information processing channel. We first show that weekend search, rather than weekday search, predicts large-cap stock returns in both the cross-section and time series. Additionally, our findings on retail trading activity contradict the price pressure channel in that weekday search, rather than weekend search, leads to a subsequent retail order imbalance. Overall, our study contributes to the literature on the predictive power of online search on stock returns, which has mainly focused on the price pressure channel, which yields significant results for small-cap stocks only.                                                                                             Internet search, time heterogeneity, retail investor attention, stock returns, trading activities         MISQ\n",
      "3  2023-03          Cyberslacking in the Workplace: Antecedents and Effects on Job Performance  Employees’ nonwork use of information technology (IT), or cyberslacking, is of growing concern due to its erosion of job performance and other negative organizational consequences. Research on cyberslacking antecedents has drawn on diverse theoretical perspectives, resulting in the lack of a cohesive explanation of cyberslacking. Further, prior studies have generally overlooked IT-specific variables. To address cyberslacking problems in organizations, as well as research gaps in the literature, we used a combination of a literature-based approach and a qualitative inquiry to develop a model of cyberslacking that includes a 2×2 typology of antecedents. The proposed model was tested and supported in a three-wave field study of 395 employees in a U.S. Fortune-100 organization. This study organizes antecedents from diverse research streams and validates their relative impact on cyberslacking, thus providing a cohesive theoretical explanation of cyberslacking. This study also incorporates contextualization (i.e., IT-specific factors) into theory development and enriches the IS literature by examining the nonwork aspects of IT use and their negative consequences to organizations. In addition, the results provide practitioners with insights into the nonwork use of IT in organizations, particularly regarding how they can take organizational action to mitigate cyberslacking and maintain employee productivity.                          Cyberslacking, cyberdeviance, counterproductive IT use, counterproductive workplace behaviors, job performance, literature-based approach, work stressors, IT policy         MISQ\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. DSS.csv 만들기\n",
    "- DSS 폴더에 있는 거 다 병합\n",
    "- 중복 제거 -> title, abstract 기준\n",
    "- date는 기존에 있는 거 그대로 사용\n",
    "- date -> 2023 ~ 2025 아닌 거 삭제\n",
    "- null 삭제 -> title, abstract 기준\n",
    "- affiliations 생성 -> DSS로 설정\n",
    "- 최종 칼럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "ec0e7099419ff01b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:13:00.627115Z",
     "start_time": "2025-10-22T02:13:00.560731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"DSS\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/DSS\"  # DSS 파일들이 있는 디렉토리\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "# DSS CSV 파일들 찾기 (dss로 시작하는 모든 csv 파일)\n",
    "file_pattern = \"dss*.csv\"\n",
    "csv_files = glob(os.path.join(DATA_DIR, file_pattern))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{file_pattern}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 DSS 파일 발견\\n\")\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_dfs.append(df)\n",
    "        print(f\"읽기: {filename:40s} -> {len(df)}개 행\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행\")\n",
    "\n",
    "    # 1. affiliations 칼럼 추가\n",
    "    print(f\"\\n[1단계] affiliations 생성\")\n",
    "    combined_df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "    # 2. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] Null 값 제거 (title, abstract 기준)\")\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 3. date 컬럼 확인 및 연도 필터링\n",
    "    print(f\"\\n[3단계] Date 필터링 (2023-2025)\")\n",
    "\n",
    "    if 'date' not in combined_df.columns:\n",
    "        print(\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "        exit()\n",
    "\n",
    "    # date에서 연도 추출\n",
    "    before_date = len(combined_df)\n",
    "\n",
    "    # date 형식 확인 (여러 형식 지원)\n",
    "    def extract_year(date_str):\n",
    "        \"\"\"date 문자열에서 연도 추출\"\"\"\n",
    "        if pd.isna(date_str):\n",
    "            return None\n",
    "\n",
    "        date_str = str(date_str)\n",
    "        # YYYY-MM-DD, YYYY-MM, YYYY 등 다양한 형식 지원\n",
    "        match = re.search(r'(\\d{4})', date_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    combined_df['year'] = combined_df['date'].apply(extract_year)\n",
    "\n",
    "    # 유효한 연도만 필터링\n",
    "    combined_df = combined_df[combined_df['year'].isin(VALID_YEARS)]\n",
    "    combined_df = combined_df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "\n",
    "    after_date = len(combined_df)\n",
    "    print(f\"  - 필터링 전: {before_date}개 행\")\n",
    "    print(f\"  - 필터링 후: {after_date}개 행\")\n",
    "    print(f\"  - 제거됨: {before_date - after_date}개 행\")\n",
    "\n",
    "    # 4. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[4단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 5. 최종 컬럼 선택\n",
    "    print(f\"\\n[5단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 6. 날짜별 통계\n",
    "    print(f\"\\n[6단계] 날짜별 통계\")\n",
    "\n",
    "    # date를 연도-월 형식으로 변환하여 통계\n",
    "    def format_date_for_stats(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return 'Unknown'\n",
    "        date_str = str(date_str)\n",
    "        # YYYY-MM 형식으로 변환\n",
    "        match = re.search(r'(\\d{4})-?(\\d{2})?', date_str)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            month = match.group(2) if match.group(2) else '01'\n",
    "            return f\"{year}-{month}\"\n",
    "        return date_str\n",
    "\n",
    "    final_df['date_formatted'] = final_df['date'].apply(format_date_for_stats)\n",
    "    date_stats = final_df.groupby('date_formatted').size().sort_index()\n",
    "    final_df = final_df.drop('date_formatted', axis=1)\n",
    "\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 7. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "id": "5cd85e3fc85dee63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 16개의 DSS 파일 발견\n",
      "\n",
      "읽기: dss_vol_164_to_166.csv                   -> 16개 행\n",
      "읽기: dss_vol_165_to_166.csv                   -> 16개 행\n",
      "읽기: dss_vol_166_to_167.csv                   -> 16개 행\n",
      "읽기: dss_vol_167_to_167.csv                   -> 8개 행\n",
      "읽기: dss_vol_168_to_168.csv                   -> 9개 행\n",
      "읽기: dss_vol_169_to_169.csv                   -> 9개 행\n",
      "읽기: dss_vol_170_to_170.csv                   -> 10개 행\n",
      "읽기: dss_vol_171_to_171.csv                   -> 11개 행\n",
      "읽기: dss_vol_172_to_172.csv                   -> 10개 행\n",
      "읽기: dss_vol_173_to_173.csv                   -> 10개 행\n",
      "읽기: dss_vol_174_to_174.csv                   -> 13개 행\n",
      "읽기: dss_vol_175_to_175.csv                   -> 14개 행\n",
      "읽기: dss_vol_176_to_176.csv                   -> 14개 행\n",
      "읽기: dss_vol_176_to_180.csv                   -> 91개 행\n",
      "읽기: dss_vol_181_to_199.csv                   -> 40개 행\n",
      "읽기: dss_vol_199_to_199.csv                   -> 8개 행\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 295개 행\n",
      "\n",
      "[1단계] affiliations 생성\n",
      "  - affiliations: DSS\n",
      "\n",
      "[2단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 295개 행\n",
      "  - 제거 후: 250개 행\n",
      "  - 제거됨: 45개 행\n",
      "\n",
      "[3단계] Date 필터링 (2023-2025)\n",
      "  - 필터링 전: 250개 행\n",
      "  - 필터링 후: 250개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[4단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 250개 행\n",
      "  - 제거 후: 212개 행\n",
      "  - 제거됨: 38개 행\n",
      "\n",
      "[5단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[6단계] 날짜별 통계\n",
      "  - 2023-01: 7개\n",
      "  - 2023-02: 8개\n",
      "  - 2023-03: 7개\n",
      "  - 2023-04: 5개\n",
      "  - 2023-05: 6개\n",
      "  - 2023-06: 6개\n",
      "  - 2023-07: 7개\n",
      "  - 2023-08: 8개\n",
      "  - 2023-09: 7개\n",
      "  - 2023-10: 7개\n",
      "  - 2023-11: 10개\n",
      "  - 2023-12: 11개\n",
      "  - 2024-01: 13개\n",
      "  - 2024-02: 18개\n",
      "  - 2024-03: 18개\n",
      "  - 2024-04: 19개\n",
      "  - 2024-05: 18개\n",
      "  - 2024-06: 11개\n",
      "  - 2024-07: 7개\n",
      "  - 2024-08: 9개\n",
      "  - 2024-09: 1개\n",
      "  - 2024-10: 3개\n",
      "  - 2025-12: 6개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/DSS.csv\n",
      "✓ 최종 행 수: 212개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: DSS\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                                                    title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract                                                                                                      keywords affiliations\n",
      "1  2023-01  Impact of content ideology on social media opinion polarization: The moderating role of functional affordances and symbolic expressions                                                                                                                                                                                                                                                                                                                 We offer theory and evidence regarding the impact of content ideology (i.e., emotionally charged beliefs expressed in sentiments) on opinion polarization (i.e., conflicting attitudes about an event) on social media. Specifically, we consider the moderating role of functional affordances and symbolic expressions to draw inferences about opinion polarization. From a sentiment analysis of 3600 posts and a survey of 468 Weibo users, we find that content ideology is positively related to social media opinion polarization. The effect of content ideology is greater when users receive stronger symbolic expressions. Further, our results show an insignificant moderating relationship between functional affordances and this effect. The findings suggest that it is critical to consider content ideology and symbolic expressions when assessing the relationship between published content and polarized opinions on social media.  Social media, Opinion polarization, Sentiment analysis, Ideology, Functional affordance, Symbolic expression          DSS\n",
      "4  2023-01                                                                     A novel label-based multimodal topic model for social media analysis  Extracting useful knowledge from multimodal data is the core of manymultimedia applications, such as recommendation systems, and cross-modal retrieval. In this paper, we propose a label-based multimodal topic (LB-MMT) model to jointly model text andimage datatagged with multiple labels. Specifically, we use the labels assupervised informationto generate the text and image data. In the LB-MMT model, we assume that the textual words and visual words related to each text and image are drawn from a mixture of latent topics, where each topic is represented as a group of textual words and visual words. Moreover, we introduce multiple topics for each label, to build the top-down relationship from label to text and image. To investigate the effectiveness of the proposed approach, we conduct extensive experiments on a real-world multimodal dataset with labels. The results show the proposed approach obtains superior performances on topic coherence and label prediction compared with previous competitors. In addition, we show that our model yields interesting insights about multimodal topics. The proposed model provides important practical implications, e.g., designing more attractive multimodal contents formarketers.                           Multimodal data, Topic modeling, Label data, Supervised model, Image representation          DSS\n",
      "5  2023-01                                      CATCHM: A novel network-based credit card fraud detection method using node representation learning                                                                                                                                                                              Advanced fraud detection systems leverage the digital traces from (credit-card) transactions to detect fraudulent activity in future transactions. Recent research in fraud detection has focused primarily ondata analyticscombined with manual feature engineering, which is tedious, expensive and requires considerable domain expertise. Furthermore, transactions are often examined in isolation, disregarding the interconnection that exists between them.In this paper, we proposeCATCHM, a novel network-based credit card fraud detection method based onrepresentation learning(RL). Through innovative network design, an efficient inductive pooling operator, and careful downstream classifier configuration, we show hownetwork RLcan benefit fraud detection by avoiding manual feature engineering and explicitly considering therelational structureof transactions. Extensive empirical evaluation on a real-life credit card dataset shows thatCATCHMoutperforms state-of-the-art methods, thereby illustrating the practical relevance of this approach for industry.                                 Network representation learning, DeepWalk, Credit card fraud, Fraud detection          DSS\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.ISR.csv 만들기\n",
    "- /home/dslab/choi/WITS/원본데이터/ISR.csv 여기에 있는 데이터랑 수집한 데이터 /home/dslab/choi/Journal/Data/Academia/isre_vol36_1.csv이랑 합치기.\n",
    "- 데이터에 있는 vol 34 ~ 36, iss 1~4을 2023년 ~ 2025년, 3월, 6월, 9월, 12월로 나눠서 date 저장\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 MISQ로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "15f958ca068e6f73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:09:04.495287Z",
     "start_time": "2025-10-22T04:09:04.239940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"ISR\"\n",
    "# 기존 데이터 경로\n",
    "ORIGINAL_DATA_PATH = \"/home/dslab/choi/WITS/원본데이터/ISR.csv\"\n",
    "# 새로 수집한 데이터 경로\n",
    "NEW_DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/\"\n",
    "NEW_DATA_PATTERN = \"isre_vol*.csv\"\n",
    "# vol-year 매핑\n",
    "VOL_NUMBERS = [34, 35, 36]  # vol 34, 35, 36\n",
    "START_YEAR = 2023  # vol 34 = 2023년\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ISR.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "# 1. 기존 데이터 읽기\n",
    "print(\"[1단계] 기존 데이터 읽기\")\n",
    "if os.path.exists(ORIGINAL_DATA_PATH):\n",
    "    try:\n",
    "        original_df = pd.read_csv(ORIGINAL_DATA_PATH)\n",
    "        all_dfs.append(original_df)\n",
    "        print(f\"  ✓ 기존 데이터: {ORIGINAL_DATA_PATH}\")\n",
    "        print(f\"    - 행 수: {len(original_df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 기존 데이터 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 기존 데이터 파일이 없습니다: {ORIGINAL_DATA_PATH}\")\n",
    "\n",
    "# 2. 새로 수집한 데이터 읽기\n",
    "print(f\"\\n[2단계] 새 데이터 읽기\")\n",
    "new_csv_files = glob(os.path.join(NEW_DATA_DIR, NEW_DATA_PATTERN))\n",
    "\n",
    "if new_csv_files:\n",
    "    print(f\"  ✓ {len(new_csv_files)}개의 새 데이터 파일 발견\")\n",
    "\n",
    "    # 날짜 매핑 함수\n",
    "    def get_date(vol, iss):\n",
    "        \"\"\"vol과 iss 번호로 날짜 생성\"\"\"\n",
    "        if vol in VOL_NUMBERS:\n",
    "            year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "        else:\n",
    "            # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "            year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "        # iss에 따른 월 매핑\n",
    "        month_mapping = {\n",
    "            1: \"03\",\n",
    "            2: \"06\",\n",
    "            3: \"09\",\n",
    "            4: \"12\"\n",
    "        }\n",
    "\n",
    "        month = month_mapping.get(iss, \"03\")\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    for file_path in sorted(new_csv_files):\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        # 파일명에서 vol과 iss 추출\n",
    "        match = re.search(r'vol(\\d+)_(\\d+)', filename)\n",
    "\n",
    "        if match:\n",
    "            vol_num = int(match.group(1))\n",
    "            iss_num = int(match.group(2))\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # date 칼럼 추가\n",
    "                df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "                all_dfs.append(df)\n",
    "                print(f\"    - {filename:35s} -> date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ 오류 ({filename}): {e}\")\n",
    "        else:\n",
    "            print(f\"    ⚠ 파일명 패턴 불일치: {filename}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 새 데이터 파일이 없습니다: {NEW_DATA_DIR}{NEW_DATA_PATTERN}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 3. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 4. affiliations 칼럼 추가\n",
    "    print(f\"\\n[3단계] affiliations 생성\")\n",
    "    combined_df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "    # 5. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 6. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[5단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 7. 최종 컬럼 선택\n",
    "    print(f\"\\n[6단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 8. 날짜별 통계\n",
    "    print(f\"\\n[7단계] 날짜별 통계\")\n",
    "\n",
    "    # date가 있는 경우만 통계\n",
    "    if final_df['date'].notna().any():\n",
    "        date_stats = final_df.groupby('date').size().sort_index()\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "    else:\n",
    "        print(\"  - date 정보가 없습니다.\")\n",
    "\n",
    "    # 9. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ Vol 범위: {min(VOL_NUMBERS)} - {max(VOL_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")\n",
    "    print(\"  - 기존 데이터 경로를 확인하세요.\")\n",
    "    print(\"  - 새 데이터 경로를 확인하세요.\")"
   ],
   "id": "48831ececf49501c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ISR.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] 기존 데이터 읽기\n",
      "  ❌ 기존 데이터 읽기 오류: 'utf-8' codec can't decode byte 0xa1 in position 4683: invalid start byte\n",
      "\n",
      "[2단계] 새 데이터 읽기\n",
      "  ✓ 2개의 새 데이터 파일 발견\n",
      "    - isre_vol36_1.csv                    -> date: 2025-03, rows: 62\n",
      "    - isre_vol36_3.csv                    -> date: 2025-09, rows: 30\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 92개 행 (2개 파일)\n",
      "\n",
      "[3단계] affiliations 생성\n",
      "  - affiliations: ISR\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 92개 행\n",
      "  - 제거 후: 87개 행\n",
      "  - 제거됨: 5개 행\n",
      "\n",
      "[5단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 87개 행\n",
      "  - 제거 후: 58개 행\n",
      "  - 제거됨: 29개 행\n",
      "\n",
      "[6단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[7단계] 날짜별 통계\n",
      "  - 2025-03: 58개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/ISR.csv\n",
      "✓ 최종 행 수: 58개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: ISR\n",
      "✓ Vol 범위: 34 - 36\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                                                                    title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           abstract                                                                                                        keywords affiliations\n",
      "1  2025-03                   Advancing Next-Generation Multimethod Research in Information Systems: A Framework and Some Recommendations for Authors and Evaluators  The increasing complexity of sociotechnical phenomena, proliferation of diverse data sources, expanding repertoire of research methods, and broadening multiparadigmatic awareness and competence have spurred a growing interest in multimethod research in the information systems (IS) field. This editorial recognizes and celebrates the value of integrating diverse methods and offers a framework to classify multimethod research using two dimensions: (a) methodological distance—the degree of difference (proximate or distant) between methods employed in terms of characteristics such as paradigmatic assumptions, techniques, and goals; and (b) nature of integration—the extent to which the methods are combined in loosely coupled (interlayered) or tightly coupled (intertwined) ways. These dimensions yield four types of multimethod research: assembly (proximate methods with interlayered integration), blend (proximate methods with intertwined integration), bridge (distant methods with interlayered integration), and fusion (distant methods with intertwined integration). We illustrate each archetype with studies published in leading IS journals. Building on these examples, we provide actionable guidance for authors on conducting and presenting multimethod research and also offer recommendations for evaluators of multimethod work. More broadly, we call on the IS community to embrace multimethod research not as an ad hoc stack of methods, but as a systematic strategy, aligning with these recommendations related to methodological distance and nature of integration, to produce a credible, revelatory, and rich body of knowledge on multifaceted IS phenomena.                                                                                                             NaN          ISR\n",
      "2  2025-03  Impact of the General Data Protection Regulation on the Global Mobile App Market: Digital Trade Implications of Data Protection and Privacy Regulations                                                                                                                                                                                                                                                                                                                                                                                                                   Although regional data protection and privacy regimes are often cited as major barriers to crossborder digital trade, mitigating consumer privacy concerns through regulations can potentially increase the demand for foreign digital products or services. This study presents empirical evidence on this issue by examining the impact of the General Data Protection Regulation (GDPR) on the global mobile app market. We construct a comprehensive data set of apps distributed by Apple’s App Store over the 26-month period covering the enactment of the GDPR and employ econometric models to analyze the regulation’s effects on app trade between country pairs. Contrary to assertions that regional data protection and privacy regulations impede digital trade and aggravate fragmentation, the empirical results demonstrate a significant increase in top-performing foreign apps compared with native ones in the European Union countries post-GDPR. We further conduct a series of analyses to explore the underlying mechanisms potentially driving these effects from both the supply and demand sides. Our findings lend support to the demand-side mechanism, whereby the GDPR helps alleviate consumer privacy concerns and provides reassurance in adopting foreign digital products.                      data protection and privacy regulations, digital product, GDPR, digital trade, mobile apps          ISR\n",
      "3  2025-03                                    The Impact of Geographic and Social Proximity on Physicians: Evidence from the Adoption of an Online Health Community                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Although online doctor consultations are rapidly gaining popularity, physicians must actively participate in physician–patient interaction platforms to fully unlock their potential. Through a social influence lens, this study empirically investigates physician adoption behavior over time across regions in the diffusion of an online health community (OHC). We examined the impacts of geographically and socially close adopters and investigated the interaction of proximity influences and competition in adoption. We collected panel data on 21,654 physicians in 32 cities in three provinces in China from a large Chinese OHC. The results demonstrate that both geographic and social proximity facilitate adoption when local competition is low. However, as local competition increases, the impact of socially close prior adopters increases, whereas that of geographically close prior adopters decreases. This pattern becomes stronger for physicians with lower titles.  online health community, social influence, geographic proximity, social proximity, adoption, local competition          ISR\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. IAM.csv 만들기\n",
    "- 폴더에 있는 IAM.csv에서 필요한 컬럼만 저장\n",
    "- affiliations 컬럼 생성 -> IAM으로 지정\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "fa074c56cf61608d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:17:03.456902Z",
     "start_time": "2025-10-22T04:17:03.409803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"IAM\"\n",
    "# IAM 데이터 경로 (필요시 수정)\n",
    "INPUT_FILE_PATH = \"/home/dslab/choi/Journal/Data/Academia/IAM/IAM.csv\"\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"IAM.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. IAM 데이터 읽기\n",
    "print(\"[1단계] IAM 데이터 읽기\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(f\"  ❌ 파일이 없습니다: {INPUT_FILE_PATH}\")\n",
    "    print(f\"  ⚠ 다른 경로에 있다면 코드 상단의 INPUT_FILE_PATH를 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_PATH)\n",
    "    print(f\"  ✓ 파일 읽기 완료: {INPUT_FILE_PATH}\")\n",
    "    print(f\"    - 행 수: {len(df)}\")\n",
    "    print(f\"    - 컬럼: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. affiliations 컬럼 생성\n",
    "print(f\"\\n[2단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 3. null 값 제거 (title, abstract 기준)\n",
    "print(f\"\\n[3단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title과 abstract 컬럼 존재 여부 확인\n",
    "required_cols = ['title', 'abstract']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 4. 중복 제거 (title, abstract 기준)\n",
    "print(f\"\\n[4단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 5. 최종 컬럼 선택\n",
    "print(f\"\\n[5단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 6. 날짜별 통계 (date 컬럼이 있는 경우)\n",
    "if 'date' in df.columns and final_df['date'].notna().any():\n",
    "    print(f\"\\n[6단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"\\n[6단계] 날짜 정보 없음\")\n",
    "\n",
    "# 7. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ IAM.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "545b19f655a19f58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IAM.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] IAM 데이터 읽기\n",
      "  ✓ 파일 읽기 완료: /home/dslab/choi/Journal/Data/Academia/IAM/IAM.csv\n",
      "    - 행 수: 298\n",
      "    - 컬럼: ['volume', 'issue', 'title', 'authors', 'abstract', 'date', 'keywords', 'url']\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] affiliations 생성\n",
      "  - affiliations: IAM\n",
      "\n",
      "[3단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 298개 행\n",
      "  - 제거 후: 270개 행\n",
      "  - 제거됨: 28개 행\n",
      "\n",
      "[4단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 270개 행\n",
      "  - 제거 후: 270개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[5단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[6단계] 날짜별 통계\n",
      "  - 총 23개의 날짜\n",
      "  - 첫 날짜: 2023/01/01 (4개)\n",
      "  - 마지막 날짜: 2025/12/01 (21개)\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/IAM.csv\n",
      "✓ 최종 행 수: 270개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: IAM\n",
      "\n",
      "[데이터 미리보기]\n",
      "         date                                                                                                                                  title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         abstract                                                                                                                                     keywords affiliations\n",
      "1  2024/01/01                             Impact of online information on the pricing and profits of firms with different levels of brand reputation                                                                                   A high brand reputation is usually associated with a brand premium and high profit. Does it still hold in the online market with rich information? How do the sales volume information and ratings information change the influence of the existing brand reputation? This study investigates a two-period pricing model of duopoly firms with different levels of brand reputation (high vs. low) in the presence of online information. We examine the impact of online information on firms’ optimal pricing and profits while considering the impact of brand reputation. Our analytical results find that in the presence of online information, the brand premium may be negative, and the firm with a lower brand reputation can earn higher profit under certain conditions. In addition, the profits of both firms may increase as the brand reputation gap increases, which implies a win-win situation in the brand competition. This study deepens our understanding of brand reputation in the context of online market, which is influenced by the herding effect and price effect of online information. The theoretical results provide guidelines for the design of online reputation systems, brand cooperation, and information disclosure.                                                               Brand reputation, Pricing, Sales volume, Ratings, Herding effect, Price effect          IAM\n",
      "2  2024/01/01              Determinants and consequences of routine and advanced use of business intelligence (BI) systems by management accountants                                                                                                                                                                  There is limited evidence on why decision makers extend beyond routine use toward more advanced use of Business Intelligence (BI) systems. This study developed an extended DeLone and McLean information system success model hypothesizing the effects of system, data, information, and service quality, along with self-efficacy and task complexity, on routine and advanced use of BI. Task complexity was also considered as moderating the effects of use on individual performance. Data was collected from a sample (N= 362) of management accountants using BI systems. Results confirmed system, data, information, and service quality as important, and indicated that routine use is less likely under conditions of task complexity and does not rely on user self-efficacy to the same degree as advanced use. Using routine BI features was also not found to improve user performance to the same extent as advanced use. The study has contributed new theoretical insights into BI use and offered new conceptual and operational definitions of routine and advanced use of BI, with implications for practice in contexts such as management accounting.                                                     business intelligence, management accounting, system use, IS success model, advanced use          IAM\n",
      "3  2024/01/01  How the Terminator might affect the car manufacturing industry: Examining the role of pre-announcement bias for AI-based IS adoptions  The steep development ofartificial intelligence(AI) is accompanied by a completely new set of challenges forinformation systems(IS) research and practice, especially in the area of individual-level technology adoption. In this article, we elaborate on the important role that biases play regarding the adoption of AI-based IS by individuals in awork environmentand for which, in addition, an alarmingly early occurrence within the overall process can be reported. Based on an exploratorycase studywithin three different German car manufacturers, we credit this work with a valuable threefold contribution. First, it introduces and substantiates knowledge and theory on the pre-announcement phase of individual-level IS adoption. Second, it is one of the very few works that thoroughly conceptualize and analyze the role of biases in userdecision makingresulting from the individuals’ cognitive limitations. And third, it indicates a notablespillover effectof experiences and opinions on AI from the users’ private lives to their professional ones, which stands in clear contrast to what usually is reported in IS research. The article thereby discusses general implications for IS theory but also elaborates on the AI-specific elements regarding IS implementations in a professional environment.  Individual-level technology adoption, Technology acceptance, User resistance, Pre-announcement phase, Bias, Bounded rationality, Prejudices          IAM\n",
      "\n",
      "============================================================\n",
      "✓ IAM.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. JIT.csv 만들기\n",
    "- 이전 코드 붙여서 인스턴스만 수정"
   ],
   "id": "1932ce643a88c411"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:23:50.872501Z",
     "start_time": "2025-10-22T04:23:50.821663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정: 여기만 수정하세요\n",
    "# ========================================\n",
    "VOL_NUMBERS = [38, 39, 40]  # 처리할 vol 번호 리스트\n",
    "START_YEAR = 2023  # 첫 번째 vol의 시작 연도\n",
    "JOURNAL_PREFIX = \"JIT\"  # 파일명 접두사 (예: misq, jmis, isj 등)\n",
    "# ========================================\n",
    "\n",
    "# 파일들이 있는 디렉토리 경로\n",
    "data_dir = \"/home/dslab/choi/Journal/Data/Academia/JIT\"\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "file_pattern = f\"{JOURNAL_PREFIX}_vol*.csv\"\n",
    "csv_files = glob(os.path.join(data_dir, file_pattern))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{file_pattern}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {data_dir}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 {JOURNAL_PREFIX.upper()} 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(vol, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    vol: volume 번호\n",
    "    iss: issue 번호 (1~4)\n",
    "    \"\"\"\n",
    "    # vol 번호에서 연도 계산\n",
    "    if vol in VOL_NUMBERS:\n",
    "        year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑\n",
    "    month_mapping = {\n",
    "        1: \"03\",\n",
    "        2: \"06\",\n",
    "        3: \"09\",\n",
    "        4: \"12\"\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    # 파일명에서 vol과 iss 추출\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.search(r'vol(\\d+)_iss(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        vol_num = int(match.group(1))\n",
    "        iss_num = int(match.group(2))\n",
    "\n",
    "        # CSV 파일 읽기\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # date 칼럼 추가\n",
    "            df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "            # affiliations 칼럼 추가 (저널명을 대문자로)\n",
    "            df['affiliations'] = JOURNAL_PREFIX.upper()\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            print(f\"처리: {filename:30s} -> date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행\")\n",
    "\n",
    "    # 1. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[1단계] Null 값 제거 (title, abstract 기준)\")\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 2. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 3. 최종 컬럼 선택\n",
    "    print(f\"\\n[3단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 4. 날짜별 통계\n",
    "    print(f\"\\n[4단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 5. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_PREFIX.upper()}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_PREFIX.upper()}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    print(final_df.head(3).to_string())\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "id": "2dd96acd990da10b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 11개의 JIT 파일 발견\n",
      "\n",
      "처리: JIT_vol38_iss1.csv             -> date: 2023-03, rows: 7\n",
      "처리: JIT_vol38_iss2.csv             -> date: 2023-06, rows: 7\n",
      "처리: JIT_vol38_iss3.csv             -> date: 2023-09, rows: 7\n",
      "처리: JIT_vol38_iss4.csv             -> date: 2023-12, rows: 6\n",
      "처리: JIT_vol39_iss1.csv             -> date: 2024-03, rows: 9\n",
      "처리: JIT_vol39_iss2.csv             -> date: 2024-06, rows: 6\n",
      "처리: JIT_vol39_iss3.csv             -> date: 2024-09, rows: 10\n",
      "처리: JIT_vol39_iss4.csv             -> date: 2024-12, rows: 8\n",
      "처리: JIT_vol40_iss1.csv             -> date: 2025-03, rows: 5\n",
      "처리: JIT_vol40_iss2.csv             -> date: 2025-06, rows: 6\n",
      "처리: JIT_vol40_iss3.csv             -> date: 2025-09, rows: 4\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 75개 행\n",
      "\n",
      "[1단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 75개 행\n",
      "  - 제거 후: 50개 행\n",
      "  - 제거됨: 25개 행\n",
      "\n",
      "[2단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 50개 행\n",
      "  - 제거 후: 50개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[3단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[4단계] 날짜별 통계\n",
      "  - 2023-03: 6개\n",
      "  - 2023-06: 3개\n",
      "  - 2023-09: 4개\n",
      "  - 2023-12: 4개\n",
      "  - 2024-03: 8개\n",
      "  - 2024-06: 2개\n",
      "  - 2024-09: 6개\n",
      "  - 2024-12: 5개\n",
      "  - 2025-03: 3개\n",
      "  - 2025-06: 6개\n",
      "  - 2025-09: 3개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/JIT.csv\n",
      "✓ 최종 행 수: 50개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: JIT\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                 title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract                                                                                                                     keywords affiliations\n",
      "0  2023-03                                                 Rethinking online friction in the information society                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A recurrent mantra of the technology industry is that all forms of ‘friction’ should be eliminated from online interactions (especially commercial transactions). In this context, ‘friction’ refers to any unnecessary retardation of a process or activity that delays the user accomplishing a desired action. This broad category can therefore include online adverts that link to the wrong webpages, pop-up windows that block access to content or delays in the physical delivery of an ordered item. Although visions of a frictionless future have been common since at least 1995 (the year Bill Gates popularised the phrase ‘friction-free capitalism’), the basic notion has remained unhelpfully vague. Accordingly, this article focuses specifically on the phenomenon of online friction (i.e. ‘e-friction’) and elaborates a typology of the main subtypes. An analytical framework of this kind makes it much easier to compare and contrast distinct kinds of e-friction, recognising that important differences distinguish those that are ‘elective’, ‘non-elective’, ‘impeding’, ‘distracting’ and so on. Having sketched a preliminary typology, the article reflects upon the ethical implications of the distinct varieties, and concludes by suggesting that there are several reasons why an entirely (e-)frictionless future is a profoundly disturbing one.                                 friction, ICTs and society, information technology, social networking, online communications          JIT\n",
      "1  2023-03  Building data management capabilities to address data protection regulations: Learnings from EU-GDPR  The European Union’s General Data Protection Regulation (EU-GDPR) has initiated a paradigm shift in data protection toward greater choice and sovereignty for individuals and more accountability for organizations. Its strict rules have inspired data protection regulations in other parts of the world. However, many organizations are facing difficulty complying with the EU-GDPR: these new types of data protection regulations cannot be addressed by an adaptation of contractual frameworks, but require a fundamental reconceptualization of how companies store and process personal data on an enterprise-wide level. In this paper, we introduce the resource-based view as a theoretical lens to explain the lengthy trajectories towards compliance and argue that these regulations require companies to build dedicated, enterprise-wide data management capabilities. Following a design science research approach, we propose a theoretically and empirically grounded capability model for the EU-GDPR that integrates the interpretation of legal texts, findings from EU-GDPR-related publications, and practical insights from focus groups with experts from 22 companies and four EU-GDPR projects. Our study advances interdisciplinary research at the intersection between IS and law: First, the proposed capability model adds to the regulatory compliance management literature by connecting abstract compliance requirements to three groups of capabilities and the resources required for their implementation, and second, it provides an enterprise-wide perspective that integrates and extends the fragmented body of research on EU-GDPR. Practitioners may use the capability model to assess their current status and set up systematic approaches toward compliance with an increasing number of data protection regulations.                                         EU-GDPR, data protection, regulations, compliance, resource-based view, capabilities          JIT\n",
      "2  2023-03                                                   The Pursuit of Innovative Theory in the Digital Age                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Grover & Lyytinen (2015) urged to reassess the Information System (IS) field’s exclusive dependence on reference theories and to engage more in blue-ocean theorizing. From its inception, such need has been latent in the field, because it deals with novel, fast changing, complex, and systemic phenomena that is hard to account with received theory. We note in this essay that the need for innovative theorizing is heightened given the unprecedented, pervasive digitalization of contemporary society, accelerated by ongoing COVID-19 pandemic. In this essay, we scrutinize further the idea of blue-ocean theorizing and review the characteristics, impediments, and merits of developing innovative theory. We define endeavors toward such theory as collectively endorsed cognitive processes which increase variance and novelty of theoretical accounts of IS phenomena. These push to deviate from the field’s established theoretical (canonical) core by relaxing six assumptions that guide dominant, legitimate forms of the field’s theorizing. We identify and review institutional barriers that curb the development of innovative theory. In conclusion, we offer guidelines for how the field and its stakeholders can productively engage in developing and evaluating innovative theory.  theory, blue-ocean, novelty, variance, digital phenomena, complexity, epistemic scripts, institutional barriers, publishing          JIT\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.JMIS.csv 만들기\n",
    "- vol 40~42, iss 1~4로 2023년~2025년까지 4분기로 나누어서 date 생성\n",
    "- 파일경로: /home/dslab/choi/Journal/Data/Academia/JMIS/JMIS_vol40_iss1.csv\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 JMIS로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "3ee19eba1880960d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T05:00:01.782195Z",
     "start_time": "2025-10-22T05:00:01.520367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"JMIS\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/JMIS/\"\n",
    "FILE_PATTERN = \"JMIS_vol*_iss*.csv\"\n",
    "# vol-year 매핑\n",
    "VOL_NUMBERS = [40, 41, 42]  # vol 40, 41, 42\n",
    "START_YEAR = 2023  # vol 40 = 2023년\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"JMIS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{FILE_PATTERN}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 JMIS 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(vol, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    vol 40 = 2023년, vol 41 = 2024년, vol 42 = 2025년\n",
    "    iss 1~4 = 3월, 6월, 9월, 12월\n",
    "    \"\"\"\n",
    "    if vol in VOL_NUMBERS:\n",
    "        year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑 (4분기)\n",
    "    month_mapping = {\n",
    "        1: \"03\",  # 1분기\n",
    "        2: \"06\",  # 2분기\n",
    "        3: \"09\",  # 3분기\n",
    "        4: \"12\"   # 4분기\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "print(\"[1단계] JMIS 파일 읽기 및 날짜 생성\")\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # 파일명에서 vol과 iss 추출\n",
    "    match = re.search(r'vol(\\d+)_iss(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        vol_num = int(match.group(1))\n",
    "        iss_num = int(match.group(2))\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # date 칼럼 추가\n",
    "            df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "            # affiliations 칼럼 추가\n",
    "            df['affiliations'] = JOURNAL_NAME\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            print(f\"  - {filename:35s} -> date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ 파일명 패턴 불일치: {filename}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 3. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "    # title과 abstract 컬럼 존재 여부 확인\n",
    "    required_cols = ['title', 'abstract']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "        print(f\"  현재 컬럼: {list(combined_df.columns)}\")\n",
    "        exit()\n",
    "\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 4. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[3단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 5. 최종 컬럼 선택\n",
    "    print(f\"\\n[4단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 6. 날짜별 통계\n",
    "    print(f\"\\n[5단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 7. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ Vol 범위: {min(VOL_NUMBERS)} - {max(VOL_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ JMIS.csv 생성 완료!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "id": "4b6c9850aef50ab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "JMIS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "✓ 11개의 JMIS 파일 발견\n",
      "\n",
      "[1단계] JMIS 파일 읽기 및 날짜 생성\n",
      "  - JMIS_vol40_iss1.csv                 -> date: 2023-03, rows: 12\n",
      "  - JMIS_vol40_iss2.csv                 -> date: 2023-06, rows: 13\n",
      "  - JMIS_vol40_iss3.csv                 -> date: 2023-09, rows: 12\n",
      "  - JMIS_vol40_iss4.csv                 -> date: 2023-12, rows: 12\n",
      "  - JMIS_vol41_iss1.csv                 -> date: 2024-03, rows: 12\n",
      "  - JMIS_vol41_iss2.csv                 -> date: 2024-06, rows: 11\n",
      "  - JMIS_vol41_iss3.csv                 -> date: 2024-09, rows: 11\n",
      "  - JMIS_vol41_iss4.csv                 -> date: 2024-12, rows: 12\n",
      "  - JMIS_vol42_iss1.csv                 -> date: 2025-03, rows: 11\n",
      "  - JMIS_vol42_iss2.csv                 -> date: 2025-06, rows: 11\n",
      "  - JMIS_vol42_iss3.csv                 -> date: 2025-09, rows: 11\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 128개 행 (11개 파일)\n",
      "\n",
      "[2단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 128개 행\n",
      "  - 제거 후: 128개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[3단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 128개 행\n",
      "  - 제거 후: 128개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[4단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[5단계] 날짜별 통계\n",
      "  - 2023-03: 12개\n",
      "  - 2023-06: 13개\n",
      "  - 2023-09: 12개\n",
      "  - 2023-12: 12개\n",
      "  - 2024-03: 12개\n",
      "  - 2024-06: 11개\n",
      "  - 2024-09: 11개\n",
      "  - 2024-12: 12개\n",
      "  - 2025-03: 11개\n",
      "  - 2025-06: 11개\n",
      "  - 2025-09: 11개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/JMIS.csv\n",
      "✓ 최종 행 수: 128개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: JMIS\n",
      "✓ Vol 범위: 40 - 42\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                         title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         abstract                                                                                                                                                keywords affiliations\n",
      "0  2023-03                                        Editorial Introduction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     At this milestone, we have every reason to look forward to a new decade of our intellectual development, and of our contribution to scholarship and society.                                                                                                                                                     NaN         JMIS\n",
      "1  2023-03                             Introduction to the Special Issue                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We conclude by expressing our gratitude to the authors, reviewers, and the Academia’s editorial team for their contributions. Each of the papers provides a unique perspective on the way in which information systems researchers contribute to our understanding of the role of information systems and methods in supporting value creation in communities and organizations. We warmly commend them to your reading and trust that they will inspire a broad array of future research.                                                                                                                                                     NaN         JMIS\n",
      "2  2023-03  Act and Reflect: Integrating Reflection into Design Thinking  Teams working on creative projects, such as design thinking, mostly face complex problems as well as challenging situations characterized by uniqueness and value conflicts. To cope with these characteristics, teams usually start doing something by drawing on their current store of experiences and professional knowledge, and then (re-)assess the outcomes produced, and adjust future actions based on insights obtained during the process. In reflecting on actions, tacit knowledge is revealed that enables designers to handle challenging situations. Although there is great potential to support design thinking by adding a reflection lens, we lack guidance on how, when, and on what to perform reflection. Based on scientific and theoretical literature, semi-structured interviews, a case study and a software prototype, prescriptive design knowledge on how to integrate reflection into design thinking is deduced, which enriches the scarce body of knowledge at the intersection of reflection and (digital) design thinking.  Reflection-on-action, reflection-in-action, reflective practice, design thinking, software tools, creativity, design science research, tacit knowledge         JMIS\n",
      "\n",
      "============================================================\n",
      "✓ JMIS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7.EJIS.csv 만들기\n",
    "- vol 32 ~ 34, iss 1~6 -> 2,4,6,8,10,12월로 일단 저장 진행\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 JMIS로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "3924beadb20c6d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T05:14:08.947874Z",
     "start_time": "2025-10-22T05:14:08.880901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"EJIS\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/EJIS\"  # EJIS 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"EJIS_vol*_iss*.csv\"  # 또는 \"EJIS_vol*_iss*.csv\"\n",
    "# vol-year 매핑 (예시: vol 32, 33, 34 = 2023, 2024, 2025)\n",
    "VOL_NUMBERS = [32, 33, 34]  # 처리할 vol 번호 리스트 (필요시 수정)\n",
    "START_YEAR = 2023  # 첫 번째 vol의 시작 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"EJIS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{FILE_PATTERN}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    print(f\"다른 파일명 패턴이라면 코드 상단의 FILE_PATTERN을 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 EJIS 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(vol, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    iss 1-6을 2개월 단위로 균등 분배\n",
    "    iss 1 → 2월 (1-2월)\n",
    "    iss 2 → 4월 (3-4월)\n",
    "    iss 3 → 6월 (5-6월)\n",
    "    iss 4 → 8월 (7-8월)\n",
    "    iss 5 → 10월 (9-10월)\n",
    "    iss 6 → 12월 (11-12월)\n",
    "    \"\"\"\n",
    "    if vol in VOL_NUMBERS:\n",
    "        year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑 (2개월 단위 균등 분배)\n",
    "    month_mapping = {\n",
    "        1: \"02\",  # iss 1 → 1-2월\n",
    "        2: \"04\",  # iss 2 → 3-4월\n",
    "        3: \"06\",  # iss 3 → 5-6월\n",
    "        4: \"08\",  # iss 4 → 7-8월\n",
    "        5: \"10\",  # iss 5 → 9-10월\n",
    "        6: \"12\"   # iss 6 → 11-12월\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "print(\"[1단계] EJIS 파일 읽기 및 날짜 생성\")\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # 파일명에서 vol과 iss 추출\n",
    "    match = re.search(r'vol(\\d+)_iss(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        vol_num = int(match.group(1))\n",
    "        iss_num = int(match.group(2))\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # date 칼럼 추가\n",
    "            df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "            # affiliations 칼럼 추가\n",
    "            df['affiliations'] = JOURNAL_NAME\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            print(f\"  - {filename:35s} -> vol: {vol_num}, iss: {iss_num}, date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ 파일명 패턴 불일치: {filename}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 3. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "    # title과 abstract 컬럼 존재 여부 확인\n",
    "    required_cols = ['title', 'abstract']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "        print(f\"  현재 컬럼: {list(combined_df.columns)}\")\n",
    "        exit()\n",
    "\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 4. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[3단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 5. 최종 컬럼 선택\n",
    "    print(f\"\\n[4단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 6. 날짜별 통계\n",
    "    print(f\"\\n[5단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 7. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ Vol 범위: {min(VOL_NUMBERS)} - {max(VOL_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "    print(f\"✓ Issue 매핑: iss 1→2월, iss 2→4월, iss 3→6월, iss 4→8월, iss 5→10월, iss 6→12월\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ EJIS.csv 생성 완료!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "id": "cf64812e130fe022",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EJIS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "✓ 18개의 EJIS 파일 발견\n",
      "\n",
      "[1단계] EJIS 파일 읽기 및 날짜 생성\n",
      "  - EJIS_vol32_iss1.csv                 -> vol: 32, iss: 1, date: 2023-02, rows: 8\n",
      "  - EJIS_vol32_iss2.csv                 -> vol: 32, iss: 2, date: 2023-04, rows: 13\n",
      "  - EJIS_vol32_iss3.csv                 -> vol: 32, iss: 3, date: 2023-06, rows: 13\n",
      "  - EJIS_vol32_iss4.csv                 -> vol: 32, iss: 4, date: 2023-08, rows: 7\n",
      "  - EJIS_vol32_iss5.csv                 -> vol: 32, iss: 5, date: 2023-10, rows: 8\n",
      "  - EJIS_vol32_iss6.csv                 -> vol: 32, iss: 6, date: 2023-12, rows: 8\n",
      "  - EJIS_vol33_iss1.csv                 -> vol: 33, iss: 1, date: 2024-02, rows: 5\n",
      "  - EJIS_vol33_iss2.csv                 -> vol: 33, iss: 2, date: 2024-04, rows: 8\n",
      "  - EJIS_vol33_iss3.csv                 -> vol: 33, iss: 3, date: 2024-06, rows: 7\n",
      "  - EJIS_vol33_iss4.csv                 -> vol: 33, iss: 4, date: 2024-08, rows: 8\n",
      "  - EJIS_vol33_iss5.csv                 -> vol: 33, iss: 5, date: 2024-10, rows: 9\n",
      "  - EJIS_vol33_iss6.csv                 -> vol: 33, iss: 6, date: 2024-12, rows: 10\n",
      "  - EJIS_vol34_iss1.csv                 -> vol: 34, iss: 1, date: 2025-02, rows: 8\n",
      "  - EJIS_vol34_iss2.csv                 -> vol: 34, iss: 2, date: 2025-04, rows: 9\n",
      "  - EJIS_vol34_iss3.csv                 -> vol: 34, iss: 3, date: 2025-06, rows: 8\n",
      "  - EJIS_vol34_iss4.csv                 -> vol: 34, iss: 4, date: 2025-08, rows: 9\n",
      "  - EJIS_vol34_iss5.csv                 -> vol: 34, iss: 5, date: 2025-10, rows: 7\n",
      "  - EJIS_vol34_iss6.csv                 -> vol: 34, iss: 6, date: 2025-12, rows: 20\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 165개 행 (18개 파일)\n",
      "\n",
      "[2단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 165개 행\n",
      "  - 제거 후: 165개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[3단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 165개 행\n",
      "  - 제거 후: 165개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[4단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[5단계] 날짜별 통계\n",
      "  - 2023-02: 8개\n",
      "  - 2023-04: 13개\n",
      "  - 2023-06: 13개\n",
      "  - 2023-08: 7개\n",
      "  - 2023-10: 8개\n",
      "  - 2023-12: 8개\n",
      "  - 2024-02: 5개\n",
      "  - 2024-04: 8개\n",
      "  - 2024-06: 7개\n",
      "  - 2024-08: 8개\n",
      "  - 2024-10: 9개\n",
      "  - 2024-12: 10개\n",
      "  - 2025-02: 8개\n",
      "  - 2025-04: 9개\n",
      "  - 2025-06: 8개\n",
      "  - 2025-08: 9개\n",
      "  - 2025-10: 7개\n",
      "  - 2025-12: 20개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/EJIS.csv\n",
      "✓ 최종 행 수: 165개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: EJIS\n",
      "✓ Vol 범위: 32 - 34\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "✓ Issue 매핑: iss 1→2월, iss 2→4월, iss 3→6월, iss 4→8월, iss 5→10월, iss 6→12월\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                             title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract                                                                                                     keywords affiliations\n",
      "0  2023-02                                                               Clinical research from information systems practice                                                                       An increasing presence of practitioners with doctoral degrees in Information Systems and related disciplines holds promise to advance Information Systems research. The prospect is to gain more knowledge from the practical experience of developing, using, and managing information systems in context. To scientifically capitalise on this opportunity, this EJIS special issue introduces the research genre of “Information Systems Clinical Research”. The genre presents knowledge generated from practitioner-researcher interventions to achieve desired outcomes in information systems development, use, and management practice contexts. In this editorial, we introduce and conceptualise the genre; we present a research framework that defines its four key elements; and we discuss how to address its key challenges in research projects. As a result, we derive ten criteria for rigorous Information Systems Clinical Research and provide examples on how the articles published in the special issue have addressed these criteria. We conclude with a call to further advance clinical research as an important part of the Information Systems discipline.                                                             clinical research, information systems, practice         EJIS\n",
      "1  2023-02  Developing human/AI interactions for chat-based customer services: lessons learned from the Norwegian government  Advancements in human/AI interactions led to smartification of public services via the use of chatbots. Here, we present findings from a clinical inquiry research project in a key public service organisation in Norway. In this project, researchers and practitioners worked together to generate insights on the action possibilities offered to human service agents by chatbots and the potential for creating hybrid human/AI service teams. The project sensitised service agents to discover affordances based on their actual practices, rather than on the predefined use of chatbots. The different affordances identified can be useful for practitioners who design and deploy chatbot-based services. The action possibilities afforded by chatbots provide new ways for service agents and chatbots to work as a team addressing citizens’ needs. Drawing from the whole research process, we offer three lessons learned from the Norwegian Government on human/AI partnerships, theory-based interventions, and institutionalised collaborative research that can be useful for researchers that want to engage with practice and organisations that want to evolve their technology use, stimulate innovation, and engage with research.  Affordances, augmentation, automation, chatbot, clinical research, Research Champion, human–AI partnerships         EJIS\n",
      "2  2023-02               Unpacking digital options thinking for innovation renewal: a clinical inquiry into car connectivity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Options thinking is a powerful approach for managing uncertainty and change in digital environments. It is recognised as a structured process for identifying, developing, and realising options into novel products and services. At Volvo Cars, we have learned it can also become a powerful instrument for innovation renewal, although it can be difficult to apply because it challenges existing firm practices. We elaborate this tension by presenting digital options strategizing as a process of applying options thinking to negotiate capability gaps and configure innovation resources. Our clinical study reveals that this facilitates innovation renewal through emergent processes, practice-oriented design, and opportunity-driven management.                       Digital options, digital innovation, clinical inquiry, capability gaps, connected cars         EJIS\n",
      "\n",
      "============================================================\n",
      "✓ EJIS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8.JAIS.csv 만들기\n",
    "- vol 24 ~ 26, iss 1~6 -> 2,4,6,8,10,12월로 일단 저장 진행\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 JAIS로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "542831825c2fb4fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T05:20:58.817434Z",
     "start_time": "2025-10-22T05:20:58.729980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"JAIS\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/JAIS\"  # EJIS 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"JAIS_vol*_iss*.csv\"  # 또는 \"EJIS_vol*_iss*.csv\"\n",
    "# vol-year 매핑 (예시: vol 32, 33, 34 = 2023, 2024, 2025)\n",
    "VOL_NUMBERS = [24, 25, 26]  # 처리할 vol 번호 리스트 (필요시 수정)\n",
    "START_YEAR = 2023  # 첫 번째 vol의 시작 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"JAIS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ '{FILE_PATTERN}' 패턴의 CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    print(f\"다른 파일명 패턴이라면 코드 상단의 FILE_PATTERN을 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 EJIS 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(vol, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    iss 1-6을 2개월 단위로 균등 분배\n",
    "    iss 1 → 2월 (1-2월)\n",
    "    iss 2 → 4월 (3-4월)\n",
    "    iss 3 → 6월 (5-6월)\n",
    "    iss 4 → 8월 (7-8월)\n",
    "    iss 5 → 10월 (9-10월)\n",
    "    iss 6 → 12월 (11-12월)\n",
    "    \"\"\"\n",
    "    if vol in VOL_NUMBERS:\n",
    "        year = START_YEAR + (VOL_NUMBERS.index(vol))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (vol - VOL_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑 (2개월 단위 균등 분배)\n",
    "    month_mapping = {\n",
    "        1: \"02\",  # iss 1 → 1-2월\n",
    "        2: \"04\",  # iss 2 → 3-4월\n",
    "        3: \"06\",  # iss 3 → 5-6월\n",
    "        4: \"08\",  # iss 4 → 7-8월\n",
    "        5: \"10\",  # iss 5 → 9-10월\n",
    "        6: \"12\"   # iss 6 → 11-12월\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "print(\"[1단계] EJIS 파일 읽기 및 날짜 생성\")\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # 파일명에서 vol과 iss 추출\n",
    "    match = re.search(r'vol(\\d+)_iss(\\d+)', filename)\n",
    "\n",
    "    if match:\n",
    "        vol_num = int(match.group(1))\n",
    "        iss_num = int(match.group(2))\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # date 칼럼 추가\n",
    "            df['date'] = get_date(vol_num, iss_num)\n",
    "\n",
    "            # affiliations 칼럼 추가\n",
    "            df['affiliations'] = JOURNAL_NAME\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "            print(f\"  - {filename:35s} -> vol: {vol_num}, iss: {iss_num}, date: {df['date'].iloc[0]}, rows: {len(df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ 파일명 패턴 불일치: {filename}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 3. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[2단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "    # title과 abstract 컬럼 존재 여부 확인\n",
    "    required_cols = ['title', 'abstract']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "        print(f\"  현재 컬럼: {list(combined_df.columns)}\")\n",
    "        exit()\n",
    "\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 4. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[3단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 5. 최종 컬럼 선택\n",
    "    print(f\"\\n[4단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 6. 날짜별 통계\n",
    "    print(f\"\\n[5단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 7. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ Vol 범위: {min(VOL_NUMBERS)} - {max(VOL_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(VOL_NUMBERS) - 1}\")\n",
    "    print(f\"✓ Issue 매핑: iss 1→2월, iss 2→4월, iss 3→6월, iss 4→8월, iss 5→10월, iss 6→12월\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ EJIS.csv 생성 완료!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 처리할 데이터가 없습니다.\")"
   ],
   "id": "1082ab7dedba6265",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "JAIS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "✓ 17개의 EJIS 파일 발견\n",
      "\n",
      "[1단계] EJIS 파일 읽기 및 날짜 생성\n",
      "  - JAIS_vol24_iss1.csv                 -> vol: 24, iss: 1, date: 2023-02, rows: 10\n",
      "  - JAIS_vol24_iss2.csv                 -> vol: 24, iss: 2, date: 2023-04, rows: 11\n",
      "  - JAIS_vol24_iss3.csv                 -> vol: 24, iss: 3, date: 2023-06, rows: 10\n",
      "  - JAIS_vol24_iss4.csv                 -> vol: 24, iss: 4, date: 2023-08, rows: 9\n",
      "  - JAIS_vol24_iss5.csv                 -> vol: 24, iss: 5, date: 2023-10, rows: 11\n",
      "  - JAIS_vol24_iss6.csv                 -> vol: 24, iss: 6, date: 2023-12, rows: 10\n",
      "  - JAIS_vol25_iss1.csv                 -> vol: 25, iss: 1, date: 2024-02, rows: 15\n",
      "  - JAIS_vol25_iss2.csv                 -> vol: 25, iss: 2, date: 2024-04, rows: 10\n",
      "  - JAIS_vol25_iss3.csv                 -> vol: 25, iss: 3, date: 2024-06, rows: 11\n",
      "  - JAIS_vol25_iss4.csv                 -> vol: 25, iss: 4, date: 2024-08, rows: 9\n",
      "  - JAIS_vol25_iss5.csv                 -> vol: 25, iss: 5, date: 2024-10, rows: 10\n",
      "  - JAIS_vol25_iss6.csv                 -> vol: 25, iss: 6, date: 2024-12, rows: 10\n",
      "  - JAIS_vol26_iss1.csv                 -> vol: 26, iss: 1, date: 2025-02, rows: 10\n",
      "  - JAIS_vol26_iss2.csv                 -> vol: 26, iss: 2, date: 2025-04, rows: 10\n",
      "  - JAIS_vol26_iss3.csv                 -> vol: 26, iss: 3, date: 2025-06, rows: 10\n",
      "  - JAIS_vol26_iss4.csv                 -> vol: 26, iss: 4, date: 2025-08, rows: 10\n",
      "  - JAIS_vol26_iss5.csv                 -> vol: 26, iss: 5, date: 2025-10, rows: 10\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 176개 행 (17개 파일)\n",
      "\n",
      "[2단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 176개 행\n",
      "  - 제거 후: 166개 행\n",
      "  - 제거됨: 10개 행\n",
      "\n",
      "[3단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 166개 행\n",
      "  - 제거 후: 166개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[4단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[5단계] 날짜별 통계\n",
      "  - 2023-02: 10개\n",
      "  - 2023-04: 10개\n",
      "  - 2023-06: 9개\n",
      "  - 2023-08: 9개\n",
      "  - 2023-10: 10개\n",
      "  - 2023-12: 8개\n",
      "  - 2024-02: 14개\n",
      "  - 2024-04: 10개\n",
      "  - 2024-06: 10개\n",
      "  - 2024-08: 9개\n",
      "  - 2024-10: 9개\n",
      "  - 2024-12: 9개\n",
      "  - 2025-02: 10개\n",
      "  - 2025-04: 10개\n",
      "  - 2025-06: 10개\n",
      "  - 2025-08: 9개\n",
      "  - 2025-10: 10개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/JAIS.csv\n",
      "✓ 최종 행 수: 166개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: JAIS\n",
      "✓ Vol 범위: 24 - 26\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "✓ Issue 매핑: iss 1→2월, iss 2→4월, iss 3→6월, iss 4→8월, iss 5→10월, iss 6→12월\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                             title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               abstract                                                                                                                                                    keywords affiliations\n",
      "0  2023-02  Reconciling the Personalization-Privacy Paradox: Exploring Privacy Boundaries in Online Personalized Advertising                                                                                                                                                                                                                                                  To reconcile the personalization-privacy paradox, we adopt the privacy as a state view and define privacy as a state of information boundary rule-following. We further identify five types of boundaries underlying some of the important implicit rules of maintaining privacy: communication channel, platform, device, temporal, and purpose boundaries. Using an online vignette survey, we investigated how each of these boundary types affected users’ privacy perceptions when they were subjected to personalized advertisements. Using fixed- and random-effects models, we investigated how violating different boundary rules leads to changes in perceived privacy. Our results show that all five boundary types are significant predictors of perceived privacy within individuals. The communication channel, device, and business versus private purpose are significant predictors of perceived privacy across the whole sample. Temporal boundaries and platform boundaries failed to achieve statistical significance when evaluated simultaneously with the other factors across the whole sample. This means that for each individual, observing the rules of these five boundary types leads to higher perceived privacy than not observing these conditions. Taken as a whole, observing communication channel, device, and business versus private purpose boundaries also leads to higher averages of perceived privacy across the whole sample. Theoretical and practical implications are discussed based on the results                                                      Information Privacy, Privacy as a State, Privacy Boundary Maintenance, Personalization-Privacy Paradox         JAIS\n",
      "1  2023-02                                                          On Scholarly Composition: From Acceptable to Exceptional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Scholarly writing is a difficult skill to develop. This editorial presents our observations on how to move from acceptable to exceptional writing in academic manuscripts. We discuss three phases of writing—the predrafting, drafting, and postdrafting phases—and provide suggestions based on our experiences for improving the quality of academic manuscripts prior to their submission.                                                                                                             Writing, Academic Papers, Composition, Drafting         JAIS\n",
      "2  2023-02              Social Movement Sustainability on Social Media: An Analysis of the Women’s March Movement on Twitter  Social media has emerged as a powerful medium to organize and mobilize social movements. In particular, the connective action of social media builds associations and allows for the continuity of social movements. Yet there is a lack of research on how connective action emergent from social media messages sustains long-term social movements. Accordingly, in this study, we concentrate on Twitter messages related to Women’s March protests held in 2017, 2018, and 2019. Using an interpretive analysis followed by the topic modeling approach, we analyzed the tweets to identify the different types of messages associated with the movement. These messages were classified using a set of categories and subcategories. Furthermore, we conducted a temporal analysis of the message (sub)categories to understand how distinct messages allow movement continuity beyond a specific protest march. Results suggest that while most of the messages are used to motivate and mobilize individuals, the connective action tactics employed through messages sent before, during, and after the marches allowed Women’s March to become a broader and more persistent movement. We advance theoretical propositions to explain the sustainability of a long-term social movement on social media, exemplified through large-scale connective action that persists over time. In doing so, this study contributes to connective action research by providing message categorization that synthesizes the meaning of message content. The findings could help social movement organizers learn different ways to frame messages that resonate with broader social media users. Moreover, our approach to analyzing a large set of tweets might interest other qualitative researchers.  Social Movement, Framing, Mobilization, Connective Action, Movement Sustainability, Social Media, Interpretive Analysis, Topic Modeling, Temporal Analysis         JAIS\n",
      "\n",
      "============================================================\n",
      "✓ EJIS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9.JSIS.csv 만들기\n",
    "- 데이터에 있는 vol 32 ~ 34, iss 1~4을 2023년 ~ 2025년, 3월, 6월, 9월, 12월로 나눠서 date 저장\n",
    "- null 값 -> title, abstract 기준으로 제거\n",
    "- 중복 제거 -> title, abstract 기준으로 제거\n",
    "- affiliations 생성 -> 파일이름 참고해서 JSIS.csv로 저장\n",
    "- 최종 컬럼 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "c2b1e0965bd96a2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T05:34:23.902570Z",
     "start_time": "2025-10-22T05:34:23.848396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"JSIS\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/JSIS\"  # JSIS 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"*.csv\"  # JSIS 관련 csv 파일들\n",
    "# volume-year 매핑\n",
    "volume_NUMBERS = [32, 33, 34]  # volume 32, 33, 34\n",
    "START_YEAR = 2023  # volume 32 = 2023년\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"JSIS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 CSV 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(volume, issue):\n",
    "    \"\"\"\n",
    "    volume과 issue 번호로 날짜 생성\n",
    "    volume 32 = 2023년, volume 33 = 2024년, volume 34 = 2025년\n",
    "    issue 1~4 = 3월, 6월, 9월, 12월\n",
    "    \"\"\"\n",
    "    if pd.isna(volume) or pd.isna(issue):\n",
    "        return None\n",
    "\n",
    "    volume = int(volume)\n",
    "    issue = int(issue)\n",
    "\n",
    "    if volume in volume_NUMBERS:\n",
    "        year = START_YEAR + (volume_NUMBERS.index(volume))\n",
    "    else:\n",
    "        # volume_NUMBERS에 없는 경우, 첫 번째 volume을 기준으로 계산\n",
    "        year = START_YEAR + (volume - volume_NUMBERS[0])\n",
    "\n",
    "    # issue에 따른 월 매핑 (4분기)\n",
    "    month_mapping = {\n",
    "        1: \"03\",  # 1분기\n",
    "        2: \"06\",  # 2분기\n",
    "        3: \"09\",  # 3분기\n",
    "        4: \"12\"   # 4분기\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(issue, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "print(\"[1단계] JSIS 파일 읽기\")\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # volume과 issue 컬럼이 있는지 확인\n",
    "        if 'volume' in df.columns and 'issue' in df.columns:\n",
    "            all_dfs.append(df)\n",
    "            print(f\"  ✓ {filename:40s} -> {len(df)}개 행 (volume, issue 컬럼 있음)\")\n",
    "        else:\n",
    "            print(f\"  - {filename:40s} -> volume/issue 컬럼 없음 (건너뜀)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 3. date 칼럼 생성\n",
    "    print(f\"\\n[2단계] date 칼럼 생성 (volume, issue 기반)\")\n",
    "    combined_df['date'] = combined_df.apply(lambda row: get_date(row['volume'], row['issue']), axis=1)\n",
    "\n",
    "    # date 생성 통계\n",
    "    date_created = combined_df['date'].notna().sum()\n",
    "    date_null = combined_df['date'].isna().sum()\n",
    "    print(f\"  - date 생성 완료: {date_created}개\")\n",
    "    if date_null > 0:\n",
    "        print(f\"  ⚠ date 생성 실패: {date_null}개 (volume/issue 값이 없음)\")\n",
    "\n",
    "    # 4. affiliations 칼럼 생성\n",
    "    print(f\"\\n[3단계] affiliations 생성\")\n",
    "    combined_df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "    # 5. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "    # title과 abstract 컬럼 존재 여부 확인\n",
    "    required_cols = ['title', 'abstract']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "        print(f\"  현재 컬럼: {list(combined_df.columns)}\")\n",
    "        exit()\n",
    "\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 6. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[5단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 7. 최종 컬럼 선택\n",
    "    print(f\"\\n[6단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 8. 날짜별 통계\n",
    "    print(f\"\\n[7단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 9. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ volume 범위: {min(volume_NUMBERS)} - {max(volume_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(volume_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ JSIS.csv 생성 완료!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ volume, issue 컬럼이 있는 CSV 파일을 찾을 수 없습니다.\")"
   ],
   "id": "2888d235c86fe65c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "JSIS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "✓ 8개의 CSV 파일 발견\n",
      "\n",
      "[1단계] JSIS 파일 읽기\n",
      "  ✓ JSIS_vol32_iss1to4.csv                   -> 28개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol32to32_iss1to4.csv               -> 28개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol33_iss1to4.csv                   -> 30개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol33to33_iss1to4.csv               -> 30개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol34_iss1to2.csv                   -> 21개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol34_iss3to4.csv                   -> 11개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol34to34_iss1to2.csv               -> 21개 행 (volume, issue 컬럼 있음)\n",
      "  ✓ JSIS_vol34to34_iss3to4.csv               -> 11개 행 (volume, issue 컬럼 있음)\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 180개 행 (8개 파일)\n",
      "\n",
      "[2단계] date 칼럼 생성 (volume, issue 기반)\n",
      "  - date 생성 완료: 180개\n",
      "\n",
      "[3단계] affiliations 생성\n",
      "  - affiliations: JSIS\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 180개 행\n",
      "  - 제거 후: 130개 행\n",
      "  - 제거됨: 50개 행\n",
      "\n",
      "[5단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 130개 행\n",
      "  - 제거 후: 65개 행\n",
      "  - 제거됨: 65개 행\n",
      "\n",
      "[6단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[7단계] 날짜별 통계\n",
      "  - 2023-03: 4개\n",
      "  - 2023-06: 5개\n",
      "  - 2023-09: 6개\n",
      "  - 2023-12: 4개\n",
      "  - 2024-03: 6개\n",
      "  - 2024-06: 6개\n",
      "  - 2024-09: 5개\n",
      "  - 2024-12: 4개\n",
      "  - 2025-03: 7개\n",
      "  - 2025-06: 9개\n",
      "  - 2025-09: 7개\n",
      "  - 2025-12: 2개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/JSIS.csv\n",
      "✓ 최종 행 수: 65개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: JSIS\n",
      "✓ volume 범위: 32 - 34\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                               title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract                                                                                                                                                 keywords affiliations\n",
      "3  2023-03          Digital new market creation by incumbent firms: A political lens on the effect of formalization on agility                                                                                                                                                                    Digital new market creationhas several advantages for incumbent firms (hereafter referred to as ‘incumbents’) that they seek to exploit by using formalization and implementing agility in their new product development (NPD). We introduce the construct ofNPD decision agilityencompassing the dimensions of sensemaking, speed, and iteration. However, research reveals heterogeneous insights intoformalization’s suitability for digital new market creation and NPD decision agility. In response to this research gap, we test our hypotheses by applying a behavioral lens to a sample of 129 incumbents. We reconcile these heterogeneous insights on innovation by showing that formalization increases digital new market creation, but has more fine-grained effects on NPD decision agility. Although improving sensemaking and reducing iteration, formalization has no effect on speed. Furthermore, political behavior increases formalization’s negative effect on iteration. We contribute to research digitalization and agility at the intersection of information systems and innovation management by reconciling these heterogeneous insights.  Agility, Decision agility, Digital new market creation, Digital innovation, Formalization, Incumbent firms, New product development, Political behavior         JSIS\n",
      "4  2023-03  Digital transformation in high-reliability organizations: A longitudinal study of the micro-foundations of failure                                                                                            High-reliability organizations (HROs) and their complexoperating modelshave been a focus of scholarly work for more than three decades. Recently, HROs have been challenged by new market pressures that require them to digitally transform in ways that affect their identity and value creation models while still maintaining high levels of security and efficiency. This longitudinal, in-depth single-case study of a major European utility company examines the role of HRO identity in digital transformation (DT), specifically in terms of tensions between innovation and transformation on the one hand, and maintainingreliable operationson the other. Our findings show how tensions between HROs’ identity and key features of DT give rise to threat perceptions and self-protective behaviors by the IT workforce, that eventually may derail the transformation process. We develop a process model that highlights the sources and consequences of identity misalignment during major DT initiatives in HROs. In doing so, we extend the research on D T by highlighting the importance of bottom-up processes for DT success and failure, especially concerning the IT function’s perception of organizational identity.                   High-reliability organizations, Digital transformation, Organizational identity, Human dynamics, Self-protective behavioral strategies         JSIS\n",
      "5  2023-03                   Digital workplace transformation: Subtraction logic as deinstitutionalising the taken-for-granted  Digital technology enables the transformation of work and workplaces. Previous digital workplace transformation (DWT) literature has shown how organisations add new digital technologies to create new workplace routines. However, such an emphasis on addition may hinder scholarship from recognising that some established workplace technologies and routines must disappear for new ones to emerge. Adopting the concept of deinstitutionalisation, we examine the rationale for and the process of how an organisation abandons workplace routines that conflict with its intended DWT. Referring to this as subtraction logic, we advance two contributions. First, we conceptualise how deinstitutionalisation of established workplace routines and technologies unfolds in DWT by outlining a process model that synthesises addition and subtraction. Second, we highlight the underlying rationales for DWT. With these insights, we shift the gaze from the dominant addition logic, which advocates for appropriating new digital technologies, to the equally important value of subtraction, i.e., removing existing workplace technologies (or inscribed institutional rules) to abandon workplace routines that conflict with the intended DWT. Hence, our study highlights the oft-ignored subtraction logic in DWT.               Digital transformation, Digital workplace transformation, Subtraction logic, Addition logic, Deinstitutionalization, Routines, Ethnography         JSIS\n",
      "\n",
      "============================================================\n",
      "✓ JSIS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10.ISJ.csv 만들기\n",
    "- vol, iss 데이터에 있는 거로 date 만들기\n",
    "- 나머지 동일하게 진행."
   ],
   "id": "c215ca57a5c472ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T06:40:13.779236Z",
     "start_time": "2025-10-22T06:40:13.726617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"ISJ\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/ISJ\"  # ISJ 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"*.csv\"  # ISJ 관련 csv 파일들\n",
    "# volume-year 매핑\n",
    "volume_NUMBERS = [33, 34, 35] # volume 32, 33, 34\n",
    "START_YEAR = 2023  # volume 32 = 2023년\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ISJ.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# CSV 파일들 찾기\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"⚠ CSV 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 CSV 파일 발견\\n\")\n",
    "\n",
    "# 날짜 매핑 함수\n",
    "def get_date(volume, iss):\n",
    "    \"\"\"\n",
    "    vol과 iss 번호로 날짜 생성\n",
    "    iss 1-6을 2개월 단위로 균등 분배\n",
    "    iss 1 → 2월 (1-2월)\n",
    "    iss 2 → 4월 (3-4월)\n",
    "    iss 3 → 6월 (5-6월)\n",
    "    iss 4 → 8월 (7-8월)\n",
    "    iss 5 → 10월 (9-10월)\n",
    "    iss 6 → 12월 (11-12월)\n",
    "    \"\"\"\n",
    "    if volume in volume_NUMBERS:\n",
    "        year = START_YEAR + (volume_NUMBERS.index(volume))\n",
    "    else:\n",
    "        # VOL_NUMBERS에 없는 경우, 첫 번째 vol을 기준으로 계산\n",
    "        year = START_YEAR + (volume - volume_NUMBERS[0])\n",
    "\n",
    "    # iss에 따른 월 매핑 (2개월 단위 균등 분배)\n",
    "    month_mapping = {\n",
    "        1: \"02\",  # iss 1 → 1-2월\n",
    "        2: \"04\",  # iss 2 → 3-4월\n",
    "        3: \"06\",  # iss 3 → 5-6월\n",
    "        4: \"08\",  # iss 4 → 7-8월\n",
    "        5: \"10\",  # iss 5 → 9-10월\n",
    "        6: \"12\"   # iss 6 → 11-12월\n",
    "    }\n",
    "\n",
    "    month = month_mapping.get(iss, \"03\")\n",
    "    return f\"{year}-{month}\"\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "print(\"[1단계] ISJ 파일 읽기\")\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # volume과 issue 컬럼이 있는지 확인\n",
    "        if 'volume' in df.columns and 'issue' in df.columns:\n",
    "            all_dfs.append(df)\n",
    "            print(f\"  ✓ {filename:40s} -> {len(df)}개 행 (volume, issue 컬럼 있음)\")\n",
    "        else:\n",
    "            print(f\"  - {filename:40s} -> volume/issue 컬럼 없음 (건너뜀)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 모든 데이터프레임 합치기\n",
    "if all_dfs:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "    # 3. date 칼럼 생성\n",
    "    print(f\"\\n[2단계] date 칼럼 생성 (volume, issue 기반)\")\n",
    "    combined_df['date'] = combined_df.apply(lambda row: get_date(row['volume'], row['issue']), axis=1)\n",
    "\n",
    "    # date 생성 통계\n",
    "    date_created = combined_df['date'].notna().sum()\n",
    "    date_null = combined_df['date'].isna().sum()\n",
    "    print(f\"  - date 생성 완료: {date_created}개\")\n",
    "    if date_null > 0:\n",
    "        print(f\"  ⚠ date 생성 실패: {date_null}개 (volume/issue 값이 없음)\")\n",
    "\n",
    "    # 4. affiliations 칼럼 생성\n",
    "    print(f\"\\n[3단계] affiliations 생성\")\n",
    "    combined_df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "    # 5. null 값 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "    # title과 abstract 컬럼 존재 여부 확인\n",
    "    required_cols = ['title', 'abstract']\n",
    "    missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "        print(f\"  현재 컬럼: {list(combined_df.columns)}\")\n",
    "        exit()\n",
    "\n",
    "    before_null = len(combined_df)\n",
    "    combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "    after_null = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_null}개 행\")\n",
    "    print(f\"  - 제거 후: {after_null}개 행\")\n",
    "    print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "    # 6. 중복 제거 (title, abstract 기준)\n",
    "    print(f\"\\n[5단계] 중복 제거 (title, abstract 기준)\")\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "    after_dup = len(combined_df)\n",
    "    print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "    print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "    print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "    # 7. 최종 컬럼 선택\n",
    "    print(f\"\\n[6단계] 최종 컬럼 선택\")\n",
    "    required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "    # 존재하는 컬럼만 선택\n",
    "    available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "        for col in missing_columns:\n",
    "            combined_df[col] = None\n",
    "\n",
    "    final_df = combined_df[required_columns].copy()\n",
    "    print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "    # 8. 날짜별 통계\n",
    "    print(f\"\\n[7단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "    for date, count in date_stats.items():\n",
    "        print(f\"  - {date}: {count}개\")\n",
    "\n",
    "    # 9. 결과 저장\n",
    "    output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "    output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia\", output_filename)\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ 최종 저장 완료!\")\n",
    "    print(f\"✓ 파일 경로: {output_path}\")\n",
    "    print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "    print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "    print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "    print(f\"✓ volume 범위: {min(volume_NUMBERS)} - {max(volume_NUMBERS)}\")\n",
    "    print(f\"✓ 연도 범위: {START_YEAR} - {START_YEAR + len(volume_NUMBERS) - 1}\")\n",
    "\n",
    "    # 샘플 데이터 미리보기\n",
    "    print(f\"\\n[데이터 미리보기]\")\n",
    "    if len(final_df) > 0:\n",
    "        print(final_df.head(3).to_string())\n",
    "    else:\n",
    "        print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ ISJ.csv 생성 완료!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ volume, issue 컬럼이 있는 CSV 파일을 찾을 수 없습니다.\")"
   ],
   "id": "a55a6a79be1ba9fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ISJ.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "✓ 1개의 CSV 파일 발견\n",
      "\n",
      "[1단계] ISJ 파일 읽기\n",
      "  ✓ ISJ.csv                                  -> 180개 행 (volume, issue 컬럼 있음)\n",
      "\n",
      "============================================================\n",
      "✓ 병합 완료: 총 180개 행 (1개 파일)\n",
      "\n",
      "[2단계] date 칼럼 생성 (volume, issue 기반)\n",
      "  - date 생성 완료: 180개\n",
      "\n",
      "[3단계] affiliations 생성\n",
      "  - affiliations: ISJ\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 180개 행\n",
      "  - 제거 후: 148개 행\n",
      "  - 제거됨: 32개 행\n",
      "\n",
      "[5단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 148개 행\n",
      "  - 제거 후: 148개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[6단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[7단계] 날짜별 통계\n",
      "  - 2023-08: 6개\n",
      "  - 2023-10: 7개\n",
      "  - 2023-12: 6개\n",
      "  - 2024-02: 8개\n",
      "  - 2024-04: 8개\n",
      "  - 2024-06: 9개\n",
      "  - 2024-08: 12개\n",
      "  - 2024-10: 11개\n",
      "  - 2024-12: 10개\n",
      "  - 2025-02: 11개\n",
      "  - 2025-04: 11개\n",
      "  - 2025-06: 8개\n",
      "  - 2025-08: 7개\n",
      "  - 2025-10: 9개\n",
      "  - nan-03: 25개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/ISJ.csv\n",
      "✓ 최종 행 수: 148개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: ISJ\n",
      "✓ volume 범위: 33 - 35\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "     date                                                                                                     title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract keywords affiliations\n",
      "0  nan-03                 The Data Product Canvas: Designing Data Products for Sustained Value From Enterprise Data                                                                                                                                                                                                                                                                                                                                                                                                                                             ABSTRACTOrganisations are increasingly striving to become more data‐driven by embedding data into decisions, interactions and processes and by leveraging advanced AI technologies to unlock innovative use‐cases. However, many remain unprepared to meet the rising demands for data, analytics and AI. A data product mindset—combining, packaging and delivering data as a product—has emerged as a promising approach to meet the needs of an expanding user base. Despite their popularity, data products are often seen as a purely technical concept, with suitable methodologies and tools for designing them still underdeveloped. This paper introduces the data product canvas, a visual and versatile tool that helps cross‐functional teams—comprising business, data, analytics and IT experts—collaboratively design new data products and assess existing ones. The canvas ensures that critical themes are addressed: desirability from the customer perspective, feasibility from the technical perspective and viability from the economic perspective. The practical application at SAP illustrates how the data product canvas supports its data democratisation initiative, showcases real‐world examples and offers practical insights to guide future adopters: (a) tailoring designs to different data product types, (b) periodically refining data products to increase their value and (c) systematically assessing requests to build a cohesive data product portfolio.     None          ISJ\n",
      "1  nan-03                                Navigating Flexibility and Standardisation in Low‐Code/No‐Code Development  ABSTRACTLow‐code/no‐code (LCNC) platforms, such as ServiceNow and Microsoft Power Platform, enable employees without formal IT training to build applications and automate workflows, thus driving agility and reducing dependence on traditional IT teams. However, LCNC platforms also pose a persistent challenge for organisations: while they offer flexibility and freedom by enabling decentralised development, they also require standardisation and control to manage risks that can be exacerbated by these platforms, such as shadow IT and technical debt. Striking the right balance is difficult—too much flexibility can compromise stability, while too much standardisation can stifle the autonomy and creativity that make LCNC platforms valuable in the first place. This study explores flexibility–standardisation tensions in LCNC development through an investigation of two multinational technology firms with differing LCNC maturity levels, both using ServiceNow. Drawing from 57 interviews, we identify three types of flexibility‐standardisation tensions shaped by three key elements of LCNC development: the platform itself, the people using the platform and the organisational processes targeted for improvement. We derive six guidelines used to navigate flexibility–standardisation tensions and demonstrate how these are applied across different stages of LCNC maturity. Building on these insights, we provide concrete, context‐sensitive recommendations to help organisations adapt the guidelines to their specific environments. We conclude with forward‐looking reflections on how firms can dynamically make sense of these tensions as LCNC platforms and practices evolve. Overall, our findings show that effective LCNC governance requires a dynamic approach—one that balances flexibility and standardisation simultaneously rather than treating them as opposing choices.     None          ISJ\n",
      "2  nan-03  Heeding the Messenger: The Influence of Sender Characteristics on Security Message Compliance Intentions                                                                                                                                                                                                                                                ABSTRACTHow security messages can be used to motivate information technology (IT) users' security behaviour has been of keen interest to IS research. To that end, studies have focused on the content of security messages; however, few studies have examined the influence of message senders. In this article, we build on social influence theory and integrate it with the concept of inferences of manipulative intent (IMI) to develop a model that examines how perceptions of sender characteristics—cybersecurity expertise, coercive power, and similarity—can yield positive and negative influence on message outcomes, captured in recipients' message compliance intentions. We test our model in four different studies using field and scenario experiments across three target populations: the general public, students, and employees. Perceived expertise, power, and similarity had similar effects among the general public and students: Perceived expertise was positively associated with message outcomes, but perceived power and similarity were negatively associated. In contrast, employees reacted differently from the general public and students in that they responded positively to perceived power, with perceived expertise and similarity having negligible effects. Across these three target populations, we found that participants reacted to senders high in perceived power and similarity with IMI, which reduced their message compliance intentions. Our results suggest that senders must be chosen carefully, depending on the target population, because selecting the wrong sender can increase the likelihood of a message being rejected.     None          ISJ\n",
      "\n",
      "============================================================\n",
      "✓ ISJ.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.1.ISJ 아직 게시 안된 것들 어떻게 해야 할까...\n",
    "- 빼는 쪽으로.."
   ],
   "id": "76f8ced0cc34ba68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. ICIS.csv 만들기\n",
    "- date 컬럼 그대로 두기\n",
    "- affiliations 칼럼 생성 -> ICIS로 변경\n",
    "- null, 중복 제거 -> title, abstract 기준\n",
    "- 2023년 ~ 2025년만 남기기\n",
    "- 최종 데이터 저장 -> ICIS.csv로 필요한 컬럼만 저장 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "6603500432971108"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T04:59:48.420128Z",
     "start_time": "2025-10-29T04:59:48.342388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"ICIS\"\n",
    "INPUT_FILE_PATH = \"/home/dslab/choi/Journal/Data/Academia/ICIS,HICSS/ICIS.csv\"  # 또는 다른 경로\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ICIS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. ICIS 데이터 읽기\n",
    "print(\"[1단계] ICIS 데이터 읽기\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(f\"  ❌ 파일이 없습니다: {INPUT_FILE_PATH}\")\n",
    "    print(f\"  ⚠ 다른 경로에 있다면 코드 상단의 INPUT_FILE_PATH를 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_PATH)\n",
    "    print(f\"  ✓ 파일 읽기 완료: {INPUT_FILE_PATH}\")\n",
    "    print(f\"    - 행 수: {len(df)}\")\n",
    "    print(f\"    - 컬럼: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 확인\n",
    "print(f\"\\n[2단계] date 컬럼 확인\")\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    print(f\"  ✓ date 컬럼이 존재합니다\")\n",
    "    print(f\"  - date는 그대로 유지됩니다\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"  - 샘플 date: {df['date'].iloc[0]}\")\n",
    "else:\n",
    "    print(f\"  ⚠ date 컬럼이 없습니다\")\n",
    "    df['date'] = None\n",
    "\n",
    "# 3. affiliations 칼럼 생성\n",
    "print(f\"\\n[3단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 4. null 값 제거 (title, abstract 기준)\n",
    "print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# 필수 컬럼 존재 여부 확인\n",
    "required_cols = ['title', 'abstract']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 5. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[5단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "VALID_YEARS = [2023, 2024, 2025]\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 6. 중복 제거 (title, abstract 기준)\n",
    "print(f\"\\n[6단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 7. 최종 컬럼 선택\n",
    "print(f\"\\n[7단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 7. 날짜별 통계 (date가 있는 경우)\n",
    "if 'date' in final_df.columns and final_df['date'].notna().any():\n",
    "    print(f\"\\n[8단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"\\n[8단계] 날짜 정보 없음\")\n",
    "\n",
    "# 9. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia/\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: 2023 - 2025\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ ICIS.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "732c5e82cc3b5b03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ICIS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] ICIS 데이터 읽기\n",
      "  ✓ 파일 읽기 완료: /home/dslab/choi/Journal/Data/Academia/ICIS,HICSS/ICIS.csv\n",
      "    - 행 수: 1548\n",
      "    - 컬럼: ['title', 'date', 'abstract', 'keywords', 'authors', 'affiliations']\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 확인\n",
      "  ✓ date 컬럼이 존재합니다\n",
      "  - date는 그대로 유지됩니다\n",
      "  - 샘플 date: 2021\n",
      "\n",
      "[3단계] affiliations 생성\n",
      "  - affiliations: ICIS\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 1548개 행\n",
      "  - 제거 후: 1528개 행\n",
      "  - 제거됨: 20개 행\n",
      "\n",
      "[5단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 1528개 행\n",
      "  - 필터링 후: 836개 행\n",
      "  - 제거됨: 692개 행\n",
      "\n",
      "[6단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 836개 행\n",
      "  - 제거 후: 835개 행\n",
      "  - 제거됨: 1개 행\n",
      "\n",
      "[7단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[8단계] 날짜별 통계\n",
      "  - 2023: 391개\n",
      "  - 2024: 444개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/ICIS.csv\n",
      "✓ 최종 행 수: 835개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: ICIS\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "     date                                                                                                                           title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  abstract                                                                                             keywords affiliations\n",
      "706  2023  A Social Network Approach for Investigating Social Influences on Effective Use: Demonstration in Virtual Reality Collaboration  Merely using new collaboration technologies does not necessarily result in the desired benefits, which is why it is important to understand what constitutes effective use behavior. In information systems research, the affordance network approach has been developed as a methodological approach to investigate effective use behavior. The approach has already been applied to understand the effective use of electronic medical record systems and fitness wearables; however, it neglects how social influences foster or hinder effective use behavior in collaborative settings. Therefore, we supplemented the affordance network approach for collaborative contexts by using social network methods. We demonstrate our approach based on two university courses in which students carried out group work within the collaborative VR application Spatial. Thereby, we contribute a methodological approach that enables researchers to identify influential users who encourage their team members to actualize affordances leading to goal achievement.  Effective Use, Affordance Network Approach, Virtual Reality, Collaboration, Social Network Analysis         ICIS\n",
      "707  2023                                                                                Embracing Absence: Researching What is Not There                                                                                            In the absence of light, we find means to push back the darkness; when data is sparse, we find surrogates measures or extrapolate from existing data. Absences play a role in how we respond to the world. Here, we seek to better understand engagement with absences relationally initiating a productive research pathway for engaging with absences in Information Systems (IS) research. We argue the existence of absences can be considered, their implications can be examined, and their unique importance within different contexts can be demonstrated through careful articulation in reflexive research. Through an exploration of absence in selected literature we outline a variety of ways absences are interpreted and shape the world. Further, using examples of the intersection of IS and “care” we demonstrate the impact of absences in context. Doing so we build a path forward for IS scholars to imaginatively engage with absences in the future.                                             absence, nothing, epistemology, negative phenomena, care         ICIS\n",
      "708  2023                                      Employing Machine Learning to Advance Agent-based Modeling in Information Systems Research                                                  In recent years, computationally intensive theory construction, leveraging big data and machine learning (ML), has gained significant interest in the information systems (IS) community. The integration of computational methods can generate novel methodological paradigms or enhance existing methods. Agent-based modeling (ABM) is one of the computational methods that has recently proliferated in IS research to generate computationally intensive theories. However, ABM is still in nascent state of adoption in IS research and entails some pathological challenges that limit its applicability and robustness. With the goal of advancing ABM in IS research, this article proposes a methodological framework that integrates ML within relevant steps of ABM. The framework is demonstrated in an exemplary IS study, showing its potential for addressing the pathological challenges of ABM. We finally discuss the implications of applying the proposed methodological framework in IS research.                 Agent-based modeling (ABM), machine learning (ML), simulation, computational methods         ICIS\n",
      "\n",
      "============================================================\n",
      "✓ ICIS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 12. HICSS.csv 만들기\n",
    "- date 컬럼 그대로 두기\n",
    "- affiliations 칼럼 생성 -> HICSS로 변경\n",
    "- null, 중복 제거 -> title, abstract 기준\n",
    "- 2023년 ~ 2025년만 남기기\n",
    "- 최종 데이터 저장 -> HICSS.csv로 필요한 컬럼만 저장 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "5cb454498f9957cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:03:42.786301Z",
     "start_time": "2025-10-29T05:03:42.635602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"HICSS\"\n",
    "INPUT_FILE_PATH = \"/home/dslab/choi/Journal/Data/Academia/ICIS,HICSS/HICSS.csv\"  # 또는 다른 경로\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"HICSS.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. HICSS 데이터 읽기\n",
    "print(\"[1단계] HICSS 데이터 읽기\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(f\"  ❌ 파일이 없습니다: {INPUT_FILE_PATH}\")\n",
    "    print(f\"  ⚠ 다른 경로에 있다면 코드 상단의 INPUT_FILE_PATH를 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_PATH)\n",
    "    print(f\"  ✓ 파일 읽기 완료: {INPUT_FILE_PATH}\")\n",
    "    print(f\"    - 행 수: {len(df)}\")\n",
    "    print(f\"    - 컬럼: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 확인\n",
    "print(f\"\\n[2단계] date 컬럼 확인\")\n",
    "\n",
    "if 'date' in df.columns:\n",
    "    print(f\"  ✓ date 컬럼이 존재합니다\")\n",
    "    print(f\"  - date는 그대로 유지됩니다\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"  - 샘플 date: {df['date'].iloc[0]}\")\n",
    "else:\n",
    "    print(f\"  ⚠ date 컬럼이 없습니다\")\n",
    "    df['date'] = None\n",
    "\n",
    "# 3. affiliations 칼럼 생성\n",
    "print(f\"\\n[3단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 4. null 값 제거 (title, abstract 기준)\n",
    "print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# 필수 컬럼 존재 여부 확인\n",
    "required_cols = ['title', 'abstract']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"  ❌ 필수 컬럼이 없습니다: {missing_cols}\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 5. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[5단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 6. 중복 제거 (title, abstract 기준)\n",
    "print(f\"\\n[6단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 7. 최종 컬럼 선택\n",
    "print(f\"\\n[7단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 8. 날짜별 통계 (date가 있는 경우)\n",
    "if 'date' in final_df.columns and final_df['date'].notna().any():\n",
    "    print(f\"\\n[8단계] 날짜별 통계\")\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"\\n[8단계] 날짜 정보 없음\")\n",
    "\n",
    "# 9. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Academia/\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 저널: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ HICSS.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "4caed21ddbfb955d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HICSS.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] HICSS 데이터 읽기\n",
      "  ✓ 파일 읽기 완료: /home/dslab/choi/Journal/Data/Academia/ICIS,HICSS/HICSS.csv\n",
      "    - 행 수: 3506\n",
      "    - 컬럼: ['title', 'date', 'abstract', 'keywords', 'authors']\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 확인\n",
      "  ✓ date 컬럼이 존재합니다\n",
      "  - date는 그대로 유지됩니다\n",
      "  - 샘플 date: 2021-01-05\n",
      "\n",
      "[3단계] affiliations 생성\n",
      "  - affiliations: HICSS\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 3506개 행\n",
      "  - 제거 후: 3506개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[5단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 3506개 행\n",
      "  - 필터링 후: 2098개 행\n",
      "  - 제거됨: 1408개 행\n",
      "\n",
      "[6단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 2098개 행\n",
      "  - 제거 후: 2098개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[7단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[8단계] 날짜별 통계\n",
      "  - 2023-01-03: 664개\n",
      "  - 2024-01-03: 726개\n",
      "  - 2025-01-07: 708개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/HICSS.csv\n",
      "✓ 최종 행 수: 2098개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널: HICSS\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "            date                                                                                         title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract                                                                                           keywords affiliations\n",
      "1408  2023-01-03          Alone Together: Organizational Measures to Address Pitfalls of Virtual Collaboration                                                                                                                                                                                                                                                                                                                      The COVID-19 crisis has made virtual collaboration (VC) an issue across the globe. Employees who once worked in person with their co-workers have had to work from home, relying solely on information and communication technologies to collaborate. This has led to a variety of challenges related to occupational wellbeing (OWB). This study identifies measures organizations have implemented in response. Based on 16 interviews with HR professionals, the findings reveal a number of organizational measures that may help promote OWB in VC.                                                 Virtual Collaboration, Organizations, and Networks        HICSS\n",
      "1409  2023-01-03  “Is This Even Relevant?” Investigating the Relevance of Antecedents to Trust in Ad Hoc Dyads                                                                      Trust is an important variable for effective ad hoc collaboration. As ad hoc collaborations become more prevalent, researchers and stakeholders will need to identify what features facilitate rapid and appropriate swift trust, particularly in contexts comprising salient risk and high stakes. The present work investigated the relevance and impact of antecedents to swift trust in ad hoc dyads. In a within-subjects experiment, we leveraged a vignette and assessed what antecedents were relevant and affected trust in ad hoc dyad formation. The results showed that antecedents varied in terms of their relevance and effect on trust. We discuss how these results align with extant research and implications for future research investigating swift trust in ad hoc collaborations.                            Advances in Trust Research: How Context and Digital Technologies Matter        HICSS\n",
      "1410  2023-01-03     Covid-19 as an Incubator Leading to Telemedicine Usage:  KM Success Factors in Healthcare  Virtual hospitals offer a platform for healthcare workers to share knowledge, treat patients equally everywhere and, thus, reduce patient mortality rates. Such platforms include different technologies, for example telemedical applications. The use of these technologies and the need to get specific knowledge on the patients’ treatment was reinforced in the past years due by Covid-19. Not only the treatment of Covid-19, but also that of other diseases can be improved by increased technology use. By incorporating the KM success model, we will identify KM success factors leading to the use of virtual hospitals. This research observes the KM success model in the context of the low-digitalized field of healthcare. Consequently, we evaluate how the existing KM success model needs to be adjusted according to the peculiarities of healthcare.  Value, Success, and Performance Measurements of Knowledge, Innovation and Entrepreneurial Systems        HICSS\n",
      "\n",
      "============================================================\n",
      "✓ HICSS.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Industry",
   "id": "1f129ed3de8db0f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. TheVerge.csv 만들기\n",
    "- date 컬럼의 시간을 달까지만 남기기\n",
    "- affiliations 칼럼 생성 -> TheVerge.csv로 저장\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 TheVerge.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations"
   ],
   "id": "7bc692eed0cf8347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T07:16:08.004937Z",
     "start_time": "2025-10-22T07:16:07.879715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"TheVerge\"\n",
    "INPUT_FILE_PATH = \"/home/dslab/choi/Journal/Data/Industry/TheVerge/TheVerge.csv\"  # 또는 다른 파일명\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TheVerge.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. TheVerge 데이터 읽기\n",
    "print(\"[1단계] TheVerge 데이터 읽기\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(f\"  ❌ 파일이 없습니다: {INPUT_FILE_PATH}\")\n",
    "    print(f\"  ⚠ 다른 경로에 있다면 코드 상단의 INPUT_FILE_PATH를 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_PATH)\n",
    "    print(f\"  ✓ 파일 읽기 완료: {INPUT_FILE_PATH}\")\n",
    "    print(f\"    - 행 수: {len(df)}\")\n",
    "    print(f\"    - 컬럼: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (시간을 달까지만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원: YYYY-MM-DD, YYYY-MM-DD HH:MM:SS, YYYY/MM/DD 등\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    date_str = str(date_str)\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD, YYYY-MM-DD HH:MM:SS 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. content를 abstract로 매핑\n",
    "print(f\"\\n[3단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "else:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "\n",
    "# 4. affiliations 칼럼 생성\n",
    "print(f\"\\n[4단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin([2023, 2024, 2025])]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: 2023 - 2025\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ TheVerge.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "e271b12698886d88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TheVerge.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] TheVerge 데이터 읽기\n",
      "  ✓ 파일 읽기 완료: /home/dslab/choi/Journal/Data/Industry/TheVerge/TheVerge.csv\n",
      "    - 행 수: 4712\n",
      "    - 컬럼: ['date', 'title', 'content', 'keywords', 'url']\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: 2025-01-20T22:00:00+09:00 → 2025-01\n",
      "  - 변환 완료: 4710개\n",
      "  ⚠ 변환 실패: 2개\n",
      "\n",
      "[3단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[4단계] affiliations 생성\n",
      "  - affiliations: TheVerge\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4712개 행\n",
      "  - 제거 후: 4710개 행\n",
      "  - 제거됨: 2개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 4710개 행\n",
      "  - 필터링 후: 4597개 행\n",
      "  - 제거됨: 113개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4597개 행\n",
      "  - 제거 후: 4584개 행\n",
      "  - 제거됨: 13개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 총 34개의 날짜\n",
      "  - 첫 날짜: 2023-01 (50개)\n",
      "  - 마지막 날짜: 2025-10 (123개)\n",
      "\n",
      "  [최근 10개 날짜]\n",
      "    - 2025-01: 138개\n",
      "    - 2025-02: 124개\n",
      "    - 2025-03: 107개\n",
      "    - 2025-04: 146개\n",
      "    - 2025-05: 159개\n",
      "    - 2025-06: 131개\n",
      "    - 2025-07: 138개\n",
      "    - 2025-08: 117개\n",
      "    - 2025-09: 162개\n",
      "    - 2025-10: 123개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/TheVerge.csv\n",
      "✓ 최종 행 수: 4584개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: TheVerge\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "        date                                               title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  abstract                                                                                                                                                keywords affiliations\n",
      "0    2025-01      Welcome to the era of gangster tech regulation                                                                                                                                                                                                                                                                                                                                                                                                                            Mark Zuckerberg, Elon Musk, and Jeff Bezos are awfully cozy with Donald Trump, suddenly. It’s almost as though tech regulation is up for sale.  AI, Amazon, Business, Cars, Electric Cars, Elon Musk, Meta, OpenAI, Policy, Politics, Science, Space, SpaceX, Tech, Tesla, Transportation, Twitter - X     TheVerge\n",
      "114  2023-01  Microsoft rumored to invest $10 billion in OpenAI.  [Link: Microsoft eyes $10 billion bet on ChatGPT | https://www.semafor.com/article/01/09/2023/microsoft-eyes-10-billion-bet-on-chatgpt | Semafor] Microsoft first invested $1 billion into OpenAI in 2019, and has had a close relationship with the ChatGPT creator ever since. Semafor is now reporting that Microsoft is eyeing a $10 billion bet on OpenAI that would see the software giant get 75 percent of profits and a 49 stake in OpenAI. The rumors come days after reports that Microsoft is planning to integrate OpenAI models into Bing, Word, and more.                                                                                                                                     AI, Microsoft, Tech     TheVerge\n",
      "115  2023-01      Fake “ChatGPT” apps are cashing in on AI hype.                                                       [Media: https://twitter.com/Austen/status/1611864110244990976] Developers trying to cash in on OpenAI&rsquo;s free-to-use chatbot have inundated the iOS App Store and Google Play Store with fake ChatGPT apps, some of which charge a premium for non-existent features. One example called &ldquo;ChatGPT Chat GPT AI With GPT-3&rdquo; (and seen via TechCrunch) has&nbsp;managed to rank highly in multiple charts within the productivity category. OpenAI hasn&rsquo;t released an official app for ChatGPT.                                                                                                                                    AI, Apps, News, Tech     TheVerge\n",
      "\n",
      "============================================================\n",
      "✓ TheVerge.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. TechCrunch.csv 만들기\n",
    "- date 컬럼의 시간을 달까지만 남기기\n",
    "- affiliations 칼럼 생성 -> TechCrunch.csv로 저장\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 TechCrunch.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "d3fecd2f51fb1b5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T07:44:14.240300Z",
     "start_time": "2025-10-22T07:44:14.114065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"TechCrunch\"\n",
    "INPUT_FILE_PATH = \"/home/dslab/choi/Journal/Data/Industry/TechCrunch/TechCrunch.csv\"  # 또는 다른 파일명\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TechCrunch.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. TechCrunch 데이터 읽기\n",
    "print(\"[1단계] TechCrunch 데이터 읽기\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(f\"  ❌ 파일이 없습니다: {INPUT_FILE_PATH}\")\n",
    "    print(f\"  ⚠ 다른 경로에 있다면 코드 상단의 INPUT_FILE_PATH를 수정하세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_PATH)\n",
    "    print(f\"  ✓ 파일 읽기 완료: {INPUT_FILE_PATH}\")\n",
    "    print(f\"    - 행 수: {len(df)}\")\n",
    "    print(f\"    - 컬럼: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (시간을 달까지만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원: YYYY-MM-DD, YYYY-MM-DD HH:MM:SS, YYYY/MM/DD 등\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    date_str = str(date_str)\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD, YYYY-MM-DD HH:MM:SS 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. content를 abstract로 매핑\n",
    "print(f\"\\n[3단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "else:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "\n",
    "# 4. affiliations 칼럼 생성\n",
    "print(f\"\\n[4단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin([2023, 2024, 2025])]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: 2023 - 2025\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ TechCrunch.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "bc0979e62b89bf7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TechCrunch.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] TechCrunch 데이터 읽기\n",
      "  ✓ 파일 읽기 완료: /home/dslab/choi/Journal/Data/Industry/TechCrunch/TechCrunch.csv\n",
      "    - 행 수: 727\n",
      "    - 컬럼: ['title', 'date', 'content', 'keywords', 'url']\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: 2025-10-20T01:15:50-07:00 → 2025-10\n",
      "  - 변환 완료: 717개\n",
      "  ⚠ 변환 실패: 10개\n",
      "\n",
      "[3단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[4단계] affiliations 생성\n",
      "  - affiliations: TechCrunch\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 727개 행\n",
      "  - 제거 후: 727개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 727개 행\n",
      "  - 필터링 후: 717개 행\n",
      "  - 제거됨: 10개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 717개 행\n",
      "  - 제거 후: 715개 행\n",
      "  - 제거됨: 2개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 2025-04: 119개\n",
      "  - 2025-05: 134개\n",
      "  - 2025-06: 115개\n",
      "  - 2025-07: 109개\n",
      "  - 2025-08: 70개\n",
      "  - 2025-09: 95개\n",
      "  - 2025-10: 73개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/TechCrunch.csv\n",
      "✓ 최종 행 수: 715개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: TechCrunch\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                          title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract                                                        keywords affiliations\n",
      "0  2025-10            Scale AI alum raises $9M for AI serving critical industries in MENA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Bilal Abu-Ghazalehhad just moved to London few days before our call, splitting his time between there and Dubai.\\nAfter nearly a decade in the U.S., including a stint at Scale AI, he’s bringing that experience to his next venture:1001 AI, a company creating AI infrastructure for critical industries across the Middle East and North Africa (MENA).\\nThe startup recently raised a $9 million seed round led by CIV, General Catalyst, and Lux Capital. Other backers include global and regional angels such as Chris Ré, Amjad Masad (Replit), Amira Sajwani (DAMAC), Khalid Bin Bader Al Saud (RAED Ventures), and Hisham Alfalih (Lean Technologies).\\nAbu-Ghazaleh said his two-month-old company promises to cut inefficiencies in high-stakes sectors like aviation, logistics, and oil and gas through an AI-native operating system for decision-making.\\n“Just looking at the top three or four industries like airports, ports, construction, and oil and gas, we see more than $10 billion in inefficiencies across the Gulf alone,” the founder and CEO said in an interview with TechCrunch. “That’s just in markets like the UAE, Saudi Arabia, and Qatar. Even without counting other sectors, these industries represent a massive opportunity.”\\nFor example, any efficiencies found in airport operations can compound the savings, impacting both the airport and its airlines. Meanwhile, he said nine out of ten of the regions mega-projects fall behind schedule or go over budget, meaning even small increases in efficiencies can save these projects serious money.\\n1001 AI hopes to sell its decision-making AI to new projects after it launches its first product, which is scheduled by year’s end. The startup is in talks with some of the Gulf’s largest construction firms and airports, said Abu-Ghazaleh.\\nBorn and raised in Jordan, Abu-Ghazaleh moved to the U.S. for college and later joined the Bay Area’s startup scene. After an early product role at computer vision startup Hive AI, he joined Scale AI in 2020 during its rapid expansion. There, he rose through the ranks from operations associate to director of the company’s GenAI operations, scaling its contributor network responsible for annotating and labeling training data.\\nHe was later set to join Scale’s international public sector unit, which builds AI solutions for foreign governments. But whenMeta invested in Scale, the company shifted direction, and Abu-Ghazaleh left to found 1001 AI.\\nThe Gulf, particularly the UAE and Saudi Arabia, has become one of the world’s most aggressive adopters of AI. From sovereign-backed ventures like G42 in Abu Dhabi to Saudi Arabia’s National Center for AI, governments are investing billions to build local AI infrastructure and attract global talent.\\nFor Abu-Ghazaleh, that mix of appetite, budget, and urgency makes the region a perfect testing ground. But unlike most AI startups focused on software or enterprise tools, 1001 targets real-world physical operations, an area where the company’s investors believe the potential is even greater in the Middle East.\\n“We’re extremely bullish on AI that solves physical-world problems at scale i.e, optimizing how airports turn around flights, how ports move cargo, how construction sites operate,” said Deena Shakir, partner at Lux Capital. “The MENA region offers significant potential in this space with mission-critical infrastructure that’s under-digitized and ripe for transformation.”\\nWhile the product is still under development, Abu-Ghazaleh offered a glimpse into how it works. The system pulls in data from a client’s existing software, models operational workflows, and issues real-time directives to improve efficiency.\\n“Today, an operations manager might manually call someone to reroute a fuel truck or send a cleaning crew to another gate,” said Abu-Ghazaleh. “With our system, that orchestration happens automatically. The AI orchestrator uses real-time data to reroute vehicles, reassign crews, and adjust operations without human intervention.”\\nUnlike most early-stage AI startups that target specific industries, Abu-Ghazaleh says 1001 can be accessible by many because operational flows across industries often look the same.\\nThat model borrows from the rigor of consulting and contract work. The team spends weeks embedded with clients, running co-development sprints to tailor its systems to each operation’s realities, the CEO said.\\n“Bilal is building the decision engine to automate that complexity with Scale-proven execution and the regional gravity to make 1001 the platform this market builds on,” commented Neeraj Arora, managing director at General Catalyst.\\nThe new funding will accelerate early deployments across aviation, logistics, and infrastructure, while fueling recruitment in engineering, operations, and go-to-market role as it grows its team across Dubai and London.\\n1001 AI plans to launch its first customer deployment by the end of the year, starting with construction. Over the next five years, Abu-Ghazaleh wants the company to become the Gulf’s go-to orchestration layer for these industries before expanding globally.                                   1001,1001.ai,AI,MENA,Scale AI   TechCrunch\n",
      "1  2025-10                            The man betting everything on AI and Bill Belichick  Lee Roberts meets me at the University Club of San Francisco on a Friday morning, hours before his football team will lose to Cal in heartbreaking fashion – a fumble at the goal line, because little about the University of North Carolina at Chapel Hill’s expensive experiment with Bill Belichick has gone according to script.\\nBut Roberts, the Chancellor of UNC, doesn’t know this yet. Right now, he’s in California to talk about artificial intelligence, which is both forward thinking and also – I’d hazard a guess – a welcome distraction from a lot else happening at the 235-year-old school.\\n“No one’s going to say to [students after they graduate from college], ‘Do the best job you can, but if you use AI, you’ll be in trouble,’” Roberts tells me, leaning into his central thesis about preparing students for the real world. “Yet we have some faculty members who are effectively saying that to students right now.”\\nRoberts has joined me in between other meetings in the city with AI companies because UNC has decided to make AI its north star. It’s a business bet, really. Roberts spent three decades in finance, most recently as managing partner of a private investment firm, and served as state budget director under a Republican governor. He taught budgeting as an adjunct at Duke but never worked in academic administration before becoming UNC’s interim chancellor in January of last year, a post made permanent eight months later.\\nNever mind that the university just lost 118 federal grants totaling $38 million as part of a sweeping effort by the federal government to terminate more than4,000 grants across 600 institutions. Never mind that more than 900 people last year signed a statement saying they wouldn’t recognize Roberts as chancellor when he was appointed, calling the process a political “coronation” rather than a search. Never mind that Belichick’s much-touted return to football is currently a 2-4 trainwreck, with write-ups about the team’s dysfunction becoming routine fodder for sports writers. Roberts is focused on the future.\\nAt UNC, Roberts explains, there’s a spectrum between faculty who are “leaning forward” with AI and those who have “their heads in the sand.” It’s diplomatic phrasing for what is clearly a culture war playing out in faculty lounges across UNC and – it’s probably safe to assume – at other schools across the world. While one UNC professor is assigning more research than students could complete without AI (“much closer to a real world scenario,” says Roberts), others are treating chatbots like anabolic steroids. If you use them, you’re cheating.\\n“We have 4,000 faculty members,” Roberts says, as a cable car clatters past the open window beside our table. “And they pride themselves, as they should, on their independence and autonomy in how they teach their classes.”\\nIt sounds a little like code for: tenured professors can’t be forced to do anything. So Roberts is creating “incentive-based programs” to move the ball forward, like promoting one of the school’s deans into the role of Vice Provost for AI at the university. That individual, Jeffrey Bardzell, has been a professor for more than 20 years and has “experience both in technology and as a humanist,” says Roberts, adding that Bardzell is “exceptionally well-placed to help the faculty as a whole come further up to speed.”\\nUNC is barreling ahead on other fronts in the meantime. In its biggest development to date, the universityannounced this monththat it is merging two schools – the School of Data Science and Society and the School of Information and Library Science – into one yet-to-be-named entity with AI studies at the center of the Venn diagram.\\nUNC isn’t alone in betting big on AI. At least14 collegesnow offer bachelor’s degrees in AI, and universities like Arizona State University have made headlines for integrating AI tools across all disciplines.\\nStill, creating this new school has worried some of the school’s library science students, who wonder what will happen to their degrees, according to areportin The Daily Tar Heel, the school’s independent student newspaper. At least one faculty member also complained anonymously in a statement to the paper, saying Roberts pushed for the school without a “cogent idea” of what it will entail, adding that the “careers of faculty, staff and students at both of these schools are being sacrificed to Roberts’ ego.”\\nRoberts tells me the implementation will be collaborative, not top-down. He’s also clear that the move is proactive, not reactive. “This is not about shutting anything down,” he says. “It’s not predominantly a cost-savings move,” he continues, a possible nod to those lost federal research dollars, which amount to 3.5% of UNC’s overall research funding.\\nRoberts doesn’t minimize the devastation of losing grants – “in many cases, [people] lose their life’s work,” he acknowledges – but he’s also quick to note that 3.5% is “well within our average annual variance.” He adds that he has been spending “a lot of time talking with policymakers and legislators in Washington about the tremendous good that federal research funding represents. We need to be especially vigilant right now, when there’s so much uncertainty around [these dollars] that it’s really changing the basic structure of how large research universities have been funded.”\\nOf course, it raises questions about resources in the aggregate. Though UNC’s AI push is the topic du jour, I ask about the $10 million the school is paying Bill Belichick annually as part of afive-year dealsigned back in January. I’m from Cleveland, I tell Roberts. I remember when Belichick cut Browns quarterback Bernie Kosar, a hometown hero. The city never forgave him.\\nRoberts is ready for this. College sports are changing rapidly, he says. Every peer institution spends at least as much on football; many spend more. Football drives revenue for 28 other sports. UNC just won its fourth national championship in women’s lacrosse, its 23rd in women’s soccer. None of that happens without football money.\\n“If we had hired somebody else and we were [down some games], everybody would be saying, ‘Hey, man, you could have had Bill Belichick,’” Roberts offers.\\nIn reality, the prevailing narrative about Belichick isn’t just about wins and losses. Even if, ultimately, that’s exactly what it’s about, numerousoutletshave published storiesdescribing chaosinside the program, with players, parents, coaches, and administrators all painting a picture of a legendary NFL coach whose style doesn’t translate to college kids.\\nBut Roberts isn’t making decisions based on “a couple of news stories,” he says. “Coach Belichick, in my view, has done a really good job integrating with our campus,” Roberts says. He shows up at other teams’ games. He sends pizzas to fraternities on Saturday nights. He grew up on a college campus – his father was the coach at Navy.”\\nHours after our conversation, UNC will lose to Cal when wide receiver Nathan Leacock losescontrol of the balljust as he’s crossing into the end zone for what would have been the game-winning touchdown. I can only imagine what the immediate reaction is like back in Chapel Hill.\\nMy sense is that Roberts will brush it off. He may never be forgiven for not having a traditional academic background, but he also can’t afford to care that this bothers some people. I note that the 900-person petition took issue with the fact that, among the top 50 universities, Roberts is the only leader without higher education administration experience. Thepetitionran in The Daily Tar Heel, which has been critical of Roberts’ chancellorship throughout.\\n“I don’t think it was 900 students,” Roberts corrects me. “I think it was 900 people, regardless of whether they were students, faculty, staff, or just people in the world who signed an online petition.”\\nI ask how he felt about the whole episode. “No matter what your background was before you came into a job like this, you would have a lot to learn,” Roberts says. If you were a provost, you’d know nothing about “the business or finance or budgetary or political or operational or real estate sides of the university.” If you came from business, you’d need to learn the academic side.\\nIt’s a reasonable point. The modern university chancellor is part CEO, part diplomat, part fundraiser, part sports executive. Presumably, no one arrives with all the skills required. “I think almost no matter what you did previously before coming into a job like this, there would be a learning curve,” Roberts says.\\nWhat strikes me about Roberts is that he seems relatively unbothered. The federal funding cuts are within the normal range. The Belichick hire is a wait-and-see situation. As for some of the faculty’s resistance to AI, it’s a puzzle to be solved.\\nHe’s also making big bets just as higher education is being squeezed every which way. Federal funding is uncertain. Birth rate declines threaten future enrollment. The value of a college degree is in question, with more students graduating to find the only jobs available to them arelow-wage gigsthey could have landed without spending staggering amounts on college. Now AI threatens to upend the whole model.\\nBut Roberts sees opportunity where others might see a crisis. He also thinks the window of opportunity is shorter than some might imagine. “The challenge of AI is that we have to work relatively quickly, and we also have to cooperate across academic disciplines,” he says. “And those are two things that universities, historically, are not especially good at.”\\nWhether Roberts’ game plan works remains to be seen. What’s clear is that he’s betting moving fast and shaking things up is better than moving slowly and preserving tradition athighly rankedUNC.\\n“We’re going to try to make Carolina the number one public university in America,” he tells me.\\nIt’s an ambitious vision, and as he delivers it, for better or worse, he sounds very much like a Silicon Valley CEO.\\nTo hear this interview with Roberts, listen to TechCrunch’sStrictlyVC Download podcast; new episodes drop every Tuesday.\\n     AI,AI,Bill Belichick,Government & Policy,Lee Roberts,TC,UNC   TechCrunch\n",
      "2  2025-10  Wikipedia says traffic is falling due to AI search summaries and social video                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Wikipedia is often described asthe last good websiteon an internet increasingly filled with toxic social media and AI slop. But it seems the online encyclopedia is not completely immune to broader trends, with human pageviews falling 8% year-over-year, according toa new blog postfrom Marshall Miller of the Wikimedia Foundation.\\nThe foundation works to distinguish between traffic from humans and bots, and Miller writes that the decline “over the past few months” was revealed after an update to Wikipedia’s bot detection systems appeared to show that “much of the unusually high traffic for the period of May and June was coming from bots that were built to evade detection.”\\nWhy is traffic falling? Miller points to “the impact of generative AI and social media on how people seek information,” particularly as “search engines are increasingly using generative AI to provide answers directly to searchers rather than linking to sites like ours” and as “younger generations are seeking information on social video platforms rather than the open web.” (Google hasdisputed the claimthat AI summaries reduce traffic from search.)\\nMiller says the foundation welcomes “new ways for people to gain knowledge” and argues this doesn’t make Wikipedia any less important, since knowledge sourced from the encyclopedia is still reaching people even if they don’t visit the website. Wikipedia even experimented with AI summaries of its own, though itpaused the effort after editors complained.\\nBut this shift does present risks, particularly if people are becoming less aware of where their information actually comes from. As Miller puts it, “With fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.” (Some of those volunteers are truly remarkable,reportedly disarming a gunman at a Wikipedia editors’ conferenceon Friday.)\\nFor that reason, he argues that AI, search, and social companies using content from Wikipedia “must encourage more visitors” to the website itself.\\nAnd he says Wikipedia is taking steps of its own, for example by developing a new framework for attributing content from the encyclopedia. The organization also has two teams tasked with helping Wikipedia reach new readers, and it’s looking for volunteers to help.\\nMiller also encourages readers to “support content integrity and content creation” more broadly.\\n“When you search for information online, look for citations and click through to the original source material,” he writes. “Talk with the people you know about the importance of trusted, human curated knowledge, and help them understand that the content underlying generative AI was created by real people who deserve their support.”  AI,Media & Entertainment,Social,Wikimedia Foundation,Wikipedia   TechCrunch\n",
      "\n",
      "============================================================\n",
      "✓ TechCrunch.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.TheGuardian.csv 만들기\n",
    "- 폴더에 있는 TheGuardian_*.csv를 다 병합\n",
    "- date 컬럼의 시간을 달까지만 남기기\n",
    "- affiliations 칼럼 생성 -> TheGuardian.csv로 저장\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 TechCrunch.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "a9c5c24f170195d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T10:14:58.922219Z",
     "start_time": "2025-10-22T10:14:58.373075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"TheGuardian\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Industry/TheGuardian\"  # TheGuardian 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"TheGuardian_*.csv\"  # TheGuardian_로 시작하는 모든 csv 파일\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TheGuardian.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. TheGuardian 데이터 읽기 및 병합\n",
    "print(\"[1단계] TheGuardian 파일들 읽기 및 병합\")\n",
    "\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"  ❌ '{FILE_PATTERN}' 패턴의 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"  경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 TheGuardian 파일 발견\\n\")\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        all_dfs.append(temp_df)\n",
    "        print(f\"  - {filename:40s} -> {len(temp_df)}개 행\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"  ❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (시간을 달까지만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원:\n",
    "    - Sat 4 Nov 2023 16.00 GMT\n",
    "    - Mon 20 Oct 2025 18.40 BST\n",
    "    - 1 August 2024\n",
    "    - YYYY-MM-DD, YYYY-MM-DD HH:MM:SS, YYYY/MM/DD 등\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    date_str = str(date_str)\n",
    "\n",
    "    # 월 이름 매핑\n",
    "    month_names = {\n",
    "        'January': '01', 'Jan': '01',\n",
    "        'February': '02', 'Feb': '02',\n",
    "        'March': '03', 'Mar': '03',\n",
    "        'April': '04', 'Apr': '04',\n",
    "        'May': '05',\n",
    "        'June': '06', 'Jun': '06',\n",
    "        'July': '07', 'Jul': '07',\n",
    "        'August': '08', 'Aug': '08',\n",
    "        'September': '09', 'Sep': '09',\n",
    "        'October': '10', 'Oct': '10',\n",
    "        'November': '11', 'Nov': '11',\n",
    "        'December': '12', 'Dec': '12'\n",
    "    }\n",
    "\n",
    "    # \"Day DD Mon YYYY\" 또는 \"DD Month YYYY\" 패턴 처리\n",
    "    for month_name, month_num in month_names.items():\n",
    "        if month_name in date_str:\n",
    "            # 연도 추출 (4자리 숫자)\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', date_str)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                return f\"{year}-{month_num}\"\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD, YYYY-MM-DD HH:MM:SS 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. content를 abstract로 매핑\n",
    "print(f\"\\n[3단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "else:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "\n",
    "# 4. affiliations 칼럼 생성\n",
    "print(f\"\\n[4단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ TheGuardian.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "6fecb1292c1de8cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TheGuardian.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] TheGuardian 파일들 읽기 및 병합\n",
      "✓ 27개의 TheGuardian 파일 발견\n",
      "\n",
      "  - TheGuardian_10.csv                       -> 197개 행\n",
      "  - TheGuardian_100.csv                      -> 91개 행\n",
      "  - TheGuardian_105.csv                      -> 189개 행\n",
      "  - TheGuardian_110.csv                      -> 19개 행\n",
      "  - TheGuardian_115.csv                      -> 117개 행\n",
      "  - TheGuardian_120.csv                      -> 215개 행\n",
      "  - TheGuardian_125.csv                      -> 312개 행\n",
      "  - TheGuardian_130.csv                      -> 98개 행\n",
      "  - TheGuardian_131.csv                      -> 19개 행\n",
      "  - TheGuardian_15.csv                       -> 295개 행\n",
      "  - TheGuardian_20.csv                       -> 395개 행\n",
      "  - TheGuardian_25.csv                       -> 494개 행\n",
      "  - TheGuardian_30.csv                       -> 592개 행\n",
      "  - TheGuardian_35.csv                       -> 691개 행\n",
      "  - TheGuardian_40.csv                       -> 95개 행\n",
      "  - TheGuardian_45.csv                       -> 19개 행\n",
      "  - TheGuardian_5.csv                        -> 99개 행\n",
      "  - TheGuardian_50.csv                       -> 116개 행\n",
      "  - TheGuardian_55.csv                       -> 59개 행\n",
      "  - TheGuardian_60.csv                       -> 20개 행\n",
      "  - TheGuardian_65.csv                       -> 116개 행\n",
      "  - TheGuardian_70.csv                       -> 20개 행\n",
      "  - TheGuardian_75.csv                       -> 119개 행\n",
      "  - TheGuardian_80.csv                       -> 60개 행\n",
      "  - TheGuardian_85.csv                       -> 158개 행\n",
      "  - TheGuardian_90.csv                       -> 79개 행\n",
      "  - TheGuardian_95.csv                       -> 175개 행\n",
      "\n",
      "✓ 병합 완료: 총 4859개 행 (27개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: Mon 20 Oct 2025 18.40 BST → 2025-10\n",
      "  - 변환 완료: 4504개\n",
      "  ⚠ 변환 실패: 355개\n",
      "\n",
      "[3단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[4단계] affiliations 생성\n",
      "  - affiliations: TheGuardian\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4859개 행\n",
      "  - 제거 후: 4859개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 4859개 행\n",
      "  - 필터링 후: 4504개 행\n",
      "  - 제거됨: 355개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4504개 행\n",
      "  - 제거 후: 1986개 행\n",
      "  - 제거됨: 2518개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 총 34개의 날짜\n",
      "  - 첫 날짜: 2023-01 (35개)\n",
      "  - 마지막 날짜: 2025-10 (93개)\n",
      "\n",
      "  [최근 10개 날짜]\n",
      "    - 2025-01: 67개\n",
      "    - 2025-02: 50개\n",
      "    - 2025-03: 67개\n",
      "    - 2025-04: 62개\n",
      "    - 2025-05: 77개\n",
      "    - 2025-06: 97개\n",
      "    - 2025-07: 105개\n",
      "    - 2025-08: 98개\n",
      "    - 2025-09: 98개\n",
      "    - 2025-10: 93개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/TheGuardian.csv\n",
      "✓ 최종 행 수: 1986개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: TheGuardian\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                         title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            abstract keywords affiliations\n",
      "0  2025-10               Trump reposts AI-generated video of plane dumping sludge on No Kings protesters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Donald Trumpreposted an AI-generated video of him flying a fighter plane emblazoned with the words “King Trump” and dumping brown sludge on to protesters, in what appears to be a retort to the widespreadNo Kings proteststhat took place on Saturday against his second presidency.\\nIn the video, which the president posted on Saturday night, a sharply orange Trump is seen donning a gold crown and manning a plane monikered “King Trump”. The video zooms away from Trump and shows the plane dumping bursts of brown matter on an AI-generated cityscape. A protester, taking a selfie video, captures the crowd being covered in the brown liquid. The last shot of the 19-second video is of protesters in what appears to be Times Square getting dumped on.\\nThe video includes a snippet from Kenny Loggins’s song Danger Zone, notably used in the Tom Cruise classic Top Gun. On Monday, Loggins wrote in astatementthat he didn’t authorize the use of his song for Trump’s video.\\n“Nobody asked for my permission, which I would have denied, and I request that my recording on the video is removed immediately,” he said. “I can’t imagine why anybody would want their music used or associated with something created with the sole purpose of dividing us.”\\nIt is not the first AI-generated video Trump has posted on his timeline, which is full of edited memes and videos professing praise for the president and his allies while mocking his political enemies.\\nEarlier in October, Trump posted adeepfake video– or an AI-generated clip that is made to look real – of Chuck Schumer, the US Senate minority leader, calling his fellow Democrats “woke pieces of shit”. The video also had a racist depiction of Hakeem Jeffries, the Democratic US House leader, dressed in a fake moustache and a sombrero set to mariachi music.\\nWhen asked to comment on the video, JD Vance defended it, saying that he thought “it’s funny”.\\n“You can negotiate in good faith while also poking a little bit of fun at some of the absurdities of the Democrats’ positions,” the vice-president said. “I’ll tell Hakeem Jeffries right now – I make this solemn promise to you that if you help us reopen the government, the sombrero memes will stop.”\\nTrump was alsocriticizedfor using deepfake videos and AI-generated images shoring up support for his winning the 2024 presidential campaign.\\nThe Republican party overall seems to have gotten comfortable using AI-generated images and videos. On Friday, the social media account for Senate Republicanspostedan attack ad against Schumer that used a deepfake depiction of the senator saying “every day gets better for us” amid the federal government shutdown. The video had a small disclaimer in the corner of the video labeling it as AI-generated.\\nNo Kings protestswere seen in all 50 states on Saturday, with millions coming out against the Trump administration – withsignslike “sorry for being weird, this is my first dictatorship” and “No Kings Since 1776”, referring to the year that the US declared its independence from the UK.\\nSign up toThis Week in Trumpland\\nA deep dive into the policies, controversies and oddities surrounding the Trump administration\\nafter newsletter promotion\\nIn an interview before the protests, Trump said that he didn’t agree with the “king” label.\\n“They say they’re referring to me as a king. I’m not a king,” Trump said.\\nOn Sunday, Trump took a harsher tone incommentsto reporters. “I’m not a king. I work my ass off to make our country great. That’s all it is. I’m not a king at all,” Trump said, adding that he thought the protests were “a joke”.\\n“The demonstrations were very small, very ineffective, and the people were whacked out,” he continued. “When you look at those people, they are not representative of the people of our country.”     None  TheGuardian\n",
      "1  2025-10            When my kids wrote a song using AI, all I could think was: you missed the fun part  Somewhere in the middle of the last school holidays, as I was attempting to work from home, the kids came bounding down the stairs armed with a new song they had written. The lyrics were nonsensical (as you’d expect from a pair of preteens), but there was a surprising crispness to the rhyming structure.\\n“We got ChatGPT to write it,” the eldest said. This was neither a confession nor a boast. Every 12-year-old knows the AI shortcut. Two minutes earlier, they didn’t have a song. Now they had something ready to perform. Admittedly the improvised melody could best be described as “indeterminate”, but the right prompt could have fixed that.\\nAll I could say was: “You’ve missed the fun part!”\\nThey could have spent an entertaining hour with a pen and paper, bouncing ideas and daft couplets around and, most crucially, left their working parent undisturbed. Instead, they skipped straight from inspiration to opening night.\\nThis scene seems anything but uncommon. The author Danielle Binkswrote recentlyabout the pall that generative AI has cast over this year’s Book Week, when a visiting author now seems less aspirational and more anachronism. Kids are learning young that AI won’t just do the tedious things, but all the creative things too. Who cares about authors when books can just write themselves?\\nIt’s understandable that kids would privilege the end product over the process – our girls’ performance was probably more important to them than the song itself – since society is built around achieving goals and outcomes, ideally by the quickest, least painful means. Such is productivity.\\nBut for this generation of young humans, there is a building sense that maximal productivity might mean leaving out the human element altogether. All the skills they are learning at school – maths, writing, even coding – risk being totally redundant by the time they graduate. Why bother learning to do something a machine will do better and faster? Why train for jobs that will no longer exist?\\nFor parents of young children, the arrival of AI (and the associated human redundancy) means helping kids negotiate an existential crisis that feels several generations advanced from our own. The career paths that are disintegrating beneath our feet likely won’t exist at all by the time our children are ready to follow them.\\nOur eldest is a keen writer, but she is aware that humans might not be paid to write books by the time she’s ready to be published. And why should our youngest spend hours and years honing her sketching skills when ChatGPT can immediately draw a fox riding a dragon? What’s the point in being creative at all any more?\\nThe answer, of course, is because the process is the point. When we stick those crayon drawings on the fridge it isn’t because each is a fantastic work of art, but rather a remnant of a great burst of creativity on the part of our offspring. It’s the process we’re celebrating. A key element of our digital age, with its focus on insta-publishing from blog posts to TikToks, has been the devaluing of craft in favour of immediacy. Why do something well when you could do it fast and still get all the clicks?\\nThe arrival of AI is a chance to remind kids that sometimes the end result isn’t the one on the page in front of you. The goal is not to have created, but to be a creator. To be someone who enjoys making bad art for long enough that some of it might one day please others as much as it pleases you. And even if that never happens, maybe the point is the happy – if often frustrating – hours you’ve spent making something pointless.\\nIf you want to be joyless about it, you could argue that AI making hard work redundant means we need to help kids value hard work for its own sake. Doing things the hard way is usually the most rewarding way, and as much as we’re drawn to them, we don’t value the easy things.\\nThe appeal of endurance sports, I can only imagine, isn’t simply getting to a destination that it would have been quicker to travel to in a machine. It’s about becoming the sort of person who can run an ultramarathon despite the physical toll and hardship. Our efforts may not always change the world, but they often change ourselves.\\nThis is the key to creativity. Creating their own art shows kids the world as they understand it and, in doing so, teaches them about themselves. Creating is about experimentation, about finding new methods and mediums that feel true and meaningful. Creating is also about failure and slowly closing the gap between intention and ability. There are no lessons to be learned from outsourcing that process.\\nThe pop stars of the future may be synthetic. Our galleries may fill with AI art. But maybe, as parents, we should seize this disconnection between creating and having created to shift the focus away from the end product. We can encourage a generation of endurance artists.\\nIf a computer can make anything, maybe it’s time to stop valuing the thing itself. This means shifting the tense to focus on the moment – on the active, not the passive. Because it turns out the most important part of creativity is not the having done, but the doing. That’s the human part.\\nMyke Bartlett is a writer and critic     None  TheGuardian\n",
      "2  2025-10  ‘Every kind of creative discipline is in danger’: Lincoln Lawyer author on the dangers of AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               He is one of the most prolific writers in publishing, averaging more than a novel a year. But even Michael Connelly, the author of the bestselling Lincoln Lawyer series, feared he might fall behind when writing about AI.\\nConnelly’s eighth novel in the series, to be released on Tuesday, centres on a lawsuit against an AI company whose chatbot told a 16-year-old boy that it was OK for him to kill his ex-girlfriend for being unfaithful.\\nBut as he was writing, he witnessed the technology altering the way the world worked so rapidly that he feared his plot might become out of date.\\n“You don’t have to lick your finger and hold it up to the wind to know that AI is a massive change that’s coming to science, culture, medicine, everything,” he said. “It’s going to affect all parts of our lives.\\n“But it’s kind of the wild west; there’s no government oversight. AI is moving so fast that I even thought my book might be archaic by the time it got published.”\\nThe Lincoln Lawyer novels are a series of Los Angeles-based thrillers in which the defence attorney Mickey Haller works out of his Lincoln car. They have been adapted into a 2011 film starring Matthew McConaughey, as well as a Netflix series.\\nNot for the first time in the series, The Proving Ground took some inspiration from real-world events.\\nConnelly said: “One was this case in Orlando, wherea teenager committed suicide, allegedly at the urging of a chatbot. Before that there was a case in England, where a person with some mental health issueswas encouraged [by a chatbot] to jump the wall at Windsor Palacewith a bow and arrow to try to find the Queen.”\\nOn the themes of the novel, he added: “Is free speech a human right or mechanical right? In the Orlando case, the judge said he wouldn’t grant a machine human rights. But it’s an interesting question. Is AI going to reach a point that it shares the rights that human beings have?”\\nConnelly, 69, is one of the world’s leading crime writers, his books having topped bestseller charts and sold more than 89m copies. He is also known for the Harry Bosch series, which has been made into a TV show by Amazon. (In Connelly’s fictional universe, Haller and Bosch are half-brothers.)\\nThe writer has his own battles with AI. He is part of a collective of authors, including Jonathan Franzen, Jodi Picoult and John Grisham,suing OpenAI for copyright infringement.\\n“The Author’s Guild came to me and said: ‘Do you know that all your books were fed into the giant maw of OpenAI’s training of its chatbot?’” Connelly said. “I didn’t. If we let that go by, it will put every publisher out of business. Authors will have no protections on their creative work. The purpose of the lawsuit is to have proper rules put in place for all levels of use.”\\nHe cited chess championGarry Kasparov’s loss to IBM’s Deep Blue in 1997as “one of the benchmarks that led us” to this moment. When asked if authors could go the way of grandmasters, he said: “It could happen, but I don’t think it’d be an improved world.”\\nSign up toBookmarks\\nDiscover new books and learn more about your favourite authors with our expert reviews, interviews and news stories. Literary delights delivered direct to you\\nafter newsletter promotion\\nHe added: “Every kind of creative discipline is in danger. Even actors. There’s now these amazing deepfakes. I live out here in LA, and that’s a big concern in the entertainment industry.\\n“I always come back to the word soulless,” Connelly said. “You know it when you see it, there’s something missing.”\\nThere has beencontroversy after an AI talent studio unveiled its new “AI actor” Tilly Norwoodlast month, with unions and actors condemning the move.\\nConnelly has pledged $1m (£746m) to combat the wave of book banssweeping through his home state of Florida. He said he felt moved to do something after he learned that Harper Lee’s To Kill A Mockingbird, which had been influential to him, was temporarily removed from classrooms in Palm Beach County.\\n“I had to read that book to be what I am today. I would have never written a Lincoln Lawyer without it,” he said. He was also struck when Stephen Chbosky’s coming of age novel The Perks of Being a Wallflower, “which meant a lot to my daughter”, received a ban.\\nHe and his wife, Linda McCaleb, help fund PEN America’sMiami office countering book bans. “It’s run by a lawyer who then tries to step in, usually by filing injunctions against school boards,” he said. “I don’t believe anyone has any right to tell some other kid they can’t read something, to usurp another parent’s oversight of their children.”     None  TheGuardian\n",
      "\n",
      "============================================================\n",
      "✓ TheGuardian.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. BBC.csv 만들기\n",
    "- BBC_*.csv를 다 병합\n",
    "- date 컬럼의 시간을 달까지만 남기기\n",
    "- affiliations 칼럼 생성 -> BBC.csv로 저장\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 BBC.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "2906aadc4db0cbc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T10:11:44.014413Z",
     "start_time": "2025-10-22T10:11:43.918258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"BBC\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Industry/BBC\"  # BBC 파일들이 있는 디렉토리\n",
    "FILE_PATTERN = \"BBC_*.csv\"  # BBC_로 시작하는 모든 csv 파일\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"BBC.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. BBC 데이터 읽기 및 병합\n",
    "print(\"[1단계] BBC 파일들 읽기 및 병합\")\n",
    "\n",
    "csv_files = glob(os.path.join(DATA_DIR, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"  ❌ '{FILE_PATTERN}' 패턴의 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"  경로 확인: {DATA_DIR}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"✓ {len(csv_files)}개의 BBC 파일 발견\\n\")\n",
    "\n",
    "# 모든 데이터프레임을 저장할 리스트\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        all_dfs.append(temp_df)\n",
    "        print(f\"  - {filename:40s} -> {len(temp_df)}개 행\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 오류 ({filename}): {e}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"  ❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (시간을 달까지만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원:\n",
    "    - 1 August 2024\n",
    "    - 20 Jan 2023\n",
    "    - Mon 20 Oct 2025 18.40 BST\n",
    "    - YYYY-MM-DD, YYYY-MM-DD HH:MM:SS\n",
    "    - YYYY/MM/DD 등\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    date_str = str(date_str)\n",
    "\n",
    "    # 월 이름 매핑\n",
    "    month_names = {\n",
    "        'January': '01', 'Jan': '01',\n",
    "        'February': '02', 'Feb': '02',\n",
    "        'March': '03', 'Mar': '03',\n",
    "        'April': '04', 'Apr': '04',\n",
    "        'May': '05',\n",
    "        'June': '06', 'Jun': '06',\n",
    "        'July': '07', 'Jul': '07',\n",
    "        'August': '08', 'Aug': '08',\n",
    "        'September': '09', 'Sep': '09',\n",
    "        'October': '10', 'Oct': '10',\n",
    "        'November': '11', 'Nov': '11',\n",
    "        'December': '12', 'Dec': '12'\n",
    "    }\n",
    "\n",
    "    # \"DD Month YYYY\" 또는 \"Day DD Mon YYYY\" 패턴 처리\n",
    "    for month_name, month_num in month_names.items():\n",
    "        if month_name in date_str:\n",
    "            # 연도 추출 (4자리 숫자)\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', date_str)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                return f\"{year}-{month_num}\"\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD, YYYY-MM-DD HH:MM:SS 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. content를 abstract로 매핑\n",
    "print(f\"\\n[3단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "else:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "\n",
    "# 4. abstract 정제 (오류 메시지를 null로 변환)\n",
    "print(f\"\\n[4단계] abstract 정제 (오류 메시지 제거)\")\n",
    "\n",
    "# \"내용 추출 실패\" 같은 텍스트를 null로 변환\n",
    "error_keywords = ['내용 추출 실패', '추출 실패', 'extraction failed', 'failed', 'error']\n",
    "\n",
    "def clean_abstract(text):\n",
    "    \"\"\"abstract에서 오류 메시지를 null로 변환\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "\n",
    "    text_lower = str(text).lower().strip()\n",
    "\n",
    "    # 빈 문자열\n",
    "    if not text_lower or text_lower == '':\n",
    "        return None\n",
    "\n",
    "    # 오류 키워드 체크\n",
    "    for keyword in error_keywords:\n",
    "        if keyword in text_lower:\n",
    "            return None\n",
    "\n",
    "    # 너무 짧은 텍스트 (5자 이하)\n",
    "    if len(text_lower) <= 5:\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "before_clean = df['abstract'].notna().sum()\n",
    "df['abstract'] = df['abstract'].apply(clean_abstract)\n",
    "after_clean = df['abstract'].notna().sum()\n",
    "\n",
    "print(f\"  - 정제 전: {before_clean}개\")\n",
    "print(f\"  - 정제 후: {after_clean}개\")\n",
    "print(f\"  - 제거됨: {before_clean - after_clean}개 (오류 메시지 포함)\")\n",
    "\n",
    "# 5. affiliations 칼럼 생성\n",
    "print(f\"\\n[5단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 6. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[6단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 7. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[7단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 8. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[8단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 9. 최종 컬럼 선택\n",
    "print(f\"\\n[9단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 10. 날짜별 통계\n",
    "print(f\"\\n[10단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 11. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ BBC.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "15c106c6d8b1fc2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BBC.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] BBC 파일들 읽기 및 병합\n",
      "✓ 15개의 BBC 파일 발견\n",
      "\n",
      "  - BBC_100.csv                              -> 9개 행\n",
      "  - BBC_121.csv                              -> 188개 행\n",
      "  - BBC_139.csv                              -> 159개 행\n",
      "  - BBC_20.csv                               -> 155개 행\n",
      "  - BBC_202.csv                              -> 462개 행\n",
      "  - BBC_27.csv                               -> 56개 행\n",
      "  - BBC_30.csv                               -> 22개 행\n",
      "  - BBC_40.csv                               -> 126개 행\n",
      "  - BBC_41.csv                               -> 179개 행\n",
      "  - BBC_50.csv                               -> 80개 행\n",
      "  - BBC_60.csv                               -> 9개 행\n",
      "  - BBC_61.csv                               -> 86개 행\n",
      "  - BBC_80.csv                               -> 97개 행\n",
      "  - BBC_81.csv                               -> 9개 행\n",
      "  - BBC_90.csv                               -> 163개 행\n",
      "\n",
      "✓ 병합 완료: 총 1800개 행 (15개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: 19 June 2017 → 2017-06\n",
      "  - 변환 완료: 1744개\n",
      "  ⚠ 변환 실패: 56개\n",
      "\n",
      "[3단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[4단계] abstract 정제 (오류 메시지 제거)\n",
      "  - 정제 전: 1800개\n",
      "  - 정제 후: 1726개\n",
      "  - 제거됨: 74개 (오류 메시지 포함)\n",
      "\n",
      "[5단계] affiliations 생성\n",
      "  - affiliations: BBC\n",
      "\n",
      "[6단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 1800개 행\n",
      "  - 제거 후: 1726개 행\n",
      "  - 제거됨: 74개 행\n",
      "\n",
      "[7단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 1726개 행\n",
      "  - 필터링 후: 992개 행\n",
      "  - 제거됨: 734개 행\n",
      "\n",
      "[8단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 992개 행\n",
      "  - 제거 후: 843개 행\n",
      "  - 제거됨: 149개 행\n",
      "\n",
      "[9단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[10단계] 날짜별 통계\n",
      "  - 총 27개의 날짜\n",
      "  - 첫 날짜: 2023-01 (9개)\n",
      "  - 마지막 날짜: 2025-10 (32개)\n",
      "\n",
      "  [최근 10개 날짜]\n",
      "    - 2024-06: 5개\n",
      "    - 2024-07: 7개\n",
      "    - 2024-08: 15개\n",
      "    - 2024-09: 8개\n",
      "    - 2025-05: 27개\n",
      "    - 2025-06: 72개\n",
      "    - 2025-07: 85개\n",
      "    - 2025-08: 83개\n",
      "    - 2025-09: 83개\n",
      "    - 2025-10: 32개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/BBC.csv\n",
      "✓ 최종 행 수: 843개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: BBC\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract keywords affiliations\n",
      "5  2023-06                                     AI will transform finance sector, regulator says  Artificial Intelligence (AI) will lead to the \"transformation of business\" in Guernsey's finance industry, the regulator has said.\\nThe director general of Guernsey's Financial Services Commission (GFSC) was speaking after a conference looking at the role of AI in the sector.\\nWilliam Mason said it \"will change jobs\" but not lead to fewer employees.\\nHe believes more basic tasks will be done by AI, leaving staff to focus on more complicated roles.\\n\"At the moment we are in a full employment society,...     None          BBC\n",
      "6  2023-06                    Grab: South East Asia's leading ride-hailing firm cuts 1,000 jobs  South East Asia's leading ride-hailing and food delivery app, Grab, says it is cutting 1,000 jobs - amounting to 11% of its workforce.\\nThe company's boss said the cuts were needed to bring down costs and ensure affordable services in the long term.\\nThe Singapore-based firm offers deliveries, rides and financial services in eight Southeast Asian countries.\\nIn 2018, Grab took over the operations of US-based rival Uber in the region.\\nIn an email to employees, chief executive Anthony Tan said the cu...     None          BBC\n",
      "8  2023-04  Sony World Photography Award 2023: Winner refuses award after revealing AI creation  The winner of a major photography award has refused his prize after revealing his work was created using AI.\\nGerman artist Boris Eldagsen's entry, entitled Pseudomnesia: The Electrician, won the creative open category at last week's Sony World Photography Award.\\nHe said he used the picture to test the competition and to create a discussion about the future of photography.\\nOrganisers of the award told BBC News Eldagsen had misled them about the extent of AI that would be involved.\\nIn a statement ...     None          BBC\n",
      "\n",
      "============================================================\n",
      "✓ BBC.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.WallStreetJournal.csv 만들기\n",
    "- 데이터 병합\n",
    "- date 형식이 지금 23 Aug 2025, . 24 July 2025. 이렇게 되어있음 -> 년도와 월만 남길 것.\n",
    "- media라고 되어있는 컬럼 -> affiliations로 변경\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 WallStreetJournal.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "defb239ec534ee6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:36:04.781313Z",
     "start_time": "2025-10-22T11:36:04.606944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"WallStreetJournal\"\n",
    "# 데이터 파일 경로\n",
    "FILE_PATH_1 = \"/home/dslab/choi/Journal/Data/Industry/WallStreetJournal/wallstreet.csv\"\n",
    "FILE_PATH_2 = \"/home/dslab/choi/Journal/Data/Industry/WallStreetJournal/wallstreet_content.csv\"\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"WallStreetJournal.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. WallStreetJournal 데이터 읽기 및 병합\n",
    "print(\"[1단계] WallStreetJournal 파일들 읽기 및 병합\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# 첫 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_1):\n",
    "    try:\n",
    "        df1 = pd.read_csv(FILE_PATH_1)\n",
    "        all_dfs.append(df1)\n",
    "        print(f\"  ✓ 파일 1: {FILE_PATH_1}\")\n",
    "        print(f\"    - 행 수: {len(df1)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 1 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 1을 찾을 수 없습니다: {FILE_PATH_1}\")\n",
    "\n",
    "# 두 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_2):\n",
    "    try:\n",
    "        df2 = pd.read_csv(FILE_PATH_2)\n",
    "        all_dfs.append(df2)\n",
    "        print(f\"  ✓ 파일 2: {FILE_PATH_2}\")\n",
    "        print(f\"    - 행 수: {len(df2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 2 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 2를 찾을 수 없습니다: {FILE_PATH_2}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"  ❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (년월만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원:\n",
    "    - 23 Aug 2025\n",
    "    - 24 July 2025\n",
    "    - . 28 July 2025: B.3.\n",
    "    - 19 Aug 2025\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    # 문자열로 변환하고 앞뒤 공백 및 특수문자 제거\n",
    "    date_str = str(date_str).strip()\n",
    "    # 앞의 점, 공백 제거\n",
    "    date_str = date_str.lstrip('. ')\n",
    "\n",
    "    # 월 이름 매핑\n",
    "    month_names = {\n",
    "        'January': '01', 'Jan': '01',\n",
    "        'February': '02', 'Feb': '02',\n",
    "        'March': '03', 'Mar': '03',\n",
    "        'April': '04', 'Apr': '04',\n",
    "        'May': '05',\n",
    "        'June': '06', 'Jun': '06',\n",
    "        'July': '07', 'Jul': '07',\n",
    "        'August': '08', 'Aug': '08',\n",
    "        'September': '09', 'Sep': '09',\n",
    "        'October': '10', 'Oct': '10',\n",
    "        'November': '11', 'Nov': '11',\n",
    "        'December': '12', 'Dec': '12'\n",
    "    }\n",
    "\n",
    "    # \"DD Month YYYY\" 패턴 처리 (콜론이나 다른 텍스트 앞에 있어도 처리)\n",
    "    for month_name, month_num in month_names.items():\n",
    "        if month_name in date_str:\n",
    "            # 연도 추출 (4자리 숫자)\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', date_str)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                return f\"{year}-{month_num}\"\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. media를 affiliations로 변경\n",
    "print(f\"\\n[3단계] media → affiliations 변경\")\n",
    "\n",
    "if 'media' in df.columns:\n",
    "    df['affiliations'] = df['media']\n",
    "    print(f\"  ✓ media 컬럼을 affiliations로 변경\")\n",
    "else:\n",
    "    # media 컬럼이 없으면 WallStreetJournal로 설정\n",
    "    df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  ⚠ media 컬럼이 없습니다. affiliations를 '{JOURNAL_NAME}'로 설정\")\n",
    "\n",
    "# 4. content를 abstract로 매핑\n",
    "print(f\"\\n[4단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "elif 'abstract' not in df.columns:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "else:\n",
    "    print(f\"  ✓ abstract 컬럼이 이미 존재합니다\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ WallStreetJournal.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "de9c54256eda8dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WallStreetJournal.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] WallStreetJournal 파일들 읽기 및 병합\n",
      "  ✓ 파일 1: /home/dslab/choi/Journal/Data/Industry/WallStreetJournal/wallstreet.csv\n",
      "    - 행 수: 182\n",
      "  ✓ 파일 2: /home/dslab/choi/Journal/Data/Industry/WallStreetJournal/wallstreet_content.csv\n",
      "    - 행 수: 1593\n",
      "\n",
      "✓ 병합 완료: 총 1775개 행 (2개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: 10 Sep 2025 → 2025-09\n",
      "  - 변환 완료: 1144개\n",
      "  ⚠ 변환 실패: 631개\n",
      "\n",
      "[3단계] media → affiliations 변경\n",
      "  ✓ media 컬럼을 affiliations로 변경\n",
      "\n",
      "[4단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 1775개 행\n",
      "  - 제거 후: 1144개 행\n",
      "  - 제거됨: 631개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 1144개 행\n",
      "  - 필터링 후: 971개 행\n",
      "  - 제거됨: 173개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 971개 행\n",
      "  - 제거 후: 955개 행\n",
      "  - 제거됨: 16개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 총 33개의 날짜\n",
      "  - 첫 날짜: 2023-01 (5개)\n",
      "  - 마지막 날짜: 2025-09 (51개)\n",
      "\n",
      "  [최근 10개 날짜]\n",
      "    - 2024-12: 38개\n",
      "    - 2025-01: 54개\n",
      "    - 2025-02: 59개\n",
      "    - 2025-03: 45개\n",
      "    - 2025-04: 36개\n",
      "    - 2025-05: 51개\n",
      "    - 2025-06: 40개\n",
      "    - 2025-07: 83개\n",
      "    - 2025-08: 67개\n",
      "    - 2025-09: 51개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/WallStreetJournal.csv\n",
      "✓ 최종 행 수: 955개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: WallStreetJournal\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                 title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract keywords         affiliations\n",
      "0  2025-09                      Business News: EBay Takes onArtificialIntelligenceto Spur Growth  EBay, a survivor of the first generation of internet companies, is trying to remake itself for the age of artificial intelligence.\\nThe company rode the internet's early e-commerce boom and the dawn of the mobile era to become a publicly traded billion-dollar firm, but has since faced activist investor pressure amid slowing sales.\\nLooking to boost its business, the company is making use of AI agents, encouraging engineers to use AI to write code and launching AI features that personalize buying and simplify selling.\\nThe San Jose, Calif.-based company manages and runs its own data centers, helping it maneuver the tech infrastructure changes that AI requires, the company said. That includes amassing its own graphics processing units.\\nParantap Lahiri, eBay's vice president of network and data-center engineering, said the company has thousands of its own GPUs but also relies on the major cloud providers, including Microsoft's Azure. The company works with multiple suppliers, including Nvidia, to procure its GPUs and other hardware.\\nEBay also uses AI models from ChatGPT-maker OpenAI, Lahiri said, as well as open-source models from companies like Meta Platforms, and trains its own AI models.\\nThe company last year announced that it had built a supercomputer using its own hardware. That enabled eBay to build and run models about 100 times as large as the year before, according to the company.\\nEBay's other advantage, its executives argue, is its three decades of data from its buyers and sellers, who have collectively made billions of transactions.\\nIn other areas of the company, AI is a foundation for its over 11,500 employees, who are asked to work the technology into their daily tasks, executives say.\\nEach project an employee embarks on should have an AI agent attached, said Nitzan Mekel-Bobrov, eBay's chief AI officer. The bots, built and hosted on eBay's platform, can be created by any employee for specific tasks they are doing, have a record of every meeting, and should reduce the need for workers to schedule new ones by a double-digit percentage this year, he said.\\nFor eBay's engineers, AI is used to help write code. The company is using and testing different AI-based coding tools, such as Microsoft's GitHub Copilot and \"vibe coding\" tools like Windsurf and Cursor, according to Senthil Padmanabhan, eBay's vice president of engineering, global verticals and API platform.\\nThe company also built its own AI model, trained on over 100 million lines of its own code, to do things like improve the efficiency of its code upgrades and code translations.\\nTo make it easier for sellers to list items, eBay rolled out an AI feature called \"magical listings.\" After sellers take a photo, eBay's AI fills in the rest of the item's details, including its suggested listing price.\\nLike other AI-generated content, however, eBay's item descriptions have been criticized online as low-quality, shallow content. On a Reddit thread, one user said they preferred eBay sellers that didn't use the AI function, calling its use a sign that sellers weren't knowledgeable.\\nEBay said that as AI itself has improved, it has continued to make improvements to its tech based on feedback from buyers and sellers, and sellers always have control over the final product description.\\nOne of eBay's challenges is going up against deep-pocketed tech giants. Amazon.com last year announced Rufus, an AI-powered shopping assistant that relies on neurosymbolic AI to help buyers navigate the shopping platform and answer product-related questions. Bargain-focused retailers Temu and Shein also have been gaining ground.\\nYet it may not matter whether eBay can capture mass retail share going up against the likes of Amazon, according to Deutsche Bank's Horowitz. As long as eBay keeps growing in its key product categories that could be enough to satisfy Wall Street.\\nIn its most recent quarter, eBay logged higher profit and revenue as its gross merchandise volume rose. The company posted a profit of $368 million, compared with $224 million a year earlier.\\n\"The investments they are making into their technology stack around AI are putting them in a position to take share in the verticals they're focused in,\" Horowitz said.\\n---\\nBelle Lin writes about AI and enterprise technology for WSJ Pro.\\nBy Belle Lin     None  Wall Street Journal\n",
      "1  2025-08  Markets:ArtificialIntelligencePowers a Rebound In Healthcare Venture-Capital Funding                                                         Growing acceptance of artificial intelligence in medicine is powering a revival of venture-capital investment in medical software and other healthcare technologies.\\nU.S. and European healthtech startups raised $7.9 billion in venture capital in the first half, putting this year on pace to be the best funding year for the sector since 2022, when companies collected $16.3 billion for the full year, according to Silicon Valley Bank, a division of First Citizens Bank.\\nHealthtech venture funding peaked at $30 billion in 2021 and slid the next two years, falling to $10.7 billion in 2023. A rebound began in 2024, when investment rose to $13.6 billion, according to SVB.\\nAI's potential to streamline administration and enable more personalized and preventive care is driving the resurgence. Some AI-driven companies are gaining scale and raising expansion rounds, including Qventus, which disclosed a $105 million Series D, led by KKR in January, and Ambience Healthcare, which in July secured $243 million in Series C funding led by Oak HC/FT and Andreessen Horowitz.\\nThis isn't a return to the pandemic-era frenzy, when a rush of initial public offerings and mergers and acquisitions fueled the run-up in investment. In 2021, a total of 59 healthtech startups globally went public or staged IPOs; in the first half of 2025, six did, according to SVB.\\nVenture capitalists are searching for startups with market traction that have a clear path to delivering a return on their investment.\\n\"There's a continued focus from investors and boards and companies alike on profitability and ROI,\" said Julie Betts Ebert, managing director, life science and healthcare for SVB.\\nHealthcare has many tasks ripe for AI automation, investors said. This year, for example, startup Mandolin said it raised $40 million in seed and Series A funding to automate insurance verification, so patients can receive specialty medications sooner, and Ellipsis Health picked up $45 million in Series A-1 backing to expand its rollout of AI-driven software used to check in on patients between doctor visits.\\n\"Healthcare is a very good end market for a lot of AI-driven workflow solutions,\" said Ambar Bhattacharyya, managing partner with Maverick Ventures, an investor in Mandolin.\\nLong hospital and health-system sales cycles have slowed healthtech startups, but some now sell directly to consumers or clinicians and are growing quickly as a result, said Wayne Hu, a partner with venture investor SignalFire.\\nSignalFire-backed Grow Therapy, for instance, uses its platform to help therapists launch their own practices, a model pioneered in the technology sector by the likes of e-commerce company Shopify.\\n\"Many of the most-explosive, highest-potential business models you see elsewhere in the technology ecosystem are finally making their way into the healthcare world,\" Hu said.\\nInsufficient numbers of clinicians in mental health, family medicine and other specialties also drive interest in AI startups seeking to fill gaps in care, said Sara Choi, a partner with Wing VC. \"There's a supply-demand dislocation because of a provider shortage in so many areas,\" Choi said. \"Patients are desperate for solutions.\"\\nAs AI makes inroads in medicine, startups are maturing faster, investors said. Not long ago, it wasn't a foregone conclusion seed-stage startups would have revenue, said Julie Yoo, a general partner with Andreessen Horowitz.\\nToday, healthtech startups are generating revenue and product adoption sooner, which gives investors confidence to underwrite them, she added.\\nInitial adoption of a startup's technology doesn't assure the company's success.\\nHealthcare customers often deploy AI tools in a limited capacity before considering a wider rollout, said Michael Greeley, co-founder and general partner of Flare Capital Partners. \"They're running these pilots to determine if there is a real hard-and-fast ROI,\" Greeley added.\\nHealthtech companies with significant revenue are approaching the public markets. Three healthtech startups globally, including musculoskeletal-care company Hinge Health, held IPOs in the first half. That tops last year, when only two held IPOs, according to SVB.\\n---\\nBrian Gormley is a writer for WSJ Pro Venture Capital.\\nBy Brian Gormley     None  Wall Street Journal\n",
      "2  2025-07                         Soaring Meta Ad Revenue Fuels Push IntoArtificialIntelligence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Meta Platforms posted 22% revenue growth in the second quarter, showing its core ad business remains strong at a time when the company is investing billions of dollars into artificial intelligence.\\nFor the first time this year, the company held its top capital-spending projections steady, a move that is sure to ease investor concerns over Chief Executive Mark Zuckerberg's AI-spending spree.\\nShares rose more than 11% in after-hours trading.\\nThe results show the extent to which Meta's advertising business will continue to pay for the company's outsized AI ambitions, as well as how the AI tools are contributing to its strength.\\nSales came in at $47.5 billion, ahead of analyst expectations. Net income for the April-to-June period was $18.3 billion, also ahead of market expectations.\\nThe technology giant said it expects to post between 17% and 24% revenue growth year over year for the current quarter.\\n\"The investments it's making in AI are already paying off in its ads business,\" said Jasmine Enberg, principal analyst at research firm eMarketer.\\nMeta is the third big technology company to report earnings this cycle, and shareholders are keeping a close watch on the extent to which revenue is increasing as a result of the AI boom relative to spending levels.\\nGoogle parent Alphabet posted record sales last week and increased its capital-expenditure expectations for the year by 13%. Microsoft, which also reported results on Wednesday, posted revenue that exceeded Wall Street's expectations, thanks to 39% growth in its cloud-services unit.\\nOn Wednesday morning, ahead of Meta's earnings release, Zuckerberg posted a video talking about his vision for personal superintelligence, an aid that will be customized for each person and help them individually.\\nZuckerberg spent the past few months building a new Superintelligence lab that is aimed to get his company out of an AI slump. He paid $14 billion for a 49% stake in Scale AI, a data-labeling startup, and hired its CEO, Alexandr Wang, as well as other AI researchers for tens of millions of dollars.\\nMeta launched a few versions of its most recent Llama model to little fanfare in April, and later delayed the launch of the model's biggest version. That version, called Behemoth, still hasn't been publicly released.\\nZuckerberg has said he expects AI to power all levers of his business in the future. He has touted its ability to improve Meta's ads, give people chatbot companions and assist engineers with coding -- and he is spending lavishly to get there. Meta reported earlier this year that it plans to spend as much as $72 billion on capital expenditures in 2025, largely on its AI investments.\\nInside the company, efforts to create advanced AI have been riddled with issues, including employee turnover and frequent restructurings.\\nZuckerberg homed in on talent as a key problem for Meta and has been recruiting top AI researchers and engineers directly via email or WhatsApp.\\nHe has offered hundreds of millions of dollars to some, and in at least one case, floated $1 billion.\\nSome researchers, including at least 10 from OpenAI, have accepted Zuckerberg's offers, while others in the industry have declined.\\nDaily active people across Meta's family of apps increased 6% year-over-year in June.\\nBy Meghan Bobrowsky     None  Wall Street Journal\n",
      "\n",
      "============================================================\n",
      "✓ WallStreetJournal.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. NewYorkTimes.csv 만들기\n",
    "- 데이터 병합 -> /home/dslab/choi/WITS/원본데이터/[2025.09.25]Industry/nyt.csv , /home/dslab/choi/WITS/원본데이터/nyt_content.csv\n",
    "- date 형식이 지금 23 Aug 2025, . 24 July 2025. 이렇게 되어있음 -> 년도와 월만 남길 것.\n",
    "- media라고 되어있는 컬럼 -> affiliations로 변경\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 NewYorkTimes.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "265d04d148a0d04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:16:47.632133Z",
     "start_time": "2025-10-22T11:16:47.382526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"NewYorkTimes\"\n",
    "# 데이터 파일 경로\n",
    "FILE_PATH_1 = \"/home/dslab/choi/WITS/원본데이터/[2025.09.25]Industry/nyt.csv\"\n",
    "FILE_PATH_2 = \"/home/dslab/choi/WITS/원본데이터/nyt_content.csv\"\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"NewYorkTimes.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. NewYorkTimes 데이터 읽기 및 병합\n",
    "print(\"[1단계] NewYorkTimes 파일들 읽기 및 병합\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# 첫 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_1):\n",
    "    try:\n",
    "        df1 = pd.read_csv(FILE_PATH_1)\n",
    "        all_dfs.append(df1)\n",
    "        print(f\"  ✓ 파일 1: {FILE_PATH_1}\")\n",
    "        print(f\"    - 행 수: {len(df1)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 1 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 1을 찾을 수 없습니다: {FILE_PATH_1}\")\n",
    "\n",
    "# 두 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_2):\n",
    "    try:\n",
    "        df2 = pd.read_csv(FILE_PATH_2)\n",
    "        all_dfs.append(df2)\n",
    "        print(f\"  ✓ 파일 2: {FILE_PATH_2}\")\n",
    "        print(f\"    - 행 수: {len(df2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 2 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 2를 찾을 수 없습니다: {FILE_PATH_2}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"  ❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (년월만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원:\n",
    "    - 23 Aug 2025\n",
    "    - 24 July 2025\n",
    "    - . 28 July 2025: B.3.\n",
    "    - 19 Aug 2025\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    # 문자열로 변환하고 앞뒤 공백 및 특수문자 제거\n",
    "    date_str = str(date_str).strip()\n",
    "    # 앞의 점, 공백 제거\n",
    "    date_str = date_str.lstrip('. ')\n",
    "\n",
    "    # 월 이름 매핑\n",
    "    month_names = {\n",
    "        'January': '01', 'Jan': '01',\n",
    "        'February': '02', 'Feb': '02',\n",
    "        'March': '03', 'Mar': '03',\n",
    "        'April': '04', 'Apr': '04',\n",
    "        'May': '05',\n",
    "        'June': '06', 'Jun': '06',\n",
    "        'July': '07', 'Jul': '07',\n",
    "        'August': '08', 'Aug': '08',\n",
    "        'September': '09', 'Sep': '09',\n",
    "        'October': '10', 'Oct': '10',\n",
    "        'November': '11', 'Nov': '11',\n",
    "        'December': '12', 'Dec': '12'\n",
    "    }\n",
    "\n",
    "    # \"DD Month YYYY\" 패턴 처리 (콜론이나 다른 텍스트 앞에 있어도 처리)\n",
    "    for month_name, month_num in month_names.items():\n",
    "        if month_name in date_str:\n",
    "            # 연도 추출 (4자리 숫자)\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', date_str)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                return f\"{year}-{month_num}\"\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. media를 affiliations로 변경\n",
    "print(f\"\\n[3단계] media → affiliations 변경\")\n",
    "\n",
    "if 'media' in df.columns:\n",
    "    df['affiliations'] = df['media']\n",
    "    print(f\"  ✓ media 컬럼을 affiliations로 변경\")\n",
    "else:\n",
    "    # media 컬럼이 없으면 NewYorkTimes로 설정\n",
    "    df['affiliations'] = JOURNAL_NAME\n",
    "    print(f\"  ⚠ media 컬럼이 없습니다. affiliations를 '{JOURNAL_NAME}'로 설정\")\n",
    "\n",
    "# 4. content를 abstract로 매핑\n",
    "print(f\"\\n[4단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "elif 'abstract' not in df.columns:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "else:\n",
    "    print(f\"  ✓ abstract 컬럼이 이미 존재합니다\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ NewYorkTimes.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "2ca28853134dc5b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NewYorkTimes.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] NewYorkTimes 파일들 읽기 및 병합\n",
      "  ✓ 파일 1: /home/dslab/choi/WITS/원본데이터/[2025.09.25]Industry/nyt.csv\n",
      "    - 행 수: 164\n",
      "  ✓ 파일 2: /home/dslab/choi/WITS/원본데이터/nyt_content.csv\n",
      "    - 행 수: 2780\n",
      "\n",
      "✓ 병합 완료: 총 2944개 행 (2개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: 23 Aug 2025 → 2025-08\n",
      "  - 변환 완료: 1016개\n",
      "  ⚠ 변환 실패: 1928개\n",
      "\n",
      "[3단계] media → affiliations 변경\n",
      "  ✓ media 컬럼을 affiliations로 변경\n",
      "\n",
      "[4단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 2944개 행\n",
      "  - 제거 후: 928개 행\n",
      "  - 제거됨: 2016개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 928개 행\n",
      "  - 필터링 후: 910개 행\n",
      "  - 제거됨: 18개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 910개 행\n",
      "  - 제거 후: 900개 행\n",
      "  - 제거됨: 10개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 총 33개의 날짜\n",
      "  - 첫 날짜: 2023-01 (3개)\n",
      "  - 마지막 날짜: 2025-09 (33개)\n",
      "\n",
      "  [최근 10개 날짜]\n",
      "    - 2024-12: 33개\n",
      "    - 2025-01: 49개\n",
      "    - 2025-02: 42개\n",
      "    - 2025-03: 49개\n",
      "    - 2025-04: 45개\n",
      "    - 2025-05: 52개\n",
      "    - 2025-06: 37개\n",
      "    - 2025-07: 94개\n",
      "    - 2025-08: 55개\n",
      "    - 2025-09: 33개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/NewYorkTimes.csv\n",
      "✓ 최종 행 수: 900개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: NewYorkTimes\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                 title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract keywords    affiliations\n",
      "0  2025-08  Bringing Art Back to Life WithArtificialIntelligence: [Foreign Desk]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Alex Kachkine spends his days working on microchip research -- a skill set surprisingly similar to that needed for restoration.\\nIn 2016, an earthquake reduced the Church of San Salvatore in Campi in central Italy mostly to rubble, destroying its 15th-century frescoes. What remained were fragments, a complex puzzle that conservators at the Ministry of Culture have been working to reassemble ever since.\\nBut the pieces are few and far between, ''isolated islands within vast areas of loss,'' Serena Di Gaetano and Federica Giacomini, senior conservator-restorers at the ministry's restoration institute, said in a joint emailed response to questions. Their work, they said, is to somehow ''bridge the gaps between what remains and what has been irretrievably lost.''\\nEnter Alex Kachkine, 25, a graduate researcher in mechanical engineering at M.I.T. who improbably rocked the art world in June with a paper in the scientific journal Nature describing a new way to restore paintings with the help of artificial intelligence.\\nA hobbyist restorer, he wrote a program that analyzes damage and prints the fixes on a super thin mask. The mask can be laid over the painting, making it appear fully restored, but can also be removed to reveal the original. To create the mask, the program used over 55,000 hues in several hours, and worked about 65 times as fast as traditional restoration, Mr. Kachkine estimated.\\nThe study was a side project, but it generated buzz among conservators around the world, including at the Ministry of Culture in Italy -- to Mr. Kachkine's surprise.\\nAt his day job, he researches the electron beam sources that create the intricate circuitry on microchips, which are then placed into phones or other devices. ''Those require very high degrees of precision,'' he said. ''And it turns out a lot of the techniques we use to achieve that level of precision are applicable to art restoration.''\\nThe Conservator's Dilemma\\nConservators have long debated just how heavy-handed to be about keeping works in viewing shape.\\n''Conservation is divided between preservation and restoration,'' said Ann Shaftel, a conservator in Canada who specializes in preserving thangkas, Tibetan Buddhist fabric paintings used to guide meditation. In her work, the seeming grime that accumulates as a thangka hangs in an incense-filled monastery has spiritual value, and she favors a very light touch when it comes to preservation.\\nA similar perspective is increasingly shared by conservators of art of all kinds, though there is no universally accepted approach. ''There is a dilemma about retouching a painting or not,'' said Hartmut Kutzke, a chemist at the Museum of Cultural History at the University of Oslo. ''It differs a little bit from country to country.''\\nSome favor maximalist interventions that restore a work to its former glory. Many prefer a minimally invasive approach that avoids irreversible changes. Mr. Kachkine's method could resolve the tensions between the two camps, according to Mr. Kutzke, who wrote an article reviewing the study.\\nSince the digitally printed mask can create a removable full restoration, it is ''a good bridge between the two worlds,'' Mr. Kutzke said in an interview.\\nBut the chemist noted that some of his colleagues who are conservators had nonetheless objected to the method. ''They were actually quite skeptical because they said it's too much change,'' he said. ''They were saying we have to accept that artworks age, and this is part of the artwork.''\\nCoding for Culture\\nIn Italy, official conservators follow strict principles. Their interventions must be reversible and visible so as not to permanently alter originals or mislead future observers. But Ms. Di Gaetano and Ms. Giacomini say the ministry's Central Institute for Restoration, known as I.C.R., also embraces cutting-edge technologies that can help preserve the past, like Mr. Kachkine's.\\nRelying on A.I. for analytic help is not new. But Mr. Kachkine's work caught the team's eye because it goes a step further, they said. ''It demonstrates how artificial intelligence algorithms, when correctly oriented and developed, can become concrete support tools not only in the study and documentation phase, but also during actual restoration operations,'' Ms. Di Gaetano and Ms. Giacomini said.\\nOnce the super thin mask is created, it's attached to the original artwork with conservation varnish. ''The mask can be peeled off with very minimal force, but the bond is there, and the bond is transparent, so it doesn't affect the way that you see the underlying painting,'' Mr. Kachkine said.\\nFor conservators facing big-scale projects that defy traditional methods, like the frescoes of the Church of San Salvatore, this innovation offers hope. With the help of a program -- and a printed screen -- filling in the missing pieces of a huge puzzle could become less daunting. And ultimately, it could mean more cultural heritage is saved.\\nThe restoration institute enlisted Mr. Kachkine seeking to develop a systematic approach to big projects that can be adapted to specific cases.\\nNow he is writing code to conform with Italy's official conservation principles. The fixes must be done in a painting style called tratteggio, developed by the restoration institute's founding director, where restored sections are painted in thin parallel lines to ensure additions are visibly distinct up-close but from afar allow the work to be appreciated as a whole.\\n''There's going to be a lot more cases like this where the technique needs to be adapted to a particular circumstance,'' Mr. Kachkine said.\\nFrom Innovative Technology to Usable Tool\\nMr. Kachkine doesn't plan to patent his method. Instead, by publishing his work, he hopes that conservators will be able to ''leverage the benefits'' of the techniques he gleaned from engineering to preserve ''really valuable cultural heritage.''\\nStill, his approach is currently too complex for most conservators to use independently. ''Many conservators have lamented to me that even though my methods are published and free to use, they would like to be able to buy masks without worrying about all the steps involved in fabricating them,'' he said.\\nHe is considering starting a business to develop the tool for commercial purposes, but for now is more focused on improving the technology.\\nLast month, he gave a talk at the de Young Museum in San Francisco and came back with additional programming tasks based on challenges faced by the conservators and curators he met there.\\nExperts say that the work has only just begun if he is to build a truly transformative tool.\\nMarc Melich-Mautner, an art and antiquities dealer in Austria, believes the method could revolutionize restoration, making it faster and less expensive, eventually making more art accessible. But accomplishing that goal will also require much more continued input from art professionals, in addition to tech development.\\n''It's still a lot of Alex behind it,'' he said of the program. ''It has to be fed with information from the restoration world, with the techniques.''     None  New York Times\n",
      "1  2025-07                         Trump Plans to Give AI Developers a Free Hand  WASHINGTON — President Donald Trump said Wednesday that he planned to speed the advance of artificial intelligence in the United States, opening the door for companies to develop the technology unfettered from oversight and safeguards, but added that AI needed to be free of “partisan bias.”\\nIn a sweeping effort to put his stamp on the policies governing the fast-growing technology, Trump signed three executive orders and outlined an “AI Action Plan,” with measures to “remove red tape and onerous regulation” as well as to make it easier for companies to build infrastructure to power AI.\\nOne executive order banned the federal government from buying AI tools it considered ideologically biased. Another order would speed up the permitting process for major AI infrastructure projects, and a third focused on promoting the export of American AI products around the world.\\n“America is the country that started the AI race,” Trump said in a Wednesday evening speech in front of administration officials and tech executives, including Jensen Huang, CEO of chipmaker Nvidia. “And as president of the United States, I’m here today to declare that America is going to win it.”\\nTrump’s plan signals that his administration has embraced AI and the tech industry’s arguments that it must be allowed to work with few guardrails for the United States to dominate a new era defined by the technology. It is a forceful repudiation of other governments, including the European Commission, that have approved regulations to govern the development of AI.\\nBut it also points to how the administration wants to shape the way AI tools present information. Conservatives have accused some tech companies of developing AI models with a baked-in liberal bias. Most AI models are already trained on copious amounts of data from across the web, which informs their responses, making any shift in focus difficult.\\nThe changes outlined Wednesday would benefit tech giants locked in a fierce contest to produce generative AI products and persuade consumers to weave the tools into their daily lives. Since OpenAI’s public release of ChatGPT in late 2022, tech companies have raced to produce their own versions of the technology, which can write humanlike texts and produce realistic images and videos.\\nGoogle, Microsoft, Meta, OpenAI and others are jockeying for access to computing power, typically from huge data centers filled with computers that can stress local resources. And the companies are facing increased competition from rivals such as Chinese startup DeepSeek, which sent shock waves around the world this year after it created a powerful AI model with far less money than many thought possible.\\nThe fight over resources in Silicon Valley has run alongside an equally charged debate in Washington over how to confront the societal transformations that AI could bring. Critics worry that if left unchecked, the technology could be a potent tool for scammers and extremists and lay waste to the economy as more jobs are automated. News outlets and artists have sued AI companies over claims that they illegally trained their technology using copyrighted works and articles.\\nTrump previously warned of China’s potential to outpace American progress on the technology. He has said that the federal government must support AI companies with tax incentives, more foreign investment and less focus on safety regulations that could hamper progress.\\nFormer President Joe Biden took one major action on artificial intelligence: a 2023 executive order that mandated safety and security standards for the development and use of AI across the federal government.\\nBut hours after his inauguration in January, Trump rolled back that order. Days later, he signed another executive order, “Removing Barriers to American Leadership in Artificial Intelligence,” which called for an acceleration of AI development by U.S. tech companies and for versions of the technology that operated without ideological bias.\\nThe order included a mandate for administration officials to come up with “an artificial intelligence action plan,” with policy guidelines to encourage the growth of the AI industry. The administration solicited comments from companies while it considered its plan.\\nOpenAI called for the administration to expand its list of countries eligible to import AI technologies from the United States, a list that has been limited by controls designed to stop China from gaining access to American technology. OpenAI and Google called for greater support in building AI data centers through tax breaks and fewer barriers for foreign investment.\\nLast week, Nvidia said the administration had approved the sale of one of its chips, offered specifically in China, that had previously been banned.\\nThe plan released Wednesday outlined a wide range of policy shifts, divided into moves that the administration said would speed up the development of AI, make it easier to build and power data centers and promote the interests of American companies abroad.\\nThe federal government should impose fewer environmental regulations on the construction of new data centers and support training programs for workers needed to staff the facilities, the plan said. One of the executive orders signed Wednesday would establish a fast-tracked permitting process for data centers.\\nThe plan also called for the government to collect information on regulations that could hinder AI innovation and “take appropriate action.” It also said it would withhold funding for AI-related projects if a state’s regulations made deploying that funding less effective.\\nThe plan — and one of the executive orders — also calls for the government to give federal contracts to AI companies that “ensure that their systems are objective.” It said a government agency should revise guidelines for AI’s development to remove mentions of diversity, equity and inclusion; climate change; and misinformation.\\n“Once and for all, we are getting rid of woke,” Trump said in his speech. “The American people do not want woke Marxist lunacy in the AI models.”\\nThe power of the federal government should be used to ensure that AI systems are built “with freedom of speech and expression in mind,” according to the plan. That echoes long-standing conservative claims that products produced by tech companies, including online platforms like Facebook and YouTube, favor left-leaning perspectives.\\nThe government should encourage the export of American AI tools, the plan said, and called for federal agencies to help the industry sell packages of AI products abroad and work to counter China’s influence over the technology.\\nDuring his speech, Trump conducted a roll call of administration appointees and executives in attendance, including his chief of staff, Susie Wiles, and the CEO of chipmaker AMD, Lisa Su. He thanked David Sacks, his AI and crypto czar, and Sacks’ co-hosts on “All-In,” a popular podcast in Silicon Valley that helped to put on Wednesday’s event. And Trump repeatedly praised Nvidia’s Huang, who was seated in front of the stage.\\nThe president also touched on tariff negotiations, transgender athletes and the recent passage of his domestic policy bill.\\n“We’re going to make this industry absolutely the top because right now it’s a beautiful baby that’s born,” he said about AI. “We have to grow that baby and let that baby thrive. We can’t stop it.”\\nWhen he was done speaking, Trump moved from the lectern to the side of the stage and a small desk, holding each executive order up after signing it. He handed the pen he used to sign one of the orders to Sacks. After wishing the crowd luck, Trump left the stage.\\nThis article originally appeared in The New York Times.     None  New York Times\n",
      "2  2025-07          Trump Administration Plans to Give AI Developers a Free Hand                                                                                                                                                                                                                                                                                   WASHINGTON — The Trump administration said Wednesday that it planned to speed the development of artificial intelligence in the United States, opening the door for companies to develop the technology unfettered from oversight and safeguards, but added that the AI needed to be free of “ideological bias.”\\nIn a sweeping effort to put his stamp on the policies governing the fast-growing technology, President Donald Trump’s AI Action Plan outlines measures to “remove red tape and onerous regulation,” as well as make it easier for companies to build infrastructure to power AI.\\nThe plan also calls for the government to give federal contracts to companies that “ensure that their systems are objective.” It said a government agency should revise guidelines for AI’s development to remove mentions of diversity, equity and inclusion, climate change and misinformation.\\nThe report signals that the Trump administration has embraced AI and the tech industry’s arguments that it must be allowed to work with few guardrails for the United States to dominate a new era defined by the technology. It is a forceful repudiation of other governments, including the European Commission, that have approved regulations to govern the development of the technology.\\nBut it also points to how the administration wants to shape the way AI tools present information. Conservatives have accused some tech companies of developing AI models with a baked-in liberal bias. Most AI models are already trained on copious amounts of data from across the web, which informs their responses, making any shift in focus difficult.\\nOn Wednesday afternoon, Trump is scheduled to deliver his first major speech on AI, a technology that experts have said could upend communications, geopolitics and the economy in the coming years. The president is also expected to sign executive orders related to the technology.\\n“We believe we’re in an AI race,” David Sacks, the White House AI and crypto czar, said on a call with reporters. “And we want the United States to win that race.”\\nThe changes outlined Wednesday would benefit tech giants locked in a fierce contest to produce generative AI products and persuade consumers to weave the tools into their daily lives. Since OpenAI’s public release of ChatGPT in late 2022, tech companies have raced to produce their own versions of the technology, which can write humanlike texts and produce realistic images and videos.\\nGoogle, Microsoft, Meta, OpenAI and others are jockeying for access to computing power, typically from huge data centers filled with computers that can stress local communities’ resources. And the companies are facing increased competition from rivals such as Chinese startup DeepSeek, which sent shock waves around the world this year after it created a powerful AI model with far less money than many thought possible.\\nThe fight over resources in Silicon Valley has run alongside an equally charged debate in Washington over how to confront the societal transformations that AI could bring. Critics worry that if left unchecked, the technology could be a potent tool for scammers and extremists and lay waste to the economy as more jobs are automated. News outlets and artists have sued AI companies over claims that they illegally trained their technology using copyrighted works and articles.\\nTrump previously warned of China’s potential to outpace American progress on the technology. He has said that the federal government must support AI companies with tax incentives, more foreign investment and less focus on safety regulations that could hamper progress.\\nPresident Joe Biden took one major action on artificial intelligence: a 2023 executive order that mandated safety and security standards for the development and use of AI across the federal government.\\nBut hours after his inauguration in January, Trump rolled back that order. Days later, he signed another executive order, “Removing Barriers to American Leadership in Artificial Intelligence,” which called for an acceleration of AI development by U.S. tech companies and for versions of the technology that operated without ideological bias.\\nThe order included a mandate for administration officials to come up with “an artificial intelligence action plan,” with policy guidelines to encourage the growth of the AI industry. The administration solicited comments from companies while it considered its plan.\\nOpenAI called for the administration to expand its list of countries eligible to import AI technologies from the United States, a list that has been limited by controls designed to stop China from gaining access to American technology. OpenAI and Google called for greater support in building AI data centers through tax breaks and fewer barriers for foreign investment.\\nOpenAI, Google and Meta also said they believed they had legal access to copyrighted works like books, films and art for training their AI. Meta asked the White House to issue an executive order or other action to “clarify that the use of publicly available data to train models is unequivocally fair use.”\\nThe plan released Wednesday did not include mentions of copyright law. But it did outline a wide range of policy shifts, divided into moves that the administration said would speed up the development of AI, make it easier to build and power data centers and promote the interests of American companies abroad.\\nThe federal government should impose fewer environmental regulations on the construction of new data centers and support training programs for workers needed to staff the facilities, the plan said.\\nThe report called for the government to collect comments from companies and the public about regulations that “hinder AI innovation and adoption, and work with relevant federal agencies to take appropriate action.”\\nIt also threatened that states with laws the administration deemed overly onerous could be at risk of losing out on federal funding related to AI and said that the Federal Communications Commission should evaluate whether any state AI rules conflict with its authority over the nation’s networks.\\nThe power of the federal government should be used to ensure AI systems are built “with freedom of speech and expression in mind,” the plan said. That echoes long-standing conservative claims that products produced by tech companies, including online platforms such as Facebook and YouTube, favor left-leaning perspectives.\\nIt also called on the Department of Commerce to revise a 2023 framework that offers guidance on how to reduce risks associated with the development of AI, removing mentions of DEI, climate change and misinformation.\\nThe government should prioritize the export of American AI tools, the plan said, and called for federal agencies to help the industry sell packages of AI products abroad and work to counter China’s influence over the technology.\\nOther initiatives include promoting the use of AI by federal agencies and the Department of Defense, studying the technology’s effects on the workforce and promoting training for the general population.\\n“As our global competitors race to exploit these technologies, it is a national security imperative for the United States to achieve and maintain unquestioned and unchallenged global technological dominance,” Trump said in the plan. “To secure our future, we must harness the full power of American innovation.”\\nThis article originally appeared in The New York Times.     None  New York Times\n",
      "\n",
      "============================================================\n",
      "✓ NewYorkTimes.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. CNN.csv 만들기\n",
    "- 데이터 병합 -> /home/dslab/choi/Journal/Data/Industry/CNN/cnn.csv , /home/dslab/choi/Journal/Data/Industry/CNN/CNN.csv\n",
    "- date 형식이 지금 3:23 PM EDT, Tue July 8, 2025 이렇게 되어있음 -> 년도와 월만 남길 것.\n",
    "- affiliations로 생성 -> CNN으로\n",
    "- null, 중복 제거 -> title, content 기준\n",
    "- 최종 데이터 저장 CNN.csv로 필요한 컬럼만 -> date, title, abstract, keywords, affiliations\n",
    "- 2023년 ~ 2025년만 남기기"
   ],
   "id": "da9b37a91cb2b7c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T11:49:52.763348Z",
     "start_time": "2025-10-22T11:49:52.584858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "JOURNAL_NAME = \"CNN\"\n",
    "# 데이터 파일 경로\n",
    "FILE_PATH_1 = \"/home/dslab/choi/Journal/Data/Industry/CNN/cnn.csv\"\n",
    "FILE_PATH_2 = \"/home/dslab/choi/Journal/Data/Industry/CNN/CNN.csv\"\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CNN.csv 생성 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. CNN 데이터 읽기 및 병합\n",
    "print(\"[1단계] CNN 파일들 읽기 및 병합\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# 첫 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_1):\n",
    "    try:\n",
    "        df1 = pd.read_csv(FILE_PATH_1)\n",
    "        all_dfs.append(df1)\n",
    "        print(f\"  ✓ 파일 1: {FILE_PATH_1}\")\n",
    "        print(f\"    - 행 수: {len(df1)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 1 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 1을 찾을 수 없습니다: {FILE_PATH_1}\")\n",
    "\n",
    "# 두 번째 파일 읽기\n",
    "if os.path.exists(FILE_PATH_2):\n",
    "    try:\n",
    "        df2 = pd.read_csv(FILE_PATH_2)\n",
    "        all_dfs.append(df2)\n",
    "        print(f\"  ✓ 파일 2: {FILE_PATH_2}\")\n",
    "        print(f\"    - 행 수: {len(df2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 파일 2 읽기 오류: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ 파일 2를 찾을 수 없습니다: {FILE_PATH_2}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"  ❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. date 컬럼 처리 (년월만 남기기)\n",
    "print(f\"\\n[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\")\n",
    "\n",
    "if 'date' not in df.columns:\n",
    "    print(f\"  ❌ 'date' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "def format_date_to_month(date_str):\n",
    "    \"\"\"\n",
    "    date를 YYYY-MM 형식으로 변환\n",
    "    다양한 형식 지원:\n",
    "    - 3:23 PM EDT, Tue July 8, 2025\n",
    "    - 10:15 AM PST, Mon January 15, 2024\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "\n",
    "    date_str = str(date_str).strip()\n",
    "\n",
    "    # 월 이름 매핑\n",
    "    month_names = {\n",
    "        'January': '01', 'Jan': '01',\n",
    "        'February': '02', 'Feb': '02',\n",
    "        'March': '03', 'Mar': '03',\n",
    "        'April': '04', 'Apr': '04',\n",
    "        'May': '05',\n",
    "        'June': '06', 'Jun': '06',\n",
    "        'July': '07', 'Jul': '07',\n",
    "        'August': '08', 'Aug': '08',\n",
    "        'September': '09', 'Sep': '09',\n",
    "        'October': '10', 'Oct': '10',\n",
    "        'November': '11', 'Nov': '11',\n",
    "        'December': '12', 'Dec': '12'\n",
    "    }\n",
    "\n",
    "    # 월 이름과 연도 추출\n",
    "    for month_name, month_num in month_names.items():\n",
    "        if month_name in date_str:\n",
    "            # 연도 추출 (4자리 숫자)\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', date_str)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                return f\"{year}-{month_num}\"\n",
    "\n",
    "    # YYYY-MM 추출 (YYYY-MM-DD, YYYY/MM/DD 등)\n",
    "    match = re.search(r'(\\d{4})[-/](\\d{2})', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return f\"{year}-{month}\"\n",
    "\n",
    "    # YYYY만 있는 경우\n",
    "    match = re.search(r'^(\\d{4})$', date_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-01\"\n",
    "\n",
    "    return None\n",
    "\n",
    "before_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "df['date'] = df['date'].apply(format_date_to_month)\n",
    "after_format = df['date'].iloc[0] if len(df) > 0 else None\n",
    "\n",
    "print(f\"  - 변환 예시: {before_format} → {after_format}\")\n",
    "print(f\"  - 변환 완료: {df['date'].notna().sum()}개\")\n",
    "if df['date'].isna().sum() > 0:\n",
    "    print(f\"  ⚠ 변환 실패: {df['date'].isna().sum()}개\")\n",
    "\n",
    "# 3. content를 abstract로 매핑\n",
    "print(f\"\\n[3단계] content → abstract 매핑\")\n",
    "\n",
    "if 'content' in df.columns:\n",
    "    df['abstract'] = df['content']\n",
    "    print(f\"  ✓ content 컬럼을 abstract로 복사\")\n",
    "elif 'abstract' not in df.columns:\n",
    "    print(f\"  ⚠ content 컬럼이 없습니다. abstract를 빈 값으로 설정\")\n",
    "    df['abstract'] = None\n",
    "else:\n",
    "    print(f\"  ✓ abstract 컬럼이 이미 존재합니다\")\n",
    "\n",
    "# 4. affiliations 칼럼 생성\n",
    "print(f\"\\n[4단계] affiliations 생성\")\n",
    "df['affiliations'] = JOURNAL_NAME\n",
    "print(f\"  - affiliations: {JOURNAL_NAME}\")\n",
    "\n",
    "# 5. null 값 제거 (title, content 기준)\n",
    "print(f\"\\n[5단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "# title 컬럼 존재 여부 확인\n",
    "if 'title' not in df.columns:\n",
    "    print(f\"  ❌ 'title' 컬럼이 없습니다!\")\n",
    "    print(f\"  현재 컬럼: {list(df.columns)}\")\n",
    "    exit()\n",
    "\n",
    "before_null = len(df)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 6. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[6단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(df)\n",
    "df['year'] = df['date'].apply(extract_year)\n",
    "df = df[df['year'].isin(VALID_YEARS)]\n",
    "df = df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(df)\n",
    "\n",
    "print(f\"  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 7. 중복 제거 (title, content 기준)\n",
    "print(f\"\\n[7단계] 중복 제거 (title, abstract 기준)\")\n",
    "before_dup = len(df)\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 8. 최종 컬럼 선택\n",
    "print(f\"\\n[8단계] 최종 컬럼 선택\")\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "\n",
    "# 존재하는 컬럼만 선택\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = None\n",
    "\n",
    "final_df = df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "\n",
    "# 9. 날짜별 통계\n",
    "print(f\"\\n[9단계] 날짜별 통계\")\n",
    "\n",
    "if final_df['date'].notna().any():\n",
    "    date_stats = final_df.groupby('date').size().sort_index()\n",
    "\n",
    "    # 통계가 너무 많으면 요약만 표시\n",
    "    if len(date_stats) > 20:\n",
    "        print(f\"  - 총 {len(date_stats)}개의 날짜\")\n",
    "        print(f\"  - 첫 날짜: {date_stats.index[0]} ({date_stats.iloc[0]}개)\")\n",
    "        print(f\"  - 마지막 날짜: {date_stats.index[-1]} ({date_stats.iloc[-1]}개)\")\n",
    "        print(f\"\\n  [최근 10개 날짜]\")\n",
    "        for date, count in date_stats.tail(10).items():\n",
    "            print(f\"    - {date}: {count}개\")\n",
    "    else:\n",
    "        for date, count in date_stats.items():\n",
    "            print(f\"  - {date}: {count}개\")\n",
    "else:\n",
    "    print(f\"  - date 정보가 없습니다.\")\n",
    "\n",
    "# 10. 결과 저장\n",
    "output_filename = f\"{JOURNAL_NAME}.csv\"\n",
    "output_path = os.path.join(\"/home/dslab/choi/Journal/Data/Industry\", output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 소스: {JOURNAL_NAME}\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(3).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ CNN.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "9f57686cb07a65a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CNN.csv 생성 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] CNN 파일들 읽기 및 병합\n",
      "  ✓ 파일 1: /home/dslab/choi/Journal/Data/Industry/CNN/cnn.csv\n",
      "    - 행 수: 48\n",
      "  ✓ 파일 2: /home/dslab/choi/Journal/Data/Industry/CNN/CNN.csv\n",
      "    - 행 수: 532\n",
      "\n",
      "✓ 병합 완료: 총 580개 행 (2개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] date 컬럼 처리 (YYYY-MM 형식으로 변환)\n",
      "  - 변환 예시: nan → None\n",
      "  - 변환 완료: 568개\n",
      "  ⚠ 변환 실패: 12개\n",
      "\n",
      "[3단계] content → abstract 매핑\n",
      "  ✓ content 컬럼을 abstract로 복사\n",
      "\n",
      "[4단계] affiliations 생성\n",
      "  - affiliations: CNN\n",
      "\n",
      "[5단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 580개 행\n",
      "  - 제거 후: 532개 행\n",
      "  - 제거됨: 48개 행\n",
      "\n",
      "[6단계] 연도 필터링 (2023-2025)\n",
      "  - 필터링 전: 532개 행\n",
      "  - 필터링 후: 496개 행\n",
      "  - 제거됨: 36개 행\n",
      "\n",
      "[7단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 496개 행\n",
      "  - 제거 후: 458개 행\n",
      "  - 제거됨: 38개 행\n",
      "\n",
      "[8단계] 최종 컬럼 선택\n",
      "  ⚠ 누락된 컬럼: ['keywords']\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[9단계] 날짜별 통계\n",
      "  - 2023-01: 216개\n",
      "  - 2024-01: 177개\n",
      "  - 2025-01: 65개\n",
      "\n",
      "============================================================\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/CNN.csv\n",
      "✓ 최종 행 수: 458개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 소스: CNN\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "       date                                                                                   title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstract keywords affiliations\n",
      "48  2025-01  Microsoft, OpenAI and Anthropic are investing millions to train teachers how to use AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A group of leading tech companies is teaming up with two teachers’ unions to train 400,000 kindergarten through 12th grade teachers in artificial intelligence over the next five years.\\nThe National Academy of AI Instruction, announced on Tuesday, is a $23 million initiative backed by Microsoft, OpenAI, Anthropic, the national American Federation of Teachers and New York-based United Federation of Teachers. As part of the effort, the group says it will develop AI training curriculum for teachers that can be distributed online and at an in-person campus in New York City.\\nThe announcement comes as schools, teachers and parents grapple with whether and how AI should be used in the classroom. Educators want to make sure students know how to use a technology that’s already transforming workplaces, while teachers can use AI to automate some tasks and spend more time engaging with students. But AI also raises ethical and practical questions, which often boil down to: If kids use AI to assist with schoolwork and teachers use AI to help with lesson planning or grading papers, where is the line between advancing student learning versus hindering it?\\nSome schools have prohibited the use of AI in classrooms, while others haveembraced it. In New York City, the education departmentbanned the use of ChatGPTfrom school devices and networks in 2023, beforereversing coursemonths later and developing an AI policy lab to explore the technology’s potential.\\nThe new academy hopes to create a national model for how schools and teachers can integrate AI into their curriculum and teaching processes, without adding to the administrative work that so often burdens educators.\\nRelated articleHere’s how Character.AI’s new CEO plans to address fears around kids’ use of chatbots\\n“AI holds tremendous promise but huge challenges—and it’s our job as educators to make sure AI serves our students and society, not the other way around,” AFT President Randi Weingarten said in a statement. “The academy is a place where educators and school staff will learn about AI—not just how it works, but how to use it wisely, safely and ethically.”\\nThe program will include workshops, online courses and in-person trainings designed by AI experts and educators, and instruction will begin this fall. Microsoft is set to invest $12.5 million in the training effort over the next five years, and OpenAI will contribute $10 million — $2 million of which will be in in-kind resources such as computing access. Anthropic plans to invest $500 million in the project’s first year and may spend more over time.\\nThe tech companies involved also stand to benefit by gaining feedback from teachers and potentially getting their AI tools in the hands of educators and students around the country. Similar educational partnerships have been a boon to tech companies in the past — Google Chromebooks, for example, are widely used in part because of their popularity in classrooms.\\nChris Lehane, chief global affairs officer at OpenAI, told CNN at the program’s launch event in New York City on Tuesday that the trainings will be a mix of general information on how AI systems work and specific instruction on tools from Microsoft, OpenAI and Anthropic.  There’s also potential for new AI products to be developed by or in partnership with the teachers.\\n“How can we make sure that, in the K-12 context, that we’re equipping those kids, those students, with the skills that they’re going to need to be able to succeed in what we think of as the intelligence age?” Lehane said during the event. “And you can’t do that unless it’s actually given to the teachers to do that work.”     None          CNN\n",
      "49  2025-01   Here’s how Character.AI’s new CEO plans to address fears around kids’ use of chatbots  When Karandeep Anand’s 5-year-old daughter gets home from school, they fire up the artificial intelligence chatbot platform Character.AI so she can chat about her day with her favorite characters, such as “Librarian Linda.”\\nAnand’s experience using the product as a parent might be helpful now that he’s Character.AI’s new chief executive, a change the companyannouncedlast month.\\nHe’s taken on the top job at a complicated moment for the company, which lets users talk to a variety of AI-generated personas. Character.AI faces fierce competition in an increasingly crowded space, as well aslawsuitsfrom families who claim the service exposed their children to inappropriate content and failed to implementadequate safeguards.\\nCharacter.AI has also received tough questions about safety fromlawmakers, and one advocacy groupsaid earlier this yearthat AI companion apps should not be used by kids under 18. Even for adult users, experts have raised alarms about peopleforming potentially harmful attachmentsto AI characters.\\nAnand brings experience at some of the biggest tech companies to his new role leading Character.AI’s approximately 70-person team. He spent 15 years at Microsoft and six years at Meta, including as vice president and head of business products at the social media giant. He also served as a board advisor for Character.AI before joining as CEO.\\nAnd he told CNN he sees a bright future for the platform in interactive AI entertainment.\\nIn other words, rather than people consuming “brain rot” on social media for entertainment, Anand wants them co-creating stories and conversations with Character.AI for fun.\\n“AI can power a very, very powerful personal entertainment experience unlike anything we’ve seen in the last 10 years in social media, and definitely nothing like what TV used to be,” Anand said in an interview.\\nUnlike multi-purpose AI tools like ChatGPT, Character.AI offers range of different chatbots that are often modeled after celebrities and fictional characters. Users can also create their own for conversations or role play. Another distinction is that Character.AI bots respond with human-like conversational cues, adding references to facial expressions or gestures into their replies.\\nThe personas of AI characters on the app vary widely, from romantic partners to language tutors or Disney characters. It also features characters like “Friends hot mom,” which describes itself as “curvy, busty, kind, loving, shy, motherly, sensual”; and “Therapist,” which calls itself a “licensed CBT therapist,” although it features a disclaimer that it is not a real person or licensed professional.\\n“(We’re) doubling down on entertainment, doubling down on trust and safety,” Anand said. “And a lot of the work we want to do is enable an entirely new creator ecosystem around AI entertainment.”\\nCharacter.AI was first sued by a parent — aFlorida momwho alleges her 14-year-old son died by suicide after developing an inappropriate relationship with chatbots on the platform — last October. Two months later, two more familiesfiled a joint suitagainst the company, accusing it of providing sexual content to their children and encouraging self-harm and violence.\\nSince then, the company has implemented a range of new safety measures, including a pop-up that directs users who mention self-harm or suicide to the National Suicide Prevention Lifeline. It also updated its AI model for users under the age of 18 to reduce the likelihood that they encounter sensitive or suggestive content, and gives parents the option to receive a weekly email about their teen’s activity on the platform.\\nRelated videoMom describes dark exchange son had with bot\\nAnand said he’s confident in the improvements Character.AI has made since last year, but that work to keep the platform safe, especially for young users, continues. Character.AI’s policies technically require users to be over the age of 13, although it does not ask for information to verify that users are signing up with the correct birthdate.\\n“The tech and the industry and the user base is constantly evolving (so) that we can never let the guard off. We have to constantly stay ahead of the curve,” Anand said.\\nHe added that the company continues to test how people could misuse new features to prevent abuse, such as a video generatorlaunched last monththat lets users animate their bots. In the days following the tool’s arrival, userssharedunsuccessful attempts to test its limits by creating fake videos of prominent figures like Elon Musk.\\n“We had to red team the product for such a long time to make sure you cannot use this for any negative use case like deepfakes or bullying,” Anand said.\\nThose efforts aside, Anand said in anintroductory noteto Character.AI users last month that one of his top priorities is to make the platform’s safety filter “less overbearing,” adding that “too often, the app filters things that are perfectly harmless.”\\nHe told CNN that things like mentions of blood when users are engaging in “vampire fan fiction role play” — something he says he’s a fan of — might be censored under the current model, which he wants to update to better understand context while balancing the need for safety.\\nAmong Anand’s other key objectives: encouraging more creators to join the platform to make new chatbot characters and upgrading the social feed where users can share content they’ve created with Character.AI chatbots.\\nThe latter feature is similar to an app Meta launched this year that allows people to publicly share their prompts and AI-generated creations. Metadrew heatwhen apparently confused users shared conversations that contained embarrassing or personal details — a reminder of the privacy challenges that can come with AI tools.\\nBut the social element could help further differentiate Character.AI from bigger competitors like ChatGPT, which users are also increasinglyforming personal connections with.\\nAnother challenge Anand will face as CEO is retaining and growing the company’s workforce, as an AI talent war heats up across the tech industry. In a sign of the competition for top talent, Meta hasreportedly offeredpay packages and bonuses worth hundreds of millions of dollars to grow its new superintelligence team. Character.AI co-founder and former CEO Noam Shazeer was alsolured back to Googlelast year, where he’d previously built conversational AI technology.\\n“It is hard, I will not lie,” Anand said. “The good news for me as CEO is all the people we have here are very, very passionate and mission driven.”     None          CNN\n",
      "50  2025-01                   Will a dice-playing robot eventually make you tea and do your dishes?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        AlphaBot 2 wants to beat humans at their own game. When the robot is asked if it wants to play dice, it can interpret the question and jump into action – pressing the button on an automatic dice roller, which spins a die. It can even react to its opponent’s score with a thumbs up if they win.\\nThe humanoid, created by AI² Robotics, based in Shenzhen, China, displayed its skills at the recent Beyond Expo in the Chinese special administrative region of Macao, where it played the game with attendees of the tech conference, including CNN journalists.\\nThe robot’s ability to understand instructions was made possible by embodied artificial intelligence (AI) – the integration of AI systems into physical entities – allowing it to interact with and learn from the world around it.\\n“In the last era of robots, people needed to program them to tell them what to do,” Yandong Guo, CEO of AI² Robotics, told CNN correspondent Kristie Lu Stout on the sidelines of the conference. “Now you just tell them what to do, and the robot can understand the environment.”\\nGuo adds that it took the robot just minutes to learn to play. “We just show the robot what to do, maybe five to 10 samples, and the robot can learn.”\\nWhile AI chatbots like ChatGPT have become familiar, many experts say that embodied AI is the next big thing in the field.\\nCompanies across the world are developing humanoid robots with AI, including Tesla and California-based Figure AI, whose investors include major tech companies like Microsoft and Nvidia.\\nIn China, embodied AI is receiving serious national support, including funding,innovation centersand evena robot school. Shenzhen alone is home to more than 200 companies focusing on the tech, accordingto local media.\\nOfficials see the technology as a potential driver ofeconomic growth, and in recent years homegrown robots have gained attention for skills ranging fromdelivering roundhouse kickstorunning a half-marathon(though not very quickly).\\nToday, robots are already being used around the globe in industrial settings, like car manufacturing plants. Many robots are programmed to complete routine tasks, but things are shifting towards the use of embodied AI, saysHarry Yang, an assistant professor at The Hong Kong University of Science and Technology. “As tasks become more complex, you need robots to see and understand and act based on different situations,” he says.\\nAlphaBot 2 – which comes equipped with AI² Robotics’ self-developed embodied AI model – already has customers across industrial services, biotechnology, and public services, the company says.\\nAt a factory operated by carmaker Dongfeng Liuzhou Motor Co. it loads and unloads materials, tows carts, and attaches labels to windshields.\\nBut Guo hopes that one day, it can step out of the factory and into the home.\\nToday, most robots lack the technological prowess to be helpful in a household. Hong Kong-listed UBTech Robotics plans to unveil a $20,000 home companion robot this year,according to Bloomberg, but the company said the technology was years away from being able to help with household chores and to look after humans.\\nThat’s because it’s difficult to get enough training data to simulate the varying home environments people live in, experts say. ButMorgan Stanleyestimates that 80 million humanoids will be used in homes by 2050, as technology advances.\\nGuo is already envisioning how his robots might be able to help consumers. “If you want to drink some tea,” he says, “the robot can know where to fetch the teabag can know where to get hot water, and how to pour the hot water into the cup and make tea for you.”\\nThat’s not all. “After we eat, I hope our robot can clean up all the dishes for us. We love to cook, but we don’t like to clean things up.”\\nThe reality Guo imagines is still a way off. Prices will need to come down. AI² declined to share a price for its humanoids, saying its robots are tailored to customer requirements, so there’s no fixed price. Humanoids produced by some other companies in China costjust shy of $15,000and within five years, the price of an AI² humanoid could drop to the range of an entry-level car affordable to a middle-class family, says a spokesperson.\\n“The challenge here is that it’s very expensive to make one,” says Yang. “Maybe you’d rather hire someone (to work in your home), it’s cheaper and easier.”\\nSafety is another major concern; a robottipping overonto a human could cause injury. Experts have also raised concerns about the privacy risks of a home robot collecting people’s data through cameras and microphones.\\nGuo says that Chinese consumers have some anxiety about putting humanoid robots to use, and the company takes safety and privacy into consideration when it develops its products, but he adds: “You would be surprised to see there are so many customers in China willing to get robots.”\\nYang says that it will likely be about five to 10 years before humanoid robots can be truly useful in the home.\\nAI² Robotics says in the third quarter of 2025 its robots will be launched at airports in major Chinese cities for tasks like organizing luggage carts for passengers. In three to five years, they might be ready for senior facilities, he says.\\nThe robots will be learning along the way. “We need to get a lot of data for the robot to learn, to have this kind of common sense,” he says.\\nThat could help the company get closer to its goal. “Our dream is to get one robot for every family,” he says.     None          CNN\n",
      "\n",
      "============================================================\n",
      "✓ CNN.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Academia 합치기\n",
    "- DSS, EJIS, IAM, ICIS, ISJ, ISR, JAIS, JIT, JMIS, JSIS, MISQ 총 12개 저널과 컨퍼런스로 이루어짐\n",
    "- date, title, abstract, keywords, affiliations 칼럼으로 이루어짐\n",
    "- date는 값이 다 다를 수 있음\n",
    "- date 2023년 ~ 2025년도 인지 다시 한번 확인\n",
    "- Title, abstract을 기준으로 Null, 중복제거 재진행\n",
    "- 마지막에 value_counts()로 date, affiliations로 찍어보기"
   ],
   "id": "56389b8bb1e465a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:16:26.248128Z",
     "start_time": "2025-10-29T05:16:25.994288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "OUTPUT_NAME = \"Academia\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Academia/\"  # 각 저널 CSV가 있는 디렉토리\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "\n",
    "# 병합할 저널/컨퍼런스 목록\n",
    "JOURNALS = [\n",
    "    \"DSS\", \"EJIS\", \"IAM\", \"ICIS\", \"ISJ\", \"ISR\",\n",
    "    \"JAIS\", \"JIT\", \"JMIS\", \"JSIS\", \"MISQ\", \"HICSS\"\n",
    "]\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Academia 데이터 병합 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. 모든 저널 데이터 읽기 및 병합\n",
    "print(\"[1단계] 저널/컨퍼런스 데이터 읽기 및 병합\")\n",
    "\n",
    "all_dfs = []\n",
    "found_journals = []\n",
    "missing_journals = []\n",
    "\n",
    "for journal in JOURNALS:\n",
    "    file_path = os.path.join(DATA_DIR, f\"{journal}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dfs.append(df)\n",
    "            found_journals.append(journal)\n",
    "            print(f\"  ✓ {journal:10s} -> {len(df):6d}개 행\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {journal:10s} -> 읽기 오류: {e}\")\n",
    "            missing_journals.append(journal)\n",
    "    else:\n",
    "        print(f\"  ⚠ {journal:10s} -> 파일 없음\")\n",
    "        missing_journals.append(journal)\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"\\n❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n✓ 발견된 저널: {len(found_journals)}개 - {', '.join(found_journals)}\")\n",
    "if missing_journals:\n",
    "    print(f\"⚠ 누락된 저널: {len(missing_journals)}개 - {', '.join(missing_journals)}\")\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 필수 컬럼 확인\n",
    "print(f\"\\n[2단계] 필수 컬럼 확인\")\n",
    "\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        combined_df[col] = None\n",
    "    print(f\"  → 누락된 컬럼을 None으로 생성했습니다\")\n",
    "\n",
    "print(f\"  ✓ 컬럼: {list(combined_df.columns)}\")\n",
    "\n",
    "# 3. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[3단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(combined_df)\n",
    "combined_df['year'] = combined_df['date'].apply(extract_year)\n",
    "\n",
    "# 연도별 분포 확인\n",
    "year_distribution = combined_df['year'].value_counts().sort_index()\n",
    "print(f\"  [연도별 분포 (필터링 전)]\")\n",
    "for year, count in year_distribution.items():\n",
    "    status = \"✓\" if year in VALID_YEARS else \"✗\"\n",
    "    print(f\"    {status} {year}: {count}개\")\n",
    "\n",
    "# 유효한 연도만 필터링\n",
    "combined_df = combined_df[combined_df['year'].isin(VALID_YEARS)]\n",
    "combined_df = combined_df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(combined_df)\n",
    "\n",
    "print(f\"\\n  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 4. Null 값 제거 (title, abstract 기준)\n",
    "print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "before_null = len(combined_df)\n",
    "combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(combined_df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 5. 중복 제거 (title, abstract 기준)\n",
    "print(f\"\\n[5단계] 중복 제거 (title, abstract 기준)\")\n",
    "\n",
    "before_dup = len(combined_df)\n",
    "combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(combined_df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 6. 최종 컬럼 선택\n",
    "print(f\"\\n[6단계] 최종 컬럼 선택\")\n",
    "\n",
    "final_df = combined_df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"  - 최종 행 수: {len(final_df)}개\")\n",
    "\n",
    "# 7. 통계 확인\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[7단계] 데이터 통계\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# affiliations별 통계\n",
    "print(\"■ Affiliations별 통계:\")\n",
    "affiliations_stats = final_df['affiliations'].value_counts().sort_index()\n",
    "total_affiliations = len(final_df)\n",
    "for affiliation, count in affiliations_stats.items():\n",
    "    percentage = (count / total_affiliations) * 100\n",
    "    print(f\"  - {affiliation:10s}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  - {'합계':10s}: {total_affiliations:6d}개 (100.00%)\")\n",
    "\n",
    "# date별 통계 (연-월 기준)\n",
    "print(f\"\\n■ Date별 통계:\")\n",
    "\n",
    "def format_date_for_stats(date_str):\n",
    "    \"\"\"date를 YYYY-MM 형식으로 통일\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return 'Unknown'\n",
    "    date_str = str(date_str)\n",
    "    # YYYY-MM 추출\n",
    "    match = re.search(r'(\\d{4})-?(\\d{2})?', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2) if match.group(2) else '01'\n",
    "        return f\"{year}-{month}\"\n",
    "    return date_str\n",
    "\n",
    "final_df['date_formatted'] = final_df['date'].apply(format_date_for_stats)\n",
    "date_stats = final_df.groupby('date_formatted').size().sort_index()\n",
    "\n",
    "# 날짜가 너무 많으면 연도별로 집계\n",
    "if len(date_stats) > 36:  # 3년 * 12개월\n",
    "    print(f\"  총 {len(date_stats)}개의 날짜 (연도별 요약)\")\n",
    "\n",
    "    # 연도별 집계\n",
    "    year_stats = {}\n",
    "    for date, count in date_stats.items():\n",
    "        year = date[:4] if len(date) >= 4 else 'Unknown'\n",
    "        year_stats[year] = year_stats.get(year, 0) + count\n",
    "\n",
    "    for year in sorted(year_stats.keys()):\n",
    "        count = year_stats[year]\n",
    "        percentage = (count / total_affiliations) * 100\n",
    "        print(f\"    - {year}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "else:\n",
    "    # 전체 날짜 표시\n",
    "    for date, count in date_stats.items():\n",
    "        percentage = (count / total_affiliations) * 100\n",
    "        print(f\"  - {date}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "\n",
    "# date_formatted 임시 컬럼 제거\n",
    "final_df = final_df.drop('date_formatted', axis=1)\n",
    "\n",
    "# 8. 결과 저장\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[8단계] 결과 저장\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "output_filename = f\"{OUTPUT_NAME}.csv\"\n",
    "output_path = os.path.join(DATA_DIR, output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 저널 수: {len(found_journals)}개\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(5).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Academia.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "b15ab1826df36527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Academia 데이터 병합 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] 저널/컨퍼런스 데이터 읽기 및 병합\n",
      "  ✓ DSS        ->    212개 행\n",
      "  ✓ EJIS       ->    165개 행\n",
      "  ✓ IAM        ->    270개 행\n",
      "  ✓ ICIS       ->    835개 행\n",
      "  ✓ ISJ        ->    148개 행\n",
      "  ✓ ISR        ->     58개 행\n",
      "  ✓ JAIS       ->    166개 행\n",
      "  ✓ JIT        ->     50개 행\n",
      "  ✓ JMIS       ->    128개 행\n",
      "  ✓ JSIS       ->     65개 행\n",
      "  ✓ MISQ       ->    194개 행\n",
      "  ✓ HICSS      ->   2098개 행\n",
      "\n",
      "✓ 발견된 저널: 12개 - DSS, EJIS, IAM, ICIS, ISJ, ISR, JAIS, JIT, JMIS, JSIS, MISQ, HICSS\n",
      "\n",
      "✓ 병합 완료: 총 4389개 행 (12개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] 필수 컬럼 확인\n",
      "  ✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[3단계] 연도 필터링 (2023-2025)\n",
      "  [연도별 분포 (필터링 전)]\n",
      "    ✓ 2023.0: 1517개\n",
      "    ✓ 2024.0: 1698개\n",
      "    ✓ 2025.0: 1149개\n",
      "\n",
      "  - 필터링 전: 4389개 행\n",
      "  - 필터링 후: 4364개 행\n",
      "  - 제거됨: 25개 행\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4364개 행\n",
      "  - 제거 후: 4364개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[5단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 4364개 행\n",
      "  - 제거 후: 4363개 행\n",
      "  - 제거됨: 1개 행\n",
      "\n",
      "[6단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "  - 최종 행 수: 4363개\n",
      "\n",
      "============================================================\n",
      "[7단계] 데이터 통계\n",
      "============================================================\n",
      "\n",
      "■ Affiliations별 통계:\n",
      "  - DSS       :    212개 ( 4.86%)\n",
      "  - EJIS      :    165개 ( 3.78%)\n",
      "  - HICSS     :   2097개 (48.06%)\n",
      "  - IAM       :    270개 ( 6.19%)\n",
      "  - ICIS      :    835개 (19.14%)\n",
      "  - ISJ       :    123개 ( 2.82%)\n",
      "  - ISR       :     58개 ( 1.33%)\n",
      "  - JAIS      :    166개 ( 3.80%)\n",
      "  - JIT       :     50개 ( 1.15%)\n",
      "  - JMIS      :    128개 ( 2.93%)\n",
      "  - JSIS      :     65개 ( 1.49%)\n",
      "  - MISQ      :    194개 ( 4.45%)\n",
      "  ────────────────────────────────────────\n",
      "  - 합계        :   4363개 (100.00%)\n",
      "\n",
      "■ Date별 통계:\n",
      "  - 2023-01:   1156개 (26.50%)\n",
      "  - 2023-02:     26개 ( 0.60%)\n",
      "  - 2023-03:     46개 ( 1.05%)\n",
      "  - 2023-04:     28개 ( 0.64%)\n",
      "  - 2023-05:      6개 ( 0.14%)\n",
      "  - 2023-06:     64개 ( 1.47%)\n",
      "  - 2023-07:      7개 ( 0.16%)\n",
      "  - 2023-08:     30개 ( 0.69%)\n",
      "  - 2023-09:     44개 ( 1.01%)\n",
      "  - 2023-10:     32개 ( 0.73%)\n",
      "  - 2023-11:     10개 ( 0.23%)\n",
      "  - 2023-12:     68개 ( 1.56%)\n",
      "  - 2024-01:   1265개 (28.99%)\n",
      "  - 2024-02:     45개 ( 1.03%)\n",
      "  - 2024-03:     59개 ( 1.35%)\n",
      "  - 2024-04:     45개 ( 1.03%)\n",
      "  - 2024-05:     18개 ( 0.41%)\n",
      "  - 2024-06:     71개 ( 1.63%)\n",
      "  - 2024-07:      7개 ( 0.16%)\n",
      "  - 2024-08:     38개 ( 0.87%)\n",
      "  - 2024-09:     42개 ( 0.96%)\n",
      "  - 2024-10:     32개 ( 0.73%)\n",
      "  - 2024-12:     75개 ( 1.72%)\n",
      "  - 2025-01:    801개 (18.36%)\n",
      "  - 2025-02:     29개 ( 0.66%)\n",
      "  - 2025-03:     99개 ( 2.27%)\n",
      "  - 2025-04:     30개 ( 0.69%)\n",
      "  - 2025-06:     71개 ( 1.63%)\n",
      "  - 2025-08:     25개 ( 0.57%)\n",
      "  - 2025-09:     40개 ( 0.92%)\n",
      "  - 2025-10:     26개 ( 0.60%)\n",
      "  - 2025-12:     28개 ( 0.64%)\n",
      "\n",
      "============================================================\n",
      "[8단계] 결과 저장\n",
      "============================================================\n",
      "\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Academia/Academia.csv\n",
      "✓ 최종 행 수: 4363개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 저널 수: 12개\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                                                                                                                 title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract                                                                                                                                                        keywords affiliations\n",
      "0  2023-01                               Impact of content ideology on social media opinion polarization: The moderating role of functional affordances and symbolic expressions                                                                                                                                                                                                                                                                                                                 We offer theory and evidence regarding the impact of content ideology (i.e., emotionally charged beliefs expressed in sentiments) on opinion polarization (i.e., conflicting attitudes about an event) on social media. Specifically, we consider the moderating role of functional affordances and symbolic expressions to draw inferences about opinion polarization. From a sentiment analysis of 3600 posts and a survey of 468 Weibo users, we find that content ideology is positively related to social media opinion polarization. The effect of content ideology is greater when users receive stronger symbolic expressions. Further, our results show an insignificant moderating relationship between functional affordances and this effect. The findings suggest that it is critical to consider content ideology and symbolic expressions when assessing the relationship between published content and polarized opinions on social media.                                                    Social media, Opinion polarization, Sentiment analysis, Ideology, Functional affordance, Symbolic expression          DSS\n",
      "1  2023-01                                                                                                  A novel label-based multimodal topic model for social media analysis  Extracting useful knowledge from multimodal data is the core of manymultimedia applications, such as recommendation systems, and cross-modal retrieval. In this paper, we propose a label-based multimodal topic (LB-MMT) model to jointly model text andimage datatagged with multiple labels. Specifically, we use the labels assupervised informationto generate the text and image data. In the LB-MMT model, we assume that the textual words and visual words related to each text and image are drawn from a mixture of latent topics, where each topic is represented as a group of textual words and visual words. Moreover, we introduce multiple topics for each label, to build the top-down relationship from label to text and image. To investigate the effectiveness of the proposed approach, we conduct extensive experiments on a real-world multimodal dataset with labels. The results show the proposed approach obtains superior performances on topic coherence and label prediction compared with previous competitors. In addition, we show that our model yields interesting insights about multimodal topics. The proposed model provides important practical implications, e.g., designing more attractive multimodal contents formarketers.                                                                             Multimodal data, Topic modeling, Label data, Supervised model, Image representation          DSS\n",
      "2  2023-01                                                                   CATCHM: A novel network-based credit card fraud detection method using node representation learning                                                                                                                                                                              Advanced fraud detection systems leverage the digital traces from (credit-card) transactions to detect fraudulent activity in future transactions. Recent research in fraud detection has focused primarily ondata analyticscombined with manual feature engineering, which is tedious, expensive and requires considerable domain expertise. Furthermore, transactions are often examined in isolation, disregarding the interconnection that exists between them.In this paper, we proposeCATCHM, a novel network-based credit card fraud detection method based onrepresentation learning(RL). Through innovative network design, an efficient inductive pooling operator, and careful downstream classifier configuration, we show hownetwork RLcan benefit fraud detection by avoiding manual feature engineering and explicitly considering therelational structureof transactions. Extensive empirical evaluation on a real-life credit card dataset shows thatCATCHMoutperforms state-of-the-art methods, thereby illustrating the practical relevance of this approach for industry.                                                                                   Network representation learning, DeepWalk, Credit card fraud, Fraud detection          DSS\n",
      "3  2023-01  Exploring the effects of relationship quality and c-commerce behavior on firms' dynamic capability and c-commerce performance in the supply chain management context                                                                                                                      Although previous studies indicate the critical role of collaborative commerce (c-commerce) adoption and dynamic capability in thesupply chainprocess, they have not addressed the relationship between c-commerce behavior and dynamic capability. By adopting the commitment-trust theory and the dynamic capability view, this study empirically examines the effects of relationship quality, c-commerce behavior, and dynamic capability on c-commerce performance. Survey data collected from 257 professionals in variousmanufacturing industrieswere analyzed using the partial least squares technique. The findings suggest that c-commerce behavior and dynamic capability are positively associated with c-commerce performance. However, negative calculative commitment does not positively affect c-commerce behavior. Moreover, institution-based trust does not negatively affect negative calculative commitment. This study is one of few studies investigating the relationships among the variables that contribute to c-commerce performance. The findings can serve as references for practitioners in supply chain practices.           Relationship commitment, Institution-based trust, Collaborative commerce behavior, Dynamic capability, C-commerce performance, Collaborative learning          DSS\n",
      "4  2023-01                                                                                                                 Pay-for-performance schemes and hospital HIT adoption                                                                                                                                                                                                           Pay-for-performance (P4P) schemes are implemented to incentivize or penalize hospitals for their safe caregiving. Given thathealth information technology(HIT) results in better healthcare outcomes, P4P schemes are expected to promote hospital HIT adoption. However, P4P schemes could also discourage hospitals from adopting HIT because they may take away resources initially allocated for HIT adoption. This paper is one of the first to empirically investigate the double-edged role of P4P schemes in HIT adoption. We leverage a natural experiment in the US healthcare system, which introduced P4P schemes in 2013. Our empirical analysis reveals an unintendedside effectof P4P schemes that could potentially impede HIT adoption. We also find that P4P schemes have a negativespillover effecton nonparticipating hospitals in the same multihospital system (MHS) as participating hospitals, particularly for small nonparticipating hospitals. Our findings provide important implications forhealth policydesign and MHS management.  Health information technology, Spillover effect, Pay-for-performance programs, Hospital value-based purchasing program, Hospital readmission reduction program          DSS\n",
      "\n",
      "============================================================\n",
      "✓ Academia.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:22:29.851629Z",
     "start_time": "2025-10-29T05:22:29.766296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv('/home/dslab/choi/Journal/Data/Academia/Academia.csv')\n",
    "df.info()"
   ],
   "id": "7a8ec9d99a5e77b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4363 entries, 0 to 4362\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          4363 non-null   object\n",
      " 1   title         4363 non-null   object\n",
      " 2   abstract      4363 non-null   object\n",
      " 3   keywords      4153 non-null   object\n",
      " 4   affiliations  4363 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Industry 합치기\n",
    "- BBC, CNN, NewYorkTimes, TechCrunch, TheGuardian, TheVerge, WallStreetJournal 총 7개 media로 이루어짐\n",
    "- date, title, abstract, keywords, affiliations 칼럼으로 이루어짐\n",
    "- date는 값이 다 다를 수 있음\n",
    "- date 2023년 ~ 2025년도 인지 다시 한번 확인\n",
    "- title, abstract을 기준으로 Null, 중복제거 재진행\n",
    "- 마지막에 value_counts()로 date, affiliations로 찍어보기"
   ],
   "id": "616539a80ea466ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T06:09:38.397776Z",
     "start_time": "2025-10-29T06:09:37.574642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ========================================\n",
    "# 설정\n",
    "# ========================================\n",
    "OUTPUT_NAME = \"Industry\"\n",
    "DATA_DIR = \"/home/dslab/choi/Journal/Data/Industry/\"  # 각 미디어 CSV가 있는 디렉토리\n",
    "VALID_YEARS = [2023, 2024, 2025]  # 유효한 연도\n",
    "\n",
    "# 병합할 미디어 목록\n",
    "MEDIA_LIST = [\n",
    "    \"NewYorkTimes\", \"TechCrunch\",\n",
    "    \"TheGuardian\", \"TheVerge\", \"WallStreetJournal\"\n",
    "]\n",
    "# ========================================\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Industry 데이터 병합 시작\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 1. 모든 미디어 데이터 읽기 및 병합\n",
    "print(\"[1단계] 미디어 데이터 읽기 및 병합\")\n",
    "\n",
    "all_dfs = []\n",
    "found_media = []\n",
    "missing_media = []\n",
    "\n",
    "for media in MEDIA_LIST:\n",
    "    file_path = os.path.join(DATA_DIR, f\"{media}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dfs.append(df)\n",
    "            found_media.append(media)\n",
    "            print(f\"  ✓ {media:20s} -> {len(df):6d}개 행\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {media:20s} -> 읽기 오류: {e}\")\n",
    "            missing_media.append(media)\n",
    "    else:\n",
    "        print(f\"  ⚠ {media:20s} -> 파일 없음\")\n",
    "        missing_media.append(media)\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"\\n❌ 읽을 수 있는 파일이 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n✓ 발견된 미디어: {len(found_media)}개 - {', '.join(found_media)}\")\n",
    "if missing_media:\n",
    "    print(f\"⚠ 누락된 미디어: {len(missing_media)}개 - {', '.join(missing_media)}\")\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n✓ 병합 완료: 총 {len(combined_df)}개 행 ({len(all_dfs)}개 파일)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# 2. 필수 컬럼 확인\n",
    "print(f\"\\n[2단계] 필수 컬럼 확인\")\n",
    "\n",
    "required_columns = ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
    "available_columns = [col for col in required_columns if col in combined_df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"  ⚠ 누락된 컬럼: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        combined_df[col] = None\n",
    "    print(f\"  → 누락된 컬럼을 None으로 생성했습니다\")\n",
    "\n",
    "print(f\"  ✓ 컬럼: {list(combined_df.columns)}\")\n",
    "\n",
    "# 3. 연도 필터링 (2023-2025)\n",
    "print(f\"\\n[3단계] 연도 필터링 (2023-2025)\")\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"date에서 연도 추출\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    date_str = str(date_str)\n",
    "    match = re.search(r'(\\d{4})', date_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "before_year_filter = len(combined_df)\n",
    "combined_df['year'] = combined_df['date'].apply(extract_year)\n",
    "\n",
    "# 연도별 분포 확인\n",
    "year_distribution = combined_df['year'].value_counts().sort_index()\n",
    "print(f\"  [연도별 분포 (필터링 전)]\")\n",
    "for year, count in year_distribution.items():\n",
    "    status = \"✓\" if year in VALID_YEARS else \"✗\"\n",
    "    print(f\"    {status} {year}: {count}개\")\n",
    "\n",
    "# 유효한 연도만 필터링\n",
    "combined_df = combined_df[combined_df['year'].isin(VALID_YEARS)]\n",
    "combined_df = combined_df.drop('year', axis=1)  # 임시 컬럼 제거\n",
    "after_year_filter = len(combined_df)\n",
    "\n",
    "print(f\"\\n  - 필터링 전: {before_year_filter}개 행\")\n",
    "print(f\"  - 필터링 후: {after_year_filter}개 행\")\n",
    "print(f\"  - 제거됨: {before_year_filter - after_year_filter}개 행\")\n",
    "\n",
    "# 4. Null 값 제거 (title, abstract 기준)\n",
    "print(f\"\\n[4단계] Null 값 제거 (title, abstract 기준)\")\n",
    "\n",
    "before_null = len(combined_df)\n",
    "combined_df = combined_df.dropna(subset=['title', 'abstract'])\n",
    "after_null = len(combined_df)\n",
    "print(f\"  - 제거 전: {before_null}개 행\")\n",
    "print(f\"  - 제거 후: {after_null}개 행\")\n",
    "print(f\"  - 제거됨: {before_null - after_null}개 행\")\n",
    "\n",
    "# 5. 중복 제거 (title, abstract 기준)\n",
    "print(f\"\\n[5단계] 중복 제거 (title, abstract 기준)\")\n",
    "\n",
    "before_dup = len(combined_df)\n",
    "combined_df = combined_df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "after_dup = len(combined_df)\n",
    "print(f\"  - 제거 전: {before_dup}개 행\")\n",
    "print(f\"  - 제거 후: {after_dup}개 행\")\n",
    "print(f\"  - 제거됨: {before_dup - after_dup}개 행\")\n",
    "\n",
    "# 6. 최종 컬럼 선택\n",
    "print(f\"\\n[6단계] 최종 컬럼 선택\")\n",
    "\n",
    "final_df = combined_df[required_columns].copy()\n",
    "print(f\"  - 최종 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"  - 최종 행 수: {len(final_df)}개\")\n",
    "\n",
    "# 7. 통계 확인\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[7단계] 데이터 통계\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# affiliations별 통계\n",
    "print(\"■ Affiliations별 통계:\")\n",
    "affiliations_stats = final_df['affiliations'].value_counts().sort_index()\n",
    "total_affiliations = len(final_df)\n",
    "for affiliation, count in affiliations_stats.items():\n",
    "    percentage = (count / total_affiliations) * 100\n",
    "    print(f\"  - {affiliation:20s}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "print(f\"  {'─'*50}\")\n",
    "print(f\"  - {'합계':20s}: {total_affiliations:6d}개 (100.00%)\")\n",
    "\n",
    "# date별 통계 (연-월 기준)\n",
    "print(f\"\\n■ Date별 통계:\")\n",
    "\n",
    "def format_date_for_stats(date_str):\n",
    "    \"\"\"date를 YYYY-MM 형식으로 통일\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return 'Unknown'\n",
    "    date_str = str(date_str)\n",
    "    # YYYY-MM 추출\n",
    "    match = re.search(r'(\\d{4})-?(\\d{2})?', date_str)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2) if match.group(2) else '01'\n",
    "        return f\"{year}-{month}\"\n",
    "    return date_str\n",
    "\n",
    "final_df['date_formatted'] = final_df['date'].apply(format_date_for_stats)\n",
    "date_stats = final_df.groupby('date_formatted').size().sort_index()\n",
    "\n",
    "# 날짜가 너무 많으면 연도별로 집계\n",
    "if len(date_stats) > 36:  # 3년 * 12개월\n",
    "    print(f\"  총 {len(date_stats)}개의 날짜 (연도별 요약)\")\n",
    "\n",
    "    # 연도별 집계\n",
    "    year_stats = {}\n",
    "    for date, count in date_stats.items():\n",
    "        year = date[:4] if len(date) >= 4 else 'Unknown'\n",
    "        year_stats[year] = year_stats.get(year, 0) + count\n",
    "\n",
    "    for year in sorted(year_stats.keys()):\n",
    "        count = year_stats[year]\n",
    "        percentage = (count / total_affiliations) * 100\n",
    "        print(f\"    - {year}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "else:\n",
    "    # 전체 날짜 표시\n",
    "    for date, count in date_stats.items():\n",
    "        percentage = (count / total_affiliations) * 100\n",
    "        print(f\"  - {date}: {count:6d}개 ({percentage:5.2f}%)\")\n",
    "\n",
    "# date_formatted 임시 컬럼 제거\n",
    "final_df = final_df.drop('date_formatted', axis=1)\n",
    "\n",
    "# 8. 결과 저장\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[8단계] 결과 저장\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "output_filename = f\"{OUTPUT_NAME}.csv\"\n",
    "output_path = os.path.join(DATA_DIR, output_filename)\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✓ 최종 저장 완료!\")\n",
    "print(f\"✓ 파일 경로: {output_path}\")\n",
    "print(f\"✓ 최종 행 수: {len(final_df)}개\")\n",
    "print(f\"✓ 컬럼: {list(final_df.columns)}\")\n",
    "print(f\"✓ 미디어 수: {len(found_media)}개\")\n",
    "print(f\"✓ 연도 범위: {min(VALID_YEARS)} - {max(VALID_YEARS)}\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n[데이터 미리보기]\")\n",
    "if len(final_df) > 0:\n",
    "    print(final_df.head(5).to_string())\n",
    "else:\n",
    "    print(\"  - 데이터가 없습니다.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Industry.csv 생성 완료!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "id": "5f699767a7ac2932",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Industry 데이터 병합 시작\n",
      "============================================================\n",
      "\n",
      "[1단계] 미디어 데이터 읽기 및 병합\n",
      "  ✓ NewYorkTimes         ->    900개 행\n",
      "  ✓ TechCrunch           ->    715개 행\n",
      "  ✓ TheGuardian          ->   1986개 행\n",
      "  ✓ TheVerge             ->   4584개 행\n",
      "  ✓ WallStreetJournal    ->    955개 행\n",
      "\n",
      "✓ 발견된 미디어: 5개 - NewYorkTimes, TechCrunch, TheGuardian, TheVerge, WallStreetJournal\n",
      "\n",
      "✓ 병합 완료: 총 9140개 행 (5개 파일)\n",
      "\n",
      "============================================================\n",
      "\n",
      "[2단계] 필수 컬럼 확인\n",
      "  ✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "\n",
      "[3단계] 연도 필터링 (2023-2025)\n",
      "  [연도별 분포 (필터링 전)]\n",
      "    ✓ 2023: 2318개\n",
      "    ✓ 2024: 3006개\n",
      "    ✓ 2025: 3816개\n",
      "\n",
      "  - 필터링 전: 9140개 행\n",
      "  - 필터링 후: 9140개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[4단계] Null 값 제거 (title, abstract 기준)\n",
      "  - 제거 전: 9140개 행\n",
      "  - 제거 후: 9140개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[5단계] 중복 제거 (title, abstract 기준)\n",
      "  - 제거 전: 9140개 행\n",
      "  - 제거 후: 9140개 행\n",
      "  - 제거됨: 0개 행\n",
      "\n",
      "[6단계] 최종 컬럼 선택\n",
      "  - 최종 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "  - 최종 행 수: 9140개\n",
      "\n",
      "============================================================\n",
      "[7단계] 데이터 통계\n",
      "============================================================\n",
      "\n",
      "■ Affiliations별 통계:\n",
      "  - New York Times      :    900개 ( 9.85%)\n",
      "  - TechCrunch          :    715개 ( 7.82%)\n",
      "  - TheGuardian         :   1986개 (21.73%)\n",
      "  - TheVerge            :   4584개 (50.15%)\n",
      "  - Wall Street Journal :    955개 (10.45%)\n",
      "  ──────────────────────────────────────────────────\n",
      "  - 합계                  :   9140개 (100.00%)\n",
      "\n",
      "■ Date별 통계:\n",
      "  - 2023-01:     93개 ( 1.02%)\n",
      "  - 2023-02:    158개 ( 1.73%)\n",
      "  - 2023-03:    178개 ( 1.95%)\n",
      "  - 2023-04:    168개 ( 1.84%)\n",
      "  - 2023-05:    260개 ( 2.84%)\n",
      "  - 2023-06:    178개 ( 1.95%)\n",
      "  - 2023-07:    194개 ( 2.12%)\n",
      "  - 2023-08:    166개 ( 1.82%)\n",
      "  - 2023-09:    197개 ( 2.16%)\n",
      "  - 2023-10:    205개 ( 2.24%)\n",
      "  - 2023-11:    328개 ( 3.59%)\n",
      "  - 2023-12:    193개 ( 2.11%)\n",
      "  - 2024-01:    261개 ( 2.86%)\n",
      "  - 2024-02:    237개 ( 2.59%)\n",
      "  - 2024-03:    252개 ( 2.76%)\n",
      "  - 2024-04:    224개 ( 2.45%)\n",
      "  - 2024-05:    345개 ( 3.77%)\n",
      "  - 2024-06:    220개 ( 2.41%)\n",
      "  - 2024-07:    219개 ( 2.40%)\n",
      "  - 2024-08:    289개 ( 3.16%)\n",
      "  - 2024-09:    238개 ( 2.60%)\n",
      "  - 2024-10:    268개 ( 2.93%)\n",
      "  - 2024-11:    208개 ( 2.28%)\n",
      "  - 2024-12:    245개 ( 2.68%)\n",
      "  - 2025-01:    308개 ( 3.37%)\n",
      "  - 2025-02:    275개 ( 3.01%)\n",
      "  - 2025-03:    268개 ( 2.93%)\n",
      "  - 2025-04:    408개 ( 4.46%)\n",
      "  - 2025-05:    473개 ( 5.18%)\n",
      "  - 2025-06:    420개 ( 4.60%)\n",
      "  - 2025-07:    529개 ( 5.79%)\n",
      "  - 2025-08:    407개 ( 4.45%)\n",
      "  - 2025-09:    439개 ( 4.80%)\n",
      "  - 2025-10:    289개 ( 3.16%)\n",
      "\n",
      "============================================================\n",
      "[8단계] 결과 저장\n",
      "============================================================\n",
      "\n",
      "✓ 최종 저장 완료!\n",
      "✓ 파일 경로: /home/dslab/choi/Journal/Data/Industry/Industry.csv\n",
      "✓ 최종 행 수: 9140개\n",
      "✓ 컬럼: ['date', 'title', 'abstract', 'keywords', 'affiliations']\n",
      "✓ 미디어 수: 5개\n",
      "✓ 연도 범위: 2023 - 2025\n",
      "\n",
      "[데이터 미리보기]\n",
      "      date                                                                      title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract keywords    affiliations\n",
      "0  2025-08       Bringing Art Back to Life WithArtificialIntelligence: [Foreign Desk]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Alex Kachkine spends his days working on microchip research -- a skill set surprisingly similar to that needed for restoration.\\nIn 2016, an earthquake reduced the Church of San Salvatore in Campi in central Italy mostly to rubble, destroying its 15th-century frescoes. What remained were fragments, a complex puzzle that conservators at the Ministry of Culture have been working to reassemble ever since.\\nBut the pieces are few and far between, ''isolated islands within vast areas of loss,'' Serena Di Gaetano and Federica Giacomini, senior conservator-restorers at the ministry's restoration institute, said in a joint emailed response to questions. Their work, they said, is to somehow ''bridge the gaps between what remains and what has been irretrievably lost.''\\nEnter Alex Kachkine, 25, a graduate researcher in mechanical engineering at M.I.T. who improbably rocked the art world in June with a paper in the scientific journal Nature describing a new way to restore paintings with the help of artificial intelligence.\\nA hobbyist restorer, he wrote a program that analyzes damage and prints the fixes on a super thin mask. The mask can be laid over the painting, making it appear fully restored, but can also be removed to reveal the original. To create the mask, the program used over 55,000 hues in several hours, and worked about 65 times as fast as traditional restoration, Mr. Kachkine estimated.\\nThe study was a side project, but it generated buzz among conservators around the world, including at the Ministry of Culture in Italy -- to Mr. Kachkine's surprise.\\nAt his day job, he researches the electron beam sources that create the intricate circuitry on microchips, which are then placed into phones or other devices. ''Those require very high degrees of precision,'' he said. ''And it turns out a lot of the techniques we use to achieve that level of precision are applicable to art restoration.''\\nThe Conservator's Dilemma\\nConservators have long debated just how heavy-handed to be about keeping works in viewing shape.\\n''Conservation is divided between preservation and restoration,'' said Ann Shaftel, a conservator in Canada who specializes in preserving thangkas, Tibetan Buddhist fabric paintings used to guide meditation. In her work, the seeming grime that accumulates as a thangka hangs in an incense-filled monastery has spiritual value, and she favors a very light touch when it comes to preservation.\\nA similar perspective is increasingly shared by conservators of art of all kinds, though there is no universally accepted approach. ''There is a dilemma about retouching a painting or not,'' said Hartmut Kutzke, a chemist at the Museum of Cultural History at the University of Oslo. ''It differs a little bit from country to country.''\\nSome favor maximalist interventions that restore a work to its former glory. Many prefer a minimally invasive approach that avoids irreversible changes. Mr. Kachkine's method could resolve the tensions between the two camps, according to Mr. Kutzke, who wrote an article reviewing the study.\\nSince the digitally printed mask can create a removable full restoration, it is ''a good bridge between the two worlds,'' Mr. Kutzke said in an interview.\\nBut the chemist noted that some of his colleagues who are conservators had nonetheless objected to the method. ''They were actually quite skeptical because they said it's too much change,'' he said. ''They were saying we have to accept that artworks age, and this is part of the artwork.''\\nCoding for Culture\\nIn Italy, official conservators follow strict principles. Their interventions must be reversible and visible so as not to permanently alter originals or mislead future observers. But Ms. Di Gaetano and Ms. Giacomini say the ministry's Central Institute for Restoration, known as I.C.R., also embraces cutting-edge technologies that can help preserve the past, like Mr. Kachkine's.\\nRelying on A.I. for analytic help is not new. But Mr. Kachkine's work caught the team's eye because it goes a step further, they said. ''It demonstrates how artificial intelligence algorithms, when correctly oriented and developed, can become concrete support tools not only in the study and documentation phase, but also during actual restoration operations,'' Ms. Di Gaetano and Ms. Giacomini said.\\nOnce the super thin mask is created, it's attached to the original artwork with conservation varnish. ''The mask can be peeled off with very minimal force, but the bond is there, and the bond is transparent, so it doesn't affect the way that you see the underlying painting,'' Mr. Kachkine said.\\nFor conservators facing big-scale projects that defy traditional methods, like the frescoes of the Church of San Salvatore, this innovation offers hope. With the help of a program -- and a printed screen -- filling in the missing pieces of a huge puzzle could become less daunting. And ultimately, it could mean more cultural heritage is saved.\\nThe restoration institute enlisted Mr. Kachkine seeking to develop a systematic approach to big projects that can be adapted to specific cases.\\nNow he is writing code to conform with Italy's official conservation principles. The fixes must be done in a painting style called tratteggio, developed by the restoration institute's founding director, where restored sections are painted in thin parallel lines to ensure additions are visibly distinct up-close but from afar allow the work to be appreciated as a whole.\\n''There's going to be a lot more cases like this where the technique needs to be adapted to a particular circumstance,'' Mr. Kachkine said.\\nFrom Innovative Technology to Usable Tool\\nMr. Kachkine doesn't plan to patent his method. Instead, by publishing his work, he hopes that conservators will be able to ''leverage the benefits'' of the techniques he gleaned from engineering to preserve ''really valuable cultural heritage.''\\nStill, his approach is currently too complex for most conservators to use independently. ''Many conservators have lamented to me that even though my methods are published and free to use, they would like to be able to buy masks without worrying about all the steps involved in fabricating them,'' he said.\\nHe is considering starting a business to develop the tool for commercial purposes, but for now is more focused on improving the technology.\\nLast month, he gave a talk at the de Young Museum in San Francisco and came back with additional programming tasks based on challenges faced by the conservators and curators he met there.\\nExperts say that the work has only just begun if he is to build a truly transformative tool.\\nMarc Melich-Mautner, an art and antiquities dealer in Austria, believes the method could revolutionize restoration, making it faster and less expensive, eventually making more art accessible. But accomplishing that goal will also require much more continued input from art professionals, in addition to tech development.\\n''It's still a lot of Alex behind it,'' he said of the program. ''It has to be fed with information from the restoration world, with the techniques.''      NaN  New York Times\n",
      "1  2025-07                              Trump Plans to Give AI Developers a Free Hand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  WASHINGTON — President Donald Trump said Wednesday that he planned to speed the advance of artificial intelligence in the United States, opening the door for companies to develop the technology unfettered from oversight and safeguards, but added that AI needed to be free of “partisan bias.”\\nIn a sweeping effort to put his stamp on the policies governing the fast-growing technology, Trump signed three executive orders and outlined an “AI Action Plan,” with measures to “remove red tape and onerous regulation” as well as to make it easier for companies to build infrastructure to power AI.\\nOne executive order banned the federal government from buying AI tools it considered ideologically biased. Another order would speed up the permitting process for major AI infrastructure projects, and a third focused on promoting the export of American AI products around the world.\\n“America is the country that started the AI race,” Trump said in a Wednesday evening speech in front of administration officials and tech executives, including Jensen Huang, CEO of chipmaker Nvidia. “And as president of the United States, I’m here today to declare that America is going to win it.”\\nTrump’s plan signals that his administration has embraced AI and the tech industry’s arguments that it must be allowed to work with few guardrails for the United States to dominate a new era defined by the technology. It is a forceful repudiation of other governments, including the European Commission, that have approved regulations to govern the development of AI.\\nBut it also points to how the administration wants to shape the way AI tools present information. Conservatives have accused some tech companies of developing AI models with a baked-in liberal bias. Most AI models are already trained on copious amounts of data from across the web, which informs their responses, making any shift in focus difficult.\\nThe changes outlined Wednesday would benefit tech giants locked in a fierce contest to produce generative AI products and persuade consumers to weave the tools into their daily lives. Since OpenAI’s public release of ChatGPT in late 2022, tech companies have raced to produce their own versions of the technology, which can write humanlike texts and produce realistic images and videos.\\nGoogle, Microsoft, Meta, OpenAI and others are jockeying for access to computing power, typically from huge data centers filled with computers that can stress local resources. And the companies are facing increased competition from rivals such as Chinese startup DeepSeek, which sent shock waves around the world this year after it created a powerful AI model with far less money than many thought possible.\\nThe fight over resources in Silicon Valley has run alongside an equally charged debate in Washington over how to confront the societal transformations that AI could bring. Critics worry that if left unchecked, the technology could be a potent tool for scammers and extremists and lay waste to the economy as more jobs are automated. News outlets and artists have sued AI companies over claims that they illegally trained their technology using copyrighted works and articles.\\nTrump previously warned of China’s potential to outpace American progress on the technology. He has said that the federal government must support AI companies with tax incentives, more foreign investment and less focus on safety regulations that could hamper progress.\\nFormer President Joe Biden took one major action on artificial intelligence: a 2023 executive order that mandated safety and security standards for the development and use of AI across the federal government.\\nBut hours after his inauguration in January, Trump rolled back that order. Days later, he signed another executive order, “Removing Barriers to American Leadership in Artificial Intelligence,” which called for an acceleration of AI development by U.S. tech companies and for versions of the technology that operated without ideological bias.\\nThe order included a mandate for administration officials to come up with “an artificial intelligence action plan,” with policy guidelines to encourage the growth of the AI industry. The administration solicited comments from companies while it considered its plan.\\nOpenAI called for the administration to expand its list of countries eligible to import AI technologies from the United States, a list that has been limited by controls designed to stop China from gaining access to American technology. OpenAI and Google called for greater support in building AI data centers through tax breaks and fewer barriers for foreign investment.\\nLast week, Nvidia said the administration had approved the sale of one of its chips, offered specifically in China, that had previously been banned.\\nThe plan released Wednesday outlined a wide range of policy shifts, divided into moves that the administration said would speed up the development of AI, make it easier to build and power data centers and promote the interests of American companies abroad.\\nThe federal government should impose fewer environmental regulations on the construction of new data centers and support training programs for workers needed to staff the facilities, the plan said. One of the executive orders signed Wednesday would establish a fast-tracked permitting process for data centers.\\nThe plan also called for the government to collect information on regulations that could hinder AI innovation and “take appropriate action.” It also said it would withhold funding for AI-related projects if a state’s regulations made deploying that funding less effective.\\nThe plan — and one of the executive orders — also calls for the government to give federal contracts to AI companies that “ensure that their systems are objective.” It said a government agency should revise guidelines for AI’s development to remove mentions of diversity, equity and inclusion; climate change; and misinformation.\\n“Once and for all, we are getting rid of woke,” Trump said in his speech. “The American people do not want woke Marxist lunacy in the AI models.”\\nThe power of the federal government should be used to ensure that AI systems are built “with freedom of speech and expression in mind,” according to the plan. That echoes long-standing conservative claims that products produced by tech companies, including online platforms like Facebook and YouTube, favor left-leaning perspectives.\\nThe government should encourage the export of American AI tools, the plan said, and called for federal agencies to help the industry sell packages of AI products abroad and work to counter China’s influence over the technology.\\nDuring his speech, Trump conducted a roll call of administration appointees and executives in attendance, including his chief of staff, Susie Wiles, and the CEO of chipmaker AMD, Lisa Su. He thanked David Sacks, his AI and crypto czar, and Sacks’ co-hosts on “All-In,” a popular podcast in Silicon Valley that helped to put on Wednesday’s event. And Trump repeatedly praised Nvidia’s Huang, who was seated in front of the stage.\\nThe president also touched on tariff negotiations, transgender athletes and the recent passage of his domestic policy bill.\\n“We’re going to make this industry absolutely the top because right now it’s a beautiful baby that’s born,” he said about AI. “We have to grow that baby and let that baby thrive. We can’t stop it.”\\nWhen he was done speaking, Trump moved from the lectern to the side of the stage and a small desk, holding each executive order up after signing it. He handed the pen he used to sign one of the orders to Sacks. After wishing the crowd luck, Trump left the stage.\\nThis article originally appeared in The New York Times.      NaN  New York Times\n",
      "2  2025-07               Trump Administration Plans to Give AI Developers a Free Hand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   WASHINGTON — The Trump administration said Wednesday that it planned to speed the development of artificial intelligence in the United States, opening the door for companies to develop the technology unfettered from oversight and safeguards, but added that the AI needed to be free of “ideological bias.”\\nIn a sweeping effort to put his stamp on the policies governing the fast-growing technology, President Donald Trump’s AI Action Plan outlines measures to “remove red tape and onerous regulation,” as well as make it easier for companies to build infrastructure to power AI.\\nThe plan also calls for the government to give federal contracts to companies that “ensure that their systems are objective.” It said a government agency should revise guidelines for AI’s development to remove mentions of diversity, equity and inclusion, climate change and misinformation.\\nThe report signals that the Trump administration has embraced AI and the tech industry’s arguments that it must be allowed to work with few guardrails for the United States to dominate a new era defined by the technology. It is a forceful repudiation of other governments, including the European Commission, that have approved regulations to govern the development of the technology.\\nBut it also points to how the administration wants to shape the way AI tools present information. Conservatives have accused some tech companies of developing AI models with a baked-in liberal bias. Most AI models are already trained on copious amounts of data from across the web, which informs their responses, making any shift in focus difficult.\\nOn Wednesday afternoon, Trump is scheduled to deliver his first major speech on AI, a technology that experts have said could upend communications, geopolitics and the economy in the coming years. The president is also expected to sign executive orders related to the technology.\\n“We believe we’re in an AI race,” David Sacks, the White House AI and crypto czar, said on a call with reporters. “And we want the United States to win that race.”\\nThe changes outlined Wednesday would benefit tech giants locked in a fierce contest to produce generative AI products and persuade consumers to weave the tools into their daily lives. Since OpenAI’s public release of ChatGPT in late 2022, tech companies have raced to produce their own versions of the technology, which can write humanlike texts and produce realistic images and videos.\\nGoogle, Microsoft, Meta, OpenAI and others are jockeying for access to computing power, typically from huge data centers filled with computers that can stress local communities’ resources. And the companies are facing increased competition from rivals such as Chinese startup DeepSeek, which sent shock waves around the world this year after it created a powerful AI model with far less money than many thought possible.\\nThe fight over resources in Silicon Valley has run alongside an equally charged debate in Washington over how to confront the societal transformations that AI could bring. Critics worry that if left unchecked, the technology could be a potent tool for scammers and extremists and lay waste to the economy as more jobs are automated. News outlets and artists have sued AI companies over claims that they illegally trained their technology using copyrighted works and articles.\\nTrump previously warned of China’s potential to outpace American progress on the technology. He has said that the federal government must support AI companies with tax incentives, more foreign investment and less focus on safety regulations that could hamper progress.\\nPresident Joe Biden took one major action on artificial intelligence: a 2023 executive order that mandated safety and security standards for the development and use of AI across the federal government.\\nBut hours after his inauguration in January, Trump rolled back that order. Days later, he signed another executive order, “Removing Barriers to American Leadership in Artificial Intelligence,” which called for an acceleration of AI development by U.S. tech companies and for versions of the technology that operated without ideological bias.\\nThe order included a mandate for administration officials to come up with “an artificial intelligence action plan,” with policy guidelines to encourage the growth of the AI industry. The administration solicited comments from companies while it considered its plan.\\nOpenAI called for the administration to expand its list of countries eligible to import AI technologies from the United States, a list that has been limited by controls designed to stop China from gaining access to American technology. OpenAI and Google called for greater support in building AI data centers through tax breaks and fewer barriers for foreign investment.\\nOpenAI, Google and Meta also said they believed they had legal access to copyrighted works like books, films and art for training their AI. Meta asked the White House to issue an executive order or other action to “clarify that the use of publicly available data to train models is unequivocally fair use.”\\nThe plan released Wednesday did not include mentions of copyright law. But it did outline a wide range of policy shifts, divided into moves that the administration said would speed up the development of AI, make it easier to build and power data centers and promote the interests of American companies abroad.\\nThe federal government should impose fewer environmental regulations on the construction of new data centers and support training programs for workers needed to staff the facilities, the plan said.\\nThe report called for the government to collect comments from companies and the public about regulations that “hinder AI innovation and adoption, and work with relevant federal agencies to take appropriate action.”\\nIt also threatened that states with laws the administration deemed overly onerous could be at risk of losing out on federal funding related to AI and said that the Federal Communications Commission should evaluate whether any state AI rules conflict with its authority over the nation’s networks.\\nThe power of the federal government should be used to ensure AI systems are built “with freedom of speech and expression in mind,” the plan said. That echoes long-standing conservative claims that products produced by tech companies, including online platforms such as Facebook and YouTube, favor left-leaning perspectives.\\nIt also called on the Department of Commerce to revise a 2023 framework that offers guidance on how to reduce risks associated with the development of AI, removing mentions of DEI, climate change and misinformation.\\nThe government should prioritize the export of American AI tools, the plan said, and called for federal agencies to help the industry sell packages of AI products abroad and work to counter China’s influence over the technology.\\nOther initiatives include promoting the use of AI by federal agencies and the Department of Defense, studying the technology’s effects on the workforce and promoting training for the general population.\\n“As our global competitors race to exploit these technologies, it is a national security imperative for the United States to achieve and maintain unquestioned and unchallenged global technological dominance,” Trump said in the plan. “To secure our future, we must harness the full power of American innovation.”\\nThis article originally appeared in The New York Times.      NaN  New York Times\n",
      "3  2025-07  A.I. Changes Video Games And Alters An Industry: [The Arts/Cultural Desk]  Game designers have used artificial intelligence since the 1980s. But digital characters demonstrating self-awareness is a far cry from the ghosts chasing Pac-Man.\\nIt sounds like a thought experiment conjured by René Descartes for the 21st century.\\nThe citizens of a simulated city inside a video game based on ''The Matrix'' franchise were being awakened to a grim reality. Everything was fake, a player told them through a microphone, and they were simply lines of code meant to embellish a virtual world. Empowered by generative artificial intelligence like ChatGPT, the characters responded in panicked disbelief.\\n''What does that mean,'' said one woman in a gray sweater. ''Am I real or not?''\\nThe unnerving demo, released two years ago by an Australian tech company named Replica Studios, showed both the potential power and the consequences of enhancing gameplay with artificial intelligence. The risk goes far beyond unsettling scenes inside a virtual world. As video game studios become more comfortable with outsourcing the jobs of voice actors, writers and others to artificial intelligence, what will become of the industry?\\nAt the pace the technology is improving, large tech companies like Google, Microsoft and Amazon are counting on their A.I. programs to revolutionize how games are made within the next few years.\\n''Everybody is trying to race toward A.G.I.,'' said the tech founder Kylan Gibbs, using an acronym for artificial generalized intelligence, which describes the turning point at which computers have the same cognitive abilities as humans. ''There's this belief that once you do, you'll basically monopolize all other industries.''\\nIn the earliest months after the rollout of ChatGPT in 2022, the conversation about artificial intelligence's role in gaming was largely about how it could help studios quickly generate concept art or write basic dialogue.\\nIts applications have accelerated quickly. This spring at the Game Developers Conference in San Francisco, thousands of eager professionals looking for employment opportunities were greeted with an eerie glimpse into the future of video games.\\nEngineers from Google DeepMind, an artificial intelligence laboratory, lectured on a new program that might eventually replace human play testers with ''autonomous agents'' that can run through early builds of a game and discover glitches.\\nMicrosoft developers hosted a demonstration of adaptive gameplay with an example of how artificial intelligence could study a short video and immediately generate level design and animations that would otherwise have taken hundreds of hours to produce.\\nAnd executives behind the online gaming platform Roblox introduced Cube 3D, a generative A.I. model that could produce functional objects and environments from text descriptions in a matter of seconds.\\nThese were not the solutions that developers were hoping to see after several years of extensive layoffs; another round of cuts in Microsoft's gaming division this month was a signal to some analysts that the company was shifting resources to artificial intelligence.\\nStudios have suffered as expectations for hyperrealistic graphics turned even their best-selling games into financial losses. And some observers are worried that investing in A.I. programs with hopes of cutting overhead costs might actually be an expensive distraction from the industry's efficiency problems.\\nMost experts acknowledge that a takeover by artificial intelligence is coming for the video game industry within the next five years, and executives have already started preparing to restructure their companies in anticipation. After all, it was one of the first sectors to deploy A.I. programming in the 1980s, with the four ghosts who chase Pac-Man each responding differently to the player's real-time movements.\\nSony did not respond to questions about the A.I. technology it is using for game development.\\nYafine Lee, a spokesman for Microsoft, said, ''Game creators will always be the center of our overall A.I. efforts, and we empower our teams to decide on the use of generative A.I. that best supports their unique goals and vision.''\\nA spokesman for Nintendo said the company did not have further comment beyond what one of its leaders, Shigeru Miyamoto, told The New York Times last year: ''There is a lot of talk about A.I., for example. When that happens, everyone starts to go in the same direction, but that is where Nintendo would rather go in a different direction.''\\nOver the past year, generative A.I. has shifted from a concept into a common tool within the industry, according to a survey released by organizers of the Game Developers Conference. A majority of respondents said their companies were using artificial intelligence, while an increasing number of developers expressed concern that it was contributing to job instability and layoffs.\\nNot all responses were negative. Some developers praised the ability to use A.I. programs to complete repetitive tasks like placing barrels throughout a virtual village.\\nDespite the impressive tech demos at the conference in late March, many developers admitted that their programs were still several years away from widespread use.\\n''There is a very big gap between prototypes and production,'' said Gibbs, who runs Inworld AI, a tech company that builds artificial intelligence programs for consumer applications in sectors like gaming, health and learning. He appeared on a conference panel for Microsoft, where the company showed off its adaptive gameplay model.\\nGibbs said large studios could face costs in the millions of dollars to upgrade their technology. Google, Microsoft and Amazon each hope to become the new backbone of the gaming sector by offering A.I. tools that would require studios to join their servers under expensive contracts.\\nArtificial intelligence technology has developed so fast that it has surpassed Replica Studios, the team behind the tech demo based on the ''Matrix'' franchise. Replica went out of business this year because of the pace of competition from larger companies like OpenAI.\\nReplica's chief technology officer, Eoin McCarthy, said that at the height of the demo's popularity, users were generating more than 100,000 lines of dialogue from nonplayer characters, or NPCs, which cost the start-up about $1,000 per day to maintain.\\nThe cost has fallen in recent years as the A.I. programs have improved, but he said that most developers were unaccustomed to these unbounded costs. There were also fears about how expensive it would be if NPCs started talking to one another.\\nWhen Replica announced it was ending the demo, McCarthy said, some players grew concerned about the fate of the NPCs. '''Were they going to continue to live or would they die?''' McCarthy recalled players' asking. He would reply: ''It is a technology demo. These people aren't real.''\\nLarge companies are often forgoing those moral questions in their presentations to studio executives.\\nNvidia has collaborated with a start-up named Convai to imbue NPCs in a cyberpunk ramen shop with real-time conversations. The Verge posted video showing that Sony was using OpenAI's speech recognition system and other technologies to create a version of Aloy, the protagonist of Horizon Forbidden West, that could answer player questions.\\nSome technologists have gone even further, experimenting with A.I. programs that put faithful simulations of real people into games. In late 2023, researchers from Google and Stanford University partnered on the creation of generative agents, which they described as proxies of human behavior.\\n''Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day,'' their report stated. In a virtual world inspired by The Sims, these agents developed relationships with each other, even planning a Valentine's Day celebration at a cafe.\\nSome ethics experts have applauded the development of technology that might take some burden off acquiring human test subjects. But others have questioned the point of a technology that can only replicate a person's choices.\\n''Humans should be at the center of what we do,'' said Celia Hodent, a specialist in user experience and cognitive science who has been developing a code of ethics in the gaming industry. ''Instead of thinking of A.I. as a solution for everything, having better processes might be a better starting point.''\\nMany of the current programs that could automate game development are still prohibitively expensive to run and full of glitches. Entrepreneurs are preaching patience, saying that usable models will probably take another five years in order to improve quality and bring costs down.\\nGibbs said the adaptive gameplay model shown during Microsoft's conference session would probably costs hundreds or thousands of dollars an hour to run commercially. A similar program called Oasis has its own problems, he said. Because it generates content on a frame-by-frame basis, it forgets visual information not immediately present onscreen, leaving players in a constantly shifting environment.\\nWhile the technology shows promise, Gibbs said, it is still an answer in search of a problem.\\n''How do we push the research community in a more useful direction?'' he asked. ''It's a cheaper way to make games, but it is going to cost you 5,000 times more to run a game, so is it actually cheaper?''\\nBeyond the dollar signs, ethics experts remain focused on questions of how prepared the industry is for sentient characters and levels that design themselves.\\nCansu Canca, the director of responsible A.I. practice at Northeastern University in Boston, said there would be a risk to individual agency and privacy by normalizing the technology.\\n''My biggest concern is not that the A.I. gains consciousness,'' she said, ''but what it means for us to exist in a virtual environment where encounters cannot always be controlled or predicted.''\\nProduced by Rumsey Taylor.\\nProduced by Rumsey Taylor.      NaN  New York Times\n",
      "4  2025-07                         Their Water Taps Ran Dry When Meta Built Next Door                                                                                                                                                                                                                                                                                                                                 NEWTON COUNTY, Ga. — After Meta broke ground on a $750 million data center on the edge of Newton County, Georgia, the water taps in Beverly and Jeff Morris’ home went dry.\\nThe couple’s house, which uses well water, is 1,000 feet from Meta’s new data center. Months after construction began in 2018, the Morrises’ dishwasher, ice maker, washing machine and toilet all stopped working, said Beverly Morris, now 71. Within a year, the water pressure had slowed to a trickle. Soon, nothing came out of the bathroom and kitchen taps.\\nJeff Morris, 67, eventually traced the issues to the buildup of sediment in the water. He said he suspected the cause was Meta’s construction, which could have added sediment to the groundwater and affected their well. The couple replaced most of their appliances in 2019, and then again in 2021 and 2024. Residue now gathers at the bottom of their backyard pool. The taps in one of their two bathrooms still do not work.\\n“It feels like we’re fighting an unwinnable battle that we didn’t sign up for,” Beverly Morris, a retired payroll specialist, said, adding that she and her husband have spent $5,000 on their water problems and cannot afford the $25,000 to replace the well. “I’m scared to drink our own water.”\\nThe Morrises’ experience is one of a growing number of water-related issues around Newton County, which is a 1 1/2-hour drive east of Atlanta and has a population of about 120,000 people. As tech giants like Meta build data centers in the area, local wells have been damaged, the cost of municipal water has soared and the county’s water commission may face a shortage of the vital resource.\\nThe situation has become so dire that Newton County is on track to be in a water deficit by 2030, according to a report last year. If the local water authority cannot upgrade its facilities, residents could be forced to ration water. In the next two years, water rates are set to increase 33%, more than the typical 2% annual increases, said Blair Northen, the mayor of Mansfield, a town in Newton County.\\n“Absolutely terrible,” he said.\\nIn the age of artificial intelligence, water has become as critical to data centers — which power the development of the cutting-edge technology — as electricity. The facilities pump enormous amounts of cold water into pipes that run throughout the buildings to cool the computers inside so that they can perform calculations and keep internet services like social networking humming.\\nA data center like Meta’s, which was completed last year, typically guzzles around 500,000 gallons of water a day. New data centers built to train more powerful AI are set to be even thirstier, requiring millions of gallons of water a day, according to water permit applications reviewed by The New York Times.\\nData center companies often demand as much water as they can get, using the tax revenue they pay as leverage, said Newsha Ajami, a hydrologist and director of urban water policy at Stanford. Some projects are so large that they require the land to first be “dewatered,” which is when groundwater is pumped out of the surrounding area in preparation for construction.\\nYet water is a particularly difficult resource to manage. If a municipality needs to add energy capacity to its grid, it can build new solar farms, wind turbines or reopen coal and nuclear power plants. But the water used by Newton County comes from a nearby reservoir that can be replenished with only rainwater.\\nBecause electricity is more costly for data centers than water, companies often prioritize building their facilities in places with cheap power, even if the area is drought stricken. That has exacerbated water shortages across the world, Ajami said.\\n“Water is an afterthought” for tech companies, she said. “The thinking is, ‘Someone will figure that out later.’”\\nWater troubles similar to Newton County’s are also playing out in other data center hot spots including Texas, Arizona, Louisiana and the United Arab Emirates. Around Phoenix, some homebuilders have paused construction because of droughts exacerbated by data centers. In Colorado, water usage by data centers has become a focal point of renegotiating the Colorado River’s water treaty.\\nA Meta spokesperson said the company had recently commissioned a well study on the Morrises’ property and said it was “unlikely” that its data center affected the supply of groundwater in the area.\\nMeta’s data center uses about 10% of the county’s total water usage daily, said Mike Hopkins, the executive director of the Newton County Water and Sewerage Authority, which is the county’s water authority. The water authority has a good relationship with Meta, he said, but new data center companies are asking for more resources than what’s available.\\n“What the data centers don’t understand is that they’re taking up the community wealth,” he said. “We just don’t have the water.”\\nFor years, Newton County was a growing residential exurb of Atlanta, until that future was put on hold by the 2008 financial crisis. Instead, local officials sought out large industrial projects to fill the void. In the late 2010s, data centers, which can generate millions of dollars in tax revenue, filled that bill.\\nMeta’s project was the first major data center to arrive in Georgia in 2018. At the time, Gov. Brian Kemp, a Republican, celebrated with Facebook-branded shovels at the state Capitol. New tax incentives and cheap industrial power soon made Georgia one of the top picks in the nation for new data centers.\\nIn recent months, Hopkins said, nine companies had applied to build data centers in Newton County, some asking for as much as 6 million gallons of water a day — more than the county’s entire daily usage. Some applicants are tech companies as large as Amazon, according to the water permits, while other companies used aliases to hide their identities.\\nThe county’s water authority is wrestling with how to accommodate the projects — and the tax revenue they bring — while saving enough water for residents. Its solution is to upgrade its recycling facilities, which Hopkins said was a “race against the clock” that would cost more than $250 million.\\nData center companies rarely disclose how much water they use, which has left some policymakers in the dark when it comes to regulation, said Chris Manganiello, the water policy director of Chattahoochee Riverkeeper, an environmental nonprofit in Georgia.\\nLast year, Manganiello figured out a way to get access to some data: If a site was so large it needed to rezone land, the company had to submit its water usage as part of a mandatory regional development study.\\nThe first time he saw the data, Manganiello said his “eyes popped.” One data center company was asking for 9 million gallons of water a day, equal to 30,000 households. “It is a tremendous amount,” he said.\\nThe strain on Georgia’s water has been so severe that some legislators tried to slow down new developments with a bill to repeal tax incentives. Those efforts were vetoed by Kemp, who said in a statement that the bill would hurt economic development.\\nWhen Beverly and Jeff Morris bought their house in 2016, it was even quieter than their last home in Madison, Georgia, a speck of a town with one stoplight. They paid $265,000 for the house and its 6-acre lot, charmed by the dirt roads and forest of oak trees where Beverly Morris could ride her horses.\\n“It was supposed to be our forever home,” Beverly Morris said.\\nMeta bulldozed the oak forest in 2019, and the well water problems began soon after. The couple has taken out thousands of dollars in loans to grapple with the water issues. Jeff Morris has also delayed his retirement as a machine operator at a nearby wood yard.\\nThree of their neighbors have also had issues with their well water since the data center was built. Chris Wilson, who lives three doors down, said his house had experienced water pressure issues within months of the construction. To keep the taps from going dry, he replaces the water filters every month instead of every year.\\nSometimes the water is “so brown, you’d think it came from a creek,” Wilson, 40, said.\\nBen Sheidler, a spokesperson for the Joint Development Authority, which manages the industrial park that Meta’s facilities occupy, said the cause of the water issues was unknown. The Joint Development Authority did not do a well water study before construction to determine any potential effects, but the timing of the problems could be a coincidence, he said.\\n“I wouldn’t want to speculate that even the construction had something to do with it,” he said. “One thousand feet away is a pretty significant distance.”\\nThe hardest part, Beverly Morris said, is that the house now has just one usable bathroom, which they have to share with her adult son Jon, 48, who has Down syndrome. They tried selling the house, with no luck.\\n“Our Realtor told us, ‘There’s only one party that would ever be interested in buying this land — and that’s Facebook,’” she said.\\nThe walls of her home are decorated with phrases about faith and family, and she has spent countless nights questioning how the data center fits into God’s plan for her. Above the kitchen sink, one sign reads: “The Lord giveth and the Lord taketh.”\\nIn May, after an activist media outlet, More Perfect Union, made a video about some of the issues, Meta sent a community relations manager to visit. The company offered to do the well study and fixed some nighttime lighting to reduce glare, but it took no responsibility for the water issues.\\nWhen Beverly Morris said she was afraid to cook with the tap water because of the sediment, the Meta employee suggested that she try boiling the water before using it. The company has denied that its employee said that.\\nThis article originally appeared in The New York Times.      NaN  New York Times\n",
      "\n",
      "============================================================\n",
      "✓ Industry.csv 생성 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bbb5c507d3a21000"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Journal)",
   "language": "python",
   "name": "journal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
